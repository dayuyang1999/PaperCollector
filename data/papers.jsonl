{"id": "http://arxiv.org/abs/2408.16498v1", "title": "A Survey on Evaluating Large Language Models in Code Generation Tasks", "abstract": "This paper provides a comprehensive review of the current methods and metrics\nused to evaluate the performance of Large Language Models (LLMs) in code\ngeneration tasks. With the rapid growth in demand for automated software\ndevelopment, LLMs have demonstrated significant potential in the field of code\ngeneration. The paper begins by reviewing the historical development of LLMs\nand their applications in code generation. Next, it details various methods and\nmetrics for assessing the code generation capabilities of LLMs, including code\ncorrectness, efficiency, readability, and evaluation methods based on expert\nreview and user experience. The paper also evaluates the widely used benchmark\ndatasets, identifying their limitations and proposing directions for future\nimprovements. Specifically, the paper analyzes the performance of code\ngeneration models across different tasks by combining multiple evaluation\nmetrics, such as code compilation/interpretation success rates, unit test pass\nrates, and performance and efficiency metrics, to comprehensively assess the\npractical application of LLMs in code generation. Finally, the paper discusses\nthe challenges faced in evaluating LLMs in code generation, particularly how to\nensure the comprehensiveness and accuracy of evaluation methods and how to\nadapt to the evolving practices of software development. These analyses and\ndiscussions provide valuable insights for further optimizing and improving the\napplication of LLMs in code generation tasks.", "authors": ["Liguo Chen", "Qi Guo", "Hongrui Jia", "Zhengran Zeng", "Xin Wang", "Yijiang Xu", "Jian Wu", "Yidong Wang", "Qing Gao", "Jindong Wang", "Wei Ye", "Shikun Zhang"], "categories": ["cs.SE"], "published": "2024-08-29T12:56:06+00:00", "updated": "2024-08-29T12:56:06+00:00", "pdf_url": "http://arxiv.org/pdf/2408.16498v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2406.00515v2", "title": "A Survey on Large Language Models for Code Generation", "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across\ndiverse code-related tasks, known as Code LLMs, particularly in code generation\nthat generates source code with LLM from natural language descriptions. This\nburgeoning field has captured significant interest from both academic\nresearchers and industry professionals due to its practical significance in\nsoftware development, e.g., GitHub Copilot. Despite the active exploration of\nLLMs for a variety of code tasks, either from the perspective of natural\nlanguage processing (NLP) or software engineering (SE) or both, there is a\nnoticeable absence of a comprehensive and up-to-date literature review\ndedicated to LLM for code generation. In this survey, we aim to bridge this gap\nby providing a systematic literature review that serves as a valuable reference\nfor researchers investigating the cutting-edge progress in LLMs for code\ngeneration. We introduce a taxonomy to categorize and discuss the recent\ndevelopments in LLMs for code generation, covering aspects such as data\ncuration, latest advances, performance evaluation, ethical implications,\nenvironmental impact, and real-world applications. In addition, we present a\nhistorical overview of the evolution of LLMs for code generation and offer an\nempirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks\nacross various levels of difficulty and types of programming tasks to highlight\nthe progressive enhancements in LLM capabilities for code generation. We\nidentify critical challenges and promising opportunities regarding the gap\nbetween academia and practical development. Furthermore, we have established a\ndedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey)\nto continuously document and disseminate the most recent advances in the field.", "authors": ["Juyong Jiang", "Fan Wang", "Jiasi Shen", "Sungju Kim", "Sunghun Kim"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "published": "2024-06-01T17:48:15+00:00", "updated": "2024-11-10T22:02:27+00:00", "pdf_url": "http://arxiv.org/pdf/2406.00515v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2404.11160v2", "title": "Low-Cost Language Models: Survey and Performance Evaluation on Python Code Generation", "abstract": "Large Language Models (LLMs) have become a popular choice for many Natural\nLanguage Processing (NLP) tasks due to their versatility and ability to produce\nhigh-quality results. Specifically, they are increasingly used for automatic\ncode generation to help developers tackle repetitive coding tasks. However,\nLLMs' substantial computational and memory requirements often make them\ninaccessible to users with limited resources. This paper focuses on very\nlow-cost models which offer a more accessible alternative to resource-intensive\nLLMs. We notably: (1) propose a thorough semi-manual evaluation of their\nperformance in generating Python code, (2) introduce a Chain-of-Thought (CoT)\nprompting strategy to improve model reasoning and code quality, and (3) propose\na new dataset of 60 programming problems, with varied difficulty levels,\ndesigned to extend existing benchmarks like HumanEval and EvalPlus. Our\nfindings show that some low-cost compatible models achieve competitive results\ncompared to larger models like ChatGPT despite using significantly fewer\nresources. We will make our dataset and prompts publicly available to support\nfurther research.", "authors": ["Jessica L\u00f3pez Espejel", "Mahaman Sanoussi Yahaya Alassan", "Merieme Bouhandi", "Walid Dahhane", "El Hassane Ettifouri"], "categories": ["cs.AI"], "published": "2024-04-17T08:16:48+00:00", "updated": "2024-08-29T13:23:15+00:00", "pdf_url": "http://arxiv.org/pdf/2404.11160v2", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2404.00227v1", "title": "A Survey of using Large Language Models for Generating Infrastructure as Code", "abstract": "Infrastructure as Code (IaC) is a revolutionary approach which has gained\nsignificant prominence in the Industry. IaC manages and provisions IT\ninfrastructure using machine-readable code by enabling automation, consistency\nacross the environments, reproducibility, version control, error reduction and\nenhancement in scalability. However, IaC orchestration is often a painstaking\neffort which requires specialised skills as well as a lot of manual effort.\nAutomation of IaC is a necessity in the present conditions of the Industry and\nin this survey, we study the feasibility of applying Large Language Models\n(LLM) to address this problem. LLMs are large neural network-based models which\nhave demonstrated significant language processing abilities and shown to be\ncapable of following a range of instructions within a broad scope. Recently,\nthey have also been adapted for code understanding and generation tasks\nsuccessfully, which makes them a promising choice for the automatic generation\nof IaC configurations. In this survey, we delve into the details of IaC, usage\nof IaC in different platforms, their challenges, LLMs in terms of\ncode-generation aspects and the importance of LLMs in IaC along with our own\nexperiments. Finally, we conclude by presenting the challenges in this area and\nhighlighting the scope for future research.", "authors": ["Kalahasti Ganesh Srivatsa", "Sabyasachi Mukhopadhyay", "Ganesh Katrapati", "Manish Shrivastava"], "categories": ["cs.SE", "cs.CL"], "published": "2024-03-30T02:57:55+00:00", "updated": "2024-03-30T02:57:55+00:00", "pdf_url": "http://arxiv.org/pdf/2404.00227v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2401.00812v2", "title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents", "abstract": "The prominent large language models (LLMs) of today differ from past language\nmodels not only in size, but also in the fact that they are trained on a\ncombination of natural language and formal language (code). As a medium between\nhumans and computers, code translates high-level goals into executable steps,\nfeaturing standard syntax, logical consistency, abstraction, and modularity. In\nthis survey, we present an overview of the various benefits of integrating code\ninto LLMs' training data. Specifically, beyond enhancing LLMs in code\ngeneration, we observe that these unique properties of code help (i) unlock the\nreasoning ability of LLMs, enabling their applications to a range of more\ncomplex natural language tasks; (ii) steer LLMs to produce structured and\nprecise intermediate steps, which can then be connected to external execution\nends through function calls; and (iii) take advantage of code compilation and\nexecution environment, which also provides diverse feedback for model\nimprovement. In addition, we trace how these profound capabilities of LLMs,\nbrought by code, have led to their emergence as intelligent agents (IAs) in\nsituations where the ability to understand instructions, decompose goals, plan\nand execute actions, and refine from feedback are crucial to their success on\ndownstream tasks. Finally, we present several key challenges and future\ndirections of empowering LLMs with code.", "authors": ["Ke Yang", "Jiateng Liu", "John Wu", "Chaoqi Yang", "Yi R. Fung", "Sha Li", "Zixuan Huang", "Xu Cao", "Xingyao Wang", "Yiquan Wang", "Heng Ji", "Chengxiang Zhai"], "categories": ["cs.CL"], "published": "2024-01-01T16:51:20+00:00", "updated": "2024-01-08T16:22:42+00:00", "pdf_url": "http://arxiv.org/pdf/2401.00812v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2311.10372v2", "title": "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends", "abstract": "General large language models (LLMs), represented by ChatGPT, have\ndemonstrated significant potential in tasks such as code generation in software\nengineering. This has led to the development of specialized LLMs for software\nengineering, known as Code LLMs. A considerable portion of Code LLMs is derived\nfrom general LLMs through model fine-tuning. As a result, Code LLMs are often\nupdated frequently and their performance can be influenced by the base LLMs.\nHowever, there is currently a lack of systematic investigation into Code LLMs\nand their performance. In this study, we conduct a comprehensive survey and\nanalysis of the types of Code LLMs and their differences in performance\ncompared to general LLMs. We aim to address three questions: (1) What LLMs are\nspecifically designed for software engineering tasks, and what is the\nrelationship between these Code LLMs? (2) Do Code LLMs really outperform\ngeneral LLMs in software engineering tasks? (3) Which LLMs are more proficient\nin different software engineering tasks? To answer these questions, we first\ncollect relevant literature and work from five major databases and open-source\ncommunities, resulting in 134 works for analysis. Next, we categorize the Code\nLLMs based on their publishers and examine their relationships with general\nLLMs and among themselves. Furthermore, we investigate the performance\ndifferences between general LLMs and Code LLMs in various software engineering\ntasks to demonstrate the impact of base models and Code LLMs. Finally, we\ncomprehensively maintained the performance of LLMs across multiple mainstream\nbenchmarks to identify the best-performing LLMs for each software engineering\ntask. Our research not only assists developers of Code LLMs in choosing base\nmodels for the development of more advanced LLMs but also provides insights for\npractitioners to better understand key improvement directions for Code LLMs.", "authors": ["Zibin Zheng", "Kaiwen Ning", "Yanlin Wang", "Jingwen Zhang", "Dewu Zheng", "Mingxi Ye", "Jiachi Chen"], "categories": ["cs.SE"], "published": "2023-11-17T07:55:16+00:00", "updated": "2024-01-08T05:41:51+00:00", "pdf_url": "http://arxiv.org/pdf/2311.10372v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2311.07989v7", "title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code", "abstract": "In this work we systematically review the recent advancements in software\nengineering with language models, covering 70+ models, 40+ evaluation tasks,\n180+ datasets, and 900 related works. Unlike previous works, we integrate\nsoftware engineering (SE) with natural language processing (NLP) by discussing\nthe perspectives of both sides: SE applies language models for development\nautomation, while NLP adopts SE tasks for language model evaluation. We break\ndown code processing models into general language models represented by the GPT\nfamily and specialized models that are specifically pretrained on code, often\nwith tailored objectives. We discuss the relations and differences between\nthese models, and highlight the historical transition of code modeling from\nstatistical models and RNNs to pretrained Transformers and LLMs, which is\nexactly the same course that had been taken by NLP. We also go beyond\nprogramming and review LLMs' application in other software engineering\nactivities including requirement engineering, testing, deployment, and\noperations in an endeavor to provide a global view of NLP in SE, and identify\nkey challenges and potential future directions in this domain. We keep the\nsurvey open and updated on GitHub at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.", "authors": ["Ziyin Zhang", "Chaoyu Chen", "Bingchang Liu", "Cong Liao", "Zi Gong", "Hang Yu", "Jianguo Li", "Rui Wang"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "published": "2023-11-14T08:34:26+00:00", "updated": "2024-06-26T07:11:00+00:00", "pdf_url": "http://arxiv.org/pdf/2311.07989v7", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2310.17903v1", "title": "Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey", "abstract": "Modern language models (LMs) have been successfully employed in source code\ngeneration and understanding, leading to a significant increase in research\nfocused on learning-based code intelligence, such as automated bug repair, and\ntest case generation. Despite their great potential, language models for code\nintelligence (LM4Code) are susceptible to potential pitfalls, which hinder\nrealistic performance and further impact their reliability and applicability in\nreal-world deployment. Such challenges drive the need for a comprehensive\nunderstanding - not just identifying these issues but delving into their\npossible implications and existing solutions to build more reliable language\nmodels tailored to code intelligence. Based on a well-defined systematic\nresearch approach, we conducted an extensive literature review to uncover the\npitfalls inherent in LM4Code. Finally, 67 primary studies from top-tier venues\nhave been identified. After carefully examining these studies, we designed a\ntaxonomy of pitfalls in LM4Code research and conducted a systematic study to\nsummarize the issues, implications, current solutions, and challenges of\ndifferent pitfalls for LM4Code systems. We developed a comprehensive\nclassification scheme that dissects pitfalls across four crucial aspects: data\ncollection and labeling, system design and learning, performance evaluation,\nand deployment and maintenance. Through this study, we aim to provide a roadmap\nfor researchers and practitioners, facilitating their understanding and\nutilization of LM4Code in reliable and trustworthy ways.", "authors": ["Xinyu She", "Yue Liu", "Yanjie Zhao", "Yiling He", "Li Li", "Chakkrit Tantithamthavorn", "Zhan Qin", "Haoyu Wang"], "categories": ["cs.SE", "cs.AI"], "published": "2023-10-27T05:32:57+00:00", "updated": "2023-10-27T05:32:57+00:00", "pdf_url": "http://arxiv.org/pdf/2310.17903v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2308.01191v3", "title": "Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey", "abstract": "Code cloning, the duplication of code fragments, is common in software\ndevelopment. While some reuse aids productivity, excessive cloning hurts\nmaintainability and introduces bugs. Hence, automatic code clone detection is\nvital. Meanwhile, large language models (LLMs) possess diverse code-related\nknowledge, making them versatile for various software engineering challenges.\nHowever, LLMs' performance in code clone detection is unclear and needs more\nstudy for accurate assessment. In this paper, we provide the first\ncomprehensive evaluation of LLMs for clone detection, covering different clone\ntypes, languages, and prompts. We find advanced LLMs excel in detecting complex\nsemantic clones, surpassing existing methods. Adding intermediate reasoning\nsteps via chain-of-thought prompts noticeably enhances performance.\nAdditionally, representing code as vector embeddings, especially with text\nencoders, effectively aids clone detection.Lastly, the ability of LLMs to\ndetect code clones differs among various programming languages. Our study\nsuggests that LLMs have potential for clone detection due to their language\ncapabilities, offering insights for developing robust LLM-based methods to\nenhance software engineering.", "authors": ["Shihan Dou", "Junjie Shan", "Haoxiang Jia", "Wenhao Deng", "Zhiheng Xi", "Wei He", "Yueming Wu", "Tao Gui", "Yang Liu", "Xuanjing Huang"], "categories": ["cs.SE"], "published": "2023-08-02T14:56:01+00:00", "updated": "2023-08-06T01:40:59+00:00", "pdf_url": "http://arxiv.org/pdf/2308.01191v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2212.10079v1", "title": "A Survey on Pretrained Language Models for Neural Code Intelligence", "abstract": "As the complexity of modern software continues to escalate, software\nengineering has become an increasingly daunting and error-prone endeavor. In\nrecent years, the field of Neural Code Intelligence (NCI) has emerged as a\npromising solution, leveraging the power of deep learning techniques to tackle\nanalytical tasks on source code with the goal of improving programming\nefficiency and minimizing human errors within the software industry. Pretrained\nlanguage models have become a dominant force in NCI research, consistently\ndelivering state-of-the-art results across a wide range of tasks, including\ncode summarization, generation, and translation. In this paper, we present a\ncomprehensive survey of the NCI domain, including a thorough review of\npretraining techniques, tasks, datasets, and model architectures. We hope this\npaper will serve as a bridge between the natural language and programming\nlanguage communities, offering insights for future research in this rapidly\nevolving field.", "authors": ["Yichen Xu", "Yanqiao Zhu"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "published": "2022-12-20T08:34:56+00:00", "updated": "2022-12-20T08:34:56+00:00", "pdf_url": "http://arxiv.org/pdf/2212.10079v1", "primary_category": "cs.SE"}
