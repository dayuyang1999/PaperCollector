{"url": "http://arxiv.org/pdf/2412.04057v1", "keywords": ["program synthesis", "game AI", "Python", "Java", "evolutionary search"], "overall_summary": "This paper explores the potential of using large language models (LLMs) to directly synthesize usable code for a wide range of gaming applications in Python and Java. An evolutionary hill-climbing algorithm is used, where LLMs generate and refine the initial programs and mutations.", "structure": "1. Introduction 2. Related Work 3. Framework Description 4. Large Language Models Used 5. Game Applications 5.1. Programmatic Policies: Minatar (Python) 5.2. Baba is You (Python) 5.3. Vehicle Driving (Python) 5.4. Maze Generation (Python) 5.5. Tabletop Games (Java) 6. Experiments and Results 7. Discussion 8. Conclusion", "methodology": ["An evolutionary hill-climbing algorithm is used to search for programs", "LLMs generate initial programs and perform mutations based on prompts", "Programs are executed and evaluated in a subprocess environment", "Prompts are updated based on program execution results to refine the programs", "Process iterates until evaluation criteria are met or maximum attempts reached"], "example": "For the Seaquest game in Minatar, the LLM is prompted to generate a Python function that controls a submarine to save divers while shooting enemies. The function is executed, and the resulting score/reward is used to update the prompt. The LLM then refines the function based on this feedback. This process continues until a satisfactory strategy emerges or the maximum attempts are exhausted."}
{"url": "http://arxiv.org/pdf/2411.17538v2", "keywords": ["Isotropy", "Code Search", "Embedding Whitening", "ZCA Whitening", "Soft-ZCA"], "overall_summary": "The paper investigates the impact of isotropy (uniform distribution of vectors) in embedding spaces on semantic code search performance. It proposes Soft-ZCA, a modified ZCA whitening technique to control isotropy levels, and demonstrates improved code search results across multiple programming languages.", "structure": "1. Introduction (motivation and background) 2. Whitening of Embeddings (technical details of ZCA whitening and Soft-ZCA) 3. Experimental Apparatus (datasets, models, procedure) 4. Results (analysis of isotropy, ranking performance, impact of Soft-ZCA) 5. Conclusion 6. Appendix (additional experiments and details)", "methodology": ["- Analyze isotropy of embedding spaces in CodeBERT, CodeT5+, and Code Llama models using IsoScore metric", "- Introduce Soft-ZCA, an extension of ZCA whitening with an eigenvalue regularizer to control whitening degree", "- Fine-tune CodeBERT on code-comment pairs with contrastive loss", "- Apply Soft-ZCA whitening to embeddings and evaluate on code search task using MRR"], "example": "For the Python dataset, applying Soft-ZCA whitening with epsilon=0.0001 to the fine-tuned CodeBERT model increased the IsoScore from 0.555 (code) / 0.381 (comment) to 0.998 for both. This near-perfect isotropy resulted in an MRR improvement of +0.062 over the non-whitened embeddings."}
{"url": "http://arxiv.org/pdf/2411.17230v1", "keywords": ["semantic code search", "fault localization", "large language models", "multi-granularity analysis", "software knowledge base"], "overall_summary": "This paper proposes CosFL, a novel approach to fault localization that treats it as a semantic code search problem. By leveraging large language models and constructing a multi-granularity software knowledge base, CosFL generates natural language queries describing buggy functionalities and retrieves semantically related code for locating faults.", "structure": {"1. Introduction": "Motivates the idea of using code search for fault localization and outlines the main challenges.", "2. Motivation": "Provides a case study illustrating how CosFL localizes a real bug from a code search perspective.", "3. Approach": "Describes the two main components: software knowledge base construction and the query generation + fault retrieval pipeline.", "4. Implementation": "Details on implementing the different components of CosFL.", "5. Experimental Setup": "Describes the benchmarks, baselines, evaluation metrics used.", "6. Results and Analysis": "Presents quantitative results, ablation studies, case studies comparing to baselines.", "7. Discussion": "Discusses implications, limitations and future work.", "8. Related Work": "Surveys related work in fault localization and code search."}, "methodology": ["- Construct a multi-granularity software knowledge base by static/dynamic analysis and LLM comprehension", "- Use LLM to generate natural language queries describing buggy functionalities at different granularities", "- Retrieve code semantically related to the queries using multi-granularity code search", "- Employ a voting mechanism to pinpoint the final fault localization results"], "example": "For a bug in the Closure compiler where function arguments were incorrectly removed, CosFL generates the query 'not correctly handling the parameters of nested functions' and successfully retrieves the removeUnreferencedFunctionArgs method as the top result."}
{"url": "http://arxiv.org/pdf/2411.12644v2", "keywords": ["code retrieval", "multimodal embeddings", "unified training", "programming languages", "code-related tasks"], "overall_summary": "This paper introduces CodeXEmbed, a family of large-scale embedding models tailored for code and text retrieval across multiple programming languages and code-related tasks. The unified training approach achieves state-of-the-art performance on code retrieval benchmarks.", "structure": "1. Introduction (motivation and background)\n2. Method (training approach, retrieval settings)\n3. Experiments (benchmarks, implementation details, results)\n4. Conclusion", "methodology": ["Unified training framework that converts diverse code tasks into retrieval problems (text-to-code, code-to-text, code-to-code, hybrid)", "Contrastive loss to maximize similarity between query and correct answer while minimizing similarity to negative samples", "Models trained on code/text data ranging from 400M to 7B parameters"], "example": "For the code-to-text retrieval setting, the task of code summarization is used as an example. Given a code file or repository as input, the goal is to retrieve a concise textual summary describing the code's functionality."}
{"url": "http://arxiv.org/pdf/2411.11053v4", "keywords": ["self-driven reasoning", "monte carlo tree search", "code generation", "data augmentation", "chain-of-thought"], "overall_summary": "The paper proposes SRA-MCTS, a self-driven reasoning augmentation method that guides language models to autonomously generate high-quality reasoning paths for complex code generation tasks. By synthesizing natural language plans and translating them to code, SRA-MCTS improves performance without additional supervision.", "structure": "1. Introduction - Motivation and background\n2. Related Work - Prior approaches like data distillation, chain-of-thought\n3. Method - SRA-MCTS algorithm details\n4. Experiments - Setup, baselines, datasets, evaluation\n5. Results and Analysis", "methodology": ["Use Monte Carlo Tree Search (MCTS) to guide an LLM to generate diverse reasoning plans in natural language for code problems", "Evaluate and select high-quality plans based on correctness, completeness and coherence scores from the LLM itself", "Use the LLM to translate the selected reasoning plans into executable code", "Fine-tune the LLM on the generated (problem, reasoning plan, code) triples"], "example": "For the LeetCode problem 'Longest Palindromic Substring', SRA-MCTS may generate a reasoning plan like:\n1. Iterate through the string to find potential palindromes\n2. For each potential palindrome, check if it is a valid palindrome\n3. Keep track of the longest valid palindrome found\n4. Return the longest palindrome\nThe LLM then converts this into Python code based on the reasoning plan."}
{"url": "http://arxiv.org/pdf/2411.08706v1", "keywords": ["latent program space", "gradient-based search", "program induction", "test-time adaptation", "ARC-AGI benchmark"], "overall_summary": "This paper introduces the Latent Program Network (LPN), a neural architecture that learns a continuous latent space representing programs. LPN enables efficient search and test-time adaptation in this latent space to solve program synthesis tasks, outperforming methods without such adaptation on the challenging ARC-AGI benchmark.", "structure": "1. Introduction - Motivation and background on program synthesis 2. Related Work - Existing symbolic, neural, and neuro-symbolic approaches 3. Background - Formal problem definition 4. Latent Program Network - Proposed model architecture and training 5. Experiments - Evaluation on ARC-AGI and ablation studies 6. Conclusion", "methodology": ["Learn a continuous latent space encoding programs using a variational autoencoder framework", "Train encoder to map input-output pairs to latent codes, and decoder to execute latent codes on new inputs", "Novel training objective prevents encoding outputs directly in latent space", "Perform gradient-based search in latent space during training and inference to find program best explaining specification", "Use test-time adaptation by optimizing latent code to solve new tasks"], "example": "For an ARC-AGI task with specification showing how to draw a line segment, LPN would: 1) Encode the line drawing examples to a latent code 2) Optimize this code to best reconstruct the line outputs 3) Execute the optimized code on a new input to predict how to extend the line segment"}
{"url": "http://arxiv.org/pdf/2411.06796v1", "keywords": ["iterative test-driven checker generation", "logic-guided API retrieval", "static code analysis", "large language model", "automated code generation"], "overall_summary": "This paper proposes AutoChecker, an approach that leverages large language models to automatically generate static code checkers based on a rule description and test suite. It uses an iterative generation process guided by individual test cases to handle complex checking logic, and employs logic-guided API retrieval to provide relevant API knowledge for the code generation.", "structure": "1. Introduction 2. Background and Motivation 3. Methodology 3.1 Overview 3.2 API-Context Retriever 3.3 Checker Generator 3.3.1 Initial Checker Generation 3.3.2 Iterative Checker Generation 4. Experimental Setup 5. Experimental Results 5.1 Effectiveness Evaluation 5.2 Practicality Evaluation 6. Discussion 7. Related Work 8. Conclusion", "methodology": ["- Iterative test-driven checker generation to handle complex logic by updating the checker with one test case at a time", "- Logic-guided API retrieval to extract relevant API contexts for each sub-operation in the checking logic", "- Construction of a Meta-Op database containing 354 atomic checking operations to guide API retrieval", "- Use of an initial checker generator and an iterative checker generator leveraging the retrieved API contexts"], "example": "For the PMD rule 'AssignmentToNonFinalStaticRule', AutoChecker generates a checker that visits all assignment expressions within constructors and checks if the assigned identifier is a static and non-final field. It retrieves relevant APIs like ASTVariableAccess, ASTFieldAccess, and methods to check symbol properties, enabling correct implementation of the logic."}
{"url": "http://arxiv.org/pdf/2411.05547v2", "keywords": ["answerability assessment", "retrieval-augmented code generation", "hallucination mitigation", "benchmark dataset", "few-shot learning"], "overall_summary": "This paper proposes a new task of assessing the answerability of queries in retrieval-augmented code generation (RaCG) systems, where a language model generates code based on a user query and retrieved API documentation. It introduces a benchmark dataset called RaCGEval to evaluate models on this task.", "structure": "1. Introduction 2. Constructing the RaCGEval Benchmark Dataset 2.1 Task description 2.2 API documentation 2.3 Generating partially answerable and unanswerable samples 2.4 Annotation 3. Methods for Assessing Answerability 3.1 Zero-shot inference on instruction-following LLMs 3.2 Fine-tuning LLMs with automatically generated training datasets 4. Experiments 4.1 Experimental settings 4.2 Experimental results 5. Discussion 5.1 In-context learning for domain adaptation 5.2 Trade-off between coverage and precision 6. Conclusion", "methodology": ["Use private/modified API documentation datasets to avoid prior knowledge in language models", "Generate partially answerable and unanswerable samples by substituting gold APIs, concatenating unrelated queries, or using out-of-database queries", "Annotate samples using multiple experts to ensure reliability", "Evaluate zero-shot inference on instruction-tuned LLMs", "Fine-tune LLMs on automatically generated training data using CoNaLa dataset", "Explore in-context learning for domain adaptation", "Analyze trade-off between coverage and precision of code generation"], "example": "For the query 'How to create a web page using NetsPresso API?' and retrieved NetsPresso API documentation on model optimization, the language model may generate plausible but incorrect code, since web page creation is out of scope for this API. The system should identify this query as unanswerable based on the retrieved documentation."}
{"url": "http://arxiv.org/pdf/2411.04752v1", "keywords": ["Code-mixing", "Roman transliteration", "Bengali language", "Prompting", "Sequential relevance"], "overall_summary": "This paper presents a novel approach to extract relevant information from code-mixed conversations in Roman transliterated Bengali mixed with English. It leverages GPT-3.5 Turbo via prompting and integrates the model's outputs into a mathematical model that accounts for sequential dependencies among documents.", "structure": "1. Introduction, 2. Related Work, 3. Dataset, 4. Task Definition, 5. Methodology (5.1 Why Prompting?, 5.2 Merging Prompt and Mathematical Model-Based Approaches), 6. Results, 7. Conclusion", "methodology": ["Used GPT-3.5 Turbo model via prompting to obtain relevance scores for documents given a query", "Designed a mathematical model that incorporates sequential dependencies among documents to refine relevance probabilities", "If previous document is relevant and current score >= 0.3, probability = 0.2 + current score", "If previous document is relevant and current score < 0.3, probability = current score", "Considered documents with probability > 0.5 as relevant to the query"], "example": "For the query 'Where can I find good Bengali food in Delhi?', the system would analyze a sequence of documents from a Facebook group like 'Bengali in Delhi'. It would use GPT-3.5's understanding of the query and each document to get relevance scores. The mathematical model would then boost the scores for documents that follow a relevant one in the sequence. Highly scored documents recommending Bengali restaurants would be retrieved as the most relevant results."}
{"url": "http://arxiv.org/pdf/2411.04329v2", "keywords": ["tree-based code generation", "multi-agent collaboration", "strategy exploration", "code refinement", "execution feedback"], "overall_summary": "The paper proposes CodeTree, a framework that uses multiple collaborating language model agents to efficiently explore the search space for code generation through a tree-based structure. Agents generate coding strategies, initial solutions, refinements, and evaluations, guided by execution feedback and critic scoring to find optimal code.", "structure": "1. Introduction 2. Related Work 3. Method: 3.1 Coding Task Agents (Thinker, Solver, Debugger), 3.2 Tree Expanding with Critic Agent, 3.3 Multi-Agent Collaboration, 3.4 Training 4. Experiments 5. Results 6. Analysis 7. Conclusion", "methodology": ["Define Thinker, Solver, Debugger agents for strategy generation, initial coding, refinement", "Build tree with root as problem, nodes as candidate solutions", "Critic agent scores nodes based on execution feedback and strategy match", "Critic expands tree by refining, aborting or accepting nodes based on scores", "Agents collaborate, passing intermediate solutions and getting critic feedback"], "example": "For the problem of 'reversing a string', the Thinker may generate strategies like 'use slicing' or 'convert to list'. The Solver generates initial code attempts for each strategy. The Debugger refines code with Critic feedback on test cases. The tree expands promising nodes until an acceptable solution is found."}
{"url": "http://arxiv.org/pdf/2411.01102v3", "keywords": ["external environment semantics", "binary code search", "semantic enhancement", "relational graph convolutional networks", "function inlining"], "overall_summary": "The paper proposes BinEnhance, a framework that leverages external environment semantics to enhance internal code semantics for improving binary code search. It constructs an External Environment Semantic Graph (EESG) and uses a Semantic Enhancement Model with Relational Graph Convolutional Networks to incorporate external semantics into function embeddings.", "structure": "1. Introduction 2. Preliminaries 3. Design of BinEnhance 4. Evaluation 5. Related Work 6. Conclusion", "methodology": ["Construct an External Environment Semantic Graph (EESG) with four edge types (call-dependency, data-co-use, address-adjacency, string-use) to model external environment", "Initialize EESG node embeddings using existing internal code semantic models", "Use Relational Graph Convolutional Networks (RGCNs) in a Semantic Enhancement Model to update node embeddings with external semantics", "Merge internal and external embeddings via a residual block to get enhanced embeddings", "Combine semantic similarity with data feature similarity for final ranking"], "example": "For the 'transfer' function compiled with different optimization levels (O0 and O3), BinEnhance's EESG remains largely stable despite function inlining changes to the internal code, allowing it to still match the two versions based on the enhanced embeddings capturing external semantics."}
{"url": "http://arxiv.org/pdf/2410.22240v1", "keywords": ["decoder-only language models", "code search", "fine-tuning", "zero-shot performance", "model generalization"], "overall_summary": "This paper systematically explores the use of decoder-only large language models (LLMs) for code search tasks. It evaluates nine state-of-the-art decoder-only LLMs using different fine-tuning methods, datasets, and model sizes, finding that fine-tuned CodeGemma significantly outperforms encoder-only models like UniXcoder in code search.", "structure": "1. Introduction 2. Background 2.1 Code Search 2.2 Decoder-only LLMs for Information Retrieval 3. Study Setup 3.1 Benchmark Datasets 3.2 Evaluation Metrics 3.3 Models 4. Results and Analysis 5. Conclusion", "methodology": ["Evaluated 9 decoder-only LLMs (e.g., CodeGemma, CodeLlama, DeepSeekCoder) and 2 encoder-only models (CodeBERT, UniXcoder) on code search tasks", "Used two datasets: CodeSearchNet (CSN) covering multiple languages, and CoSQA+ for Python", "Employed two fine-tuning methods and three model sizes for comprehensive analysis", "Measured performance using Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) metrics"], "example": "On the CoSQA+ dataset, fine-tuned CodeGemma achieved a 49.6% increase in MAP and a 41.3% improvement in MRR compared to UniXcoder, demonstrating superior generalization to new datasets even without explicit training on them."}
{"url": "http://arxiv.org/pdf/2410.16655v1", "keywords": ["semantic-guided search", "greedy decoding", "test validation feedback", "memory-efficient program repair", "large language model fine-tuning"], "overall_summary": "This paper introduces FLAMES, a novel approach for efficient and effective automated program repair using large language models (LLMs). FLAMES employs semantic-guided search with test validation feedback to steer LLM-based patch generation, improving repair performance while significantly reducing memory consumption.", "structure": "1. Introduction 2. Motivation Study 3. FLAMES Approach 4. Experimental Evaluation 5. Related Work 6. Conclusion", "methodology": ["- Uses greedy decoding (beam size 1) for initial patch generation instead of beam search", "- Employs best-first search algorithm (PG-TD) with semantic feedback from test validations to refine patches", "- Combines LLM-based and search-based program repair techniques", "- Fine-tunes large language models (1B-7B parameters) on program repair datasets"], "example": "For a buggy Java program with failing test cases, FLAMES first generates initial patch candidates using greedy decoding from a fine-tuned LLM. It then iteratively refines these candidates using a best-first search guided by semantic feedback from test case execution, such as the number of passing/failing tests for each candidate. This allows FLAMES to efficiently explore the search space and prioritize more promising patch candidates."}
{"url": "http://arxiv.org/pdf/2411.05010v1", "keywords": ["scattered forest search", "code space exploration", "textual optimization directions", "multi-start optimization", "ant colony optimization"], "overall_summary": "The paper proposes a novel approach called Scattered Forest Search (SFS) to enhance the exploration and exploitation of language models for code generation tasks. SFS frames code generation as a black-box optimization problem and employs techniques inspired by optimization methods to efficiently search the code space.", "structure": "1. Introduction 2. Background 3. Methodology (describing SFS techniques) 4. Theoretical Perspective 5. Experiments 6. Conclusion", "methodology": ["- Scattering: Dynamically vary input prompts to LLM to generate diverse textual optimization directions for new solutions", "- Foresting: Perform tree search from multiple random seed solutions to enhance exploration breadth", "- Scouting: Share feedback on successful/unsuccessful directions across search branches to guide exploitation"], "example": "For the task of finding min/max k elements from a tuple: 1) Scattering prompts LLM to propose directions like 'Modify return statement' or 'Handle k > len(tuple)' 2) Each child branch follows one direction to generate a new solution 3) Scouting updates global memory on whether that direction worked, guiding future branches"}
{"url": "http://arxiv.org/pdf/2410.16229v2", "keywords": ["code structure-aware retrieval", "dual-view code representation", "retrieval-augmented code generation", "code-documentation alignment", "masked entity prediction"], "overall_summary": "This paper proposes CONAN, a framework for building a coding assistant that mimics human knowledge-seeking behaviors during coding. It consists of a code structure-aware retriever (CONAN-R) and a dual-view code representation-based retrieval-augmented generation model (CONAN-G).", "structure": "1. Introduction 2. Related Work 3. Methodology 3.1 Preliminary 3.2 Retrieval Module (CONAN-R) 3.3 Generation Module (CONAN-G) 3.4 Assisting LLMs 4. Experiments 5. Analysis 6. Conclusion", "methodology": ["- CONAN-R pretrains CodeT5 using Code-Documentation Alignment (CDA) and Masked Entity Prediction (MEP) tasks to learn code structure-aware representations", "- CDA aligns matched code-documentation pairs to bridge modality gap", "- MEP masks code entities and predicts them to capture code semantics", "- CONAN-G uses Fusion-in-Decoder (FID) to incorporate multiple retrieved code snippets", "- CONAN-G treats documentation as prompts to better understand code semantics"], "example": "For code generation, CONAN takes a natural language description as input, retrieves relevant code snippets and documentation using CONAN-R, and then uses the retrieved information along with the description in CONAN-G to generate the target code."}
{"url": "http://arxiv.org/pdf/2410.11300v1", "keywords": ["contrastive learning", "syntax tree encoding", "code retrieval", "in-context learning", "large language model feedback"], "overall_summary": "This paper proposes a novel approach called Instructive Code Retriever (ICR) that leverages large language model feedback and syntax tree encoding to retrieve high-quality examples for enhancing in-context learning performance on code intelligence tasks like summarization, synthesis, and bug fixing.", "structure": "1. Introduction 2. Motivation 3. Instructive Code Retriever 3.1 Overview 3.2 Preprocessing and Preparation 3.3 Retrieval Model 3.4 Training 4. Experiments 4.1 Experimental Setup 4.2 Results 5. Related Work 6. Conclusion", "methodology": ["Use syntax trees to encode structural information of code and natural language", "Compute tree-based similarity between query and examples using tree encodings", "Use large language model to score examples based on their helpfulness for the task", "Train retriever with contrastive loss to distinguish good vs bad examples based on LLM feedback", "During inference, retrieve top examples using trained ICR and use them as prompts for LLM"], "example": "For the program synthesis task, the query could be 'Write a function to adjust log10 to handle values less or equal to zero'. ICR would retrieve a training example like: \n\nInput: 'Write a function to adjust squareRoot to handle negative values'\nOutput: 'public static double squareRoot(double val) { ... }'\n\nDespite textual differences, the syntax trees indicate this is a good example for adjusting a math function, so ICR would retrieve it to help the LLM generate the desired log10 function."}
{"url": "http://arxiv.org/pdf/2410.09662v1", "keywords": ["approximate nearest neighbor search", "retrieval efficiency", "code generation tasks", "demonstration retrieval", "retrieval-augmented generation"], "overall_summary": "This paper systematically evaluates the efficiency-effectiveness trade-off of different retrievers (sparse and dense) in retrieval-augmented generation (RAG) for coding tasks like program synthesis, commit message generation, and assertion generation. It finds that while sparse retrievers like BM25 excel in effectiveness for small knowledge bases, approximate dense retrievers like HNSW offer significant efficiency gains with minimal quality drop for large-scale retrieval.", "structure": "1. Introduction 2. Background (RAG, retrieval approaches) 3. Experimental Design (research questions, datasets, retrievers, metrics) 4. Results (effectiveness, efficiency, trade-off analysis) 5. Discussion 6. Conclusion", "methodology": ["Evaluated 6 retrievers: 2 sparse (BM25, BM25L), 1 exhaustive dense (SBERT Semantic Search), 3 approximate dense (ANNOY, LSH, HNSW)", "Used SBERT embeddings to encode queries and demonstrations", "Approximate retrievers build indexes to retrieve subset of neighbors instead of full scan", "Compared retrieval time and output quality metrics across tasks and knowledge base sizes"], "example": "For the commit message generation task with a large knowledge base, the HNSW approximate dense retriever achieved a 44x speedup over BM25 with only a 1.74% drop in RougeL score. This demonstrates HNSW's ability to provide an efficient-effective retrieval trade-off for RAG in large-scale coding tasks."}
{"url": "http://arxiv.org/pdf/2410.03431v2", "keywords": ["unified language model", "dual encoders", "cosine similarity loss", "FastText embeddings", "code search for Python"], "overall_summary": "This paper proposes a dual encoder approach using a unified language model and cosine similarity loss for the code search task, specifically for Python. It demonstrates improved performance over state-of-the-art methods while being more efficient.", "structure": "1. Introduction 1.1 Proposed Approach 1.2 Research Questions 1.3 Contributions 2. Background and Related Work 2.1 Code Search 2.2 Code Search with LLMs 2.3 Dual Encoders 2.4 Code Search with Dual Encoders 3. Methodology 4. Experiments and Results 5. Analysis and Discussion 6. Conclusion and Future Work", "methodology": ["- Use a unified FastText language model to generate word embeddings for both natural language and code sequences", "- Project the embeddings onto a shared space using dual encoders (one for natural language, one for code)", "- Train the encoders using cosine similarity loss to make linked pairs closer in the shared space", "- At inference, compute cosine similarity between query and code embeddings to retrieve relevant code"], "example": "For the natural language query 'Decompresses data for Content-Encoding: deflate', the model embeds both the query and candidate Python functions like 'def undeflate(data): ...' onto the shared space. The Python function with the highest cosine similarity to the query is retrieved as the most relevant result."}
{"url": "http://arxiv.org/pdf/2409.19668v1", "keywords": ["integer quadratic programming", "local search operators", "quadratic constraints", "two-mode search", "scoring functions"], "overall_summary": "This paper proposes LS-IQCQP, a novel local search solver for solving general integer quadratic programming (IQP) problems. It introduces new local search operators capable of handling quadratic terms in both the objective and constraints, along with a two-mode algorithm utilizing specialized scoring functions.", "structure": "1. Introduction 2. Preliminaries 3. Local Search Operators 3.1 Operator for Feasibility 3.2 Operators for Optimization 4. Weighting Scheme and Score Function 5. Local Search Algorithm", "methodology": ["Proposed four new local search operators for IQP: - Quadratic satisfying move operator for violated constraints - Inequality exploration move operator for optimizing objective - Equality incremental move operator for satisfied equality constraints - Free move operator for unconstrained variables", "Introduced a two-mode local search algorithm: - Satisfying mode uses operators to satisfy violated constraints - Optimization mode uses operators to improve objective value", "Designed specialized scoring functions incorporating weights for constraints and objective", "Implemented dynamic weight adjustment scheme to guide search"], "example": "For a quadratic equality constraint x^2 + y^2 = 25 involving variables x and y in the objective, the equality incremental move operator works as follows: 1) Increment/decrement x by 1 to decrease the objective value 2) Update y to a new integer value that satisfies the equality constraint 3) Accept the move if the new (x,y) assignment decreases the overall objective value"}
{"url": "http://arxiv.org/pdf/2409.15895v1", "keywords": ["retrieval-augmented code generation", "preference-guided refactorer", "generative compression", "preference gap", "two-stage training"], "overall_summary": "This paper proposes a novel framework called RRG (Retrieve, Refactor, Generate) to improve retrieval-augmented code generation by introducing a code refactorer module that bridges the preference gap between the retriever and generator. It uses a two-stage training approach to enable the refactorer to compress retrieved code into a concise, model-friendly form aligned with the generator's preferences.", "structure": "1. Introduction - Motivates retrieval-augmented code generation and identifies limitations 2. Related Work - Covers pretrained models, retrieval-augmented generation, retrieval-augmented code generation 3. Approach - Describes the RRG framework, refactored fine-tuning, and preference-aware tuning 4. Experiments 5. Conclusion", "methodology": ["Introduces a code refactorer module between the retriever and generator", "Two-stage training scheme:", "- Stage 1 (Refactored Fine-Tuning): Train refactorer for generative compression using supervised learning on target code", "- Stage 2 (Preference-Aware Tuning): Finetune refactorer using reinforcement learning to align its output with generator preferences"], "example": "For the query 'implement binary search in Python', the retriever fetches relevant code snippets. The refactorer compresses these into a concise, Python-specific code example focused on binary search, removing redundancies. This refactored code is then fed to the generator to produce the final implementation."}
{"url": "http://arxiv.org/pdf/2409.13122v2", "keywords": ["retrieval-augmented generation", "verbal reinforcement learning", "repository-level code completion", "iterative feedback optimization", "context-aware code generation"], "overall_summary": "The paper proposes RepoGenReflex, a framework that enhances repository-level code completion by integrating Retrieval-Augmented Generation (RAG) with Verbal Reinforcement Learning (VRL). It optimizes the retrieval and generation process through iterative linguistic feedback, leading to improved accuracy and relevance of code completions.", "structure": "1. Introduction 2. Related Work 2.1 Retrieval-Augmented Generation (RAG) 2.2 Verbal Reinforcement Learning (VRL) 2.3 Repository-Level Code Completion 3. Framework Design 3.1 Components - Retriever - Actor (Generative LLM) - Evaluator - Reflector - Experience 3.2 Leveraging Experience for Improved Retrieval 3.3 Iterative Process 3.4 Component Interdependency", "methodology": ["- Uses Retrieval-Augmented Generation (RAG) to retrieve relevant code snippets from repositories", "- Employs a pre-trained language model (LLM) as the Actor to generate code completions", "- Evaluates generated code using Exact Match (EM) and Edit Similarity (ES) metrics", "- Utilizes Verbal Reinforcement Learning (VRL) through the Reflector component to provide linguistic feedback", "- Stores feedback in the Experience component to refine future retrievals and generations", "- Iteratively improves code completions by leveraging feedback and adjusting retrievals"], "example": "For example, if the Reflector provides feedback suggesting improvements to a function's error handling, the Experience component extracts these suggestions. The Retriever then constructs a new retrieval target by combining the suggestions with the remaining unfinished code. It performs a similarity search against the repository files to find the most relevant code snippets aligning with the new target, which are then used to generate an improved version of the code in the next iteration."}
{"url": "http://arxiv.org/pdf/2409.09584v1", "keywords": ["thought-level code generation", "Monte Carlo tree search", "code execution feedback", "error correction", "dual evaluation"], "overall_summary": "This paper proposes RethinkMCTS, a framework that uses Monte Carlo Tree Search (MCTS) to explore the thought process of code generation and leverages detailed feedback from code execution to refine erroneous thoughts, thereby improving the overall search quality and code generation performance.", "structure": "1. Introduction 2. Related Work 3. Preliminaries 4. RethinkMCTS 4.1 Overview 4.2 Selection 4.3 Expansion 4.4 Simulation 4.5 Evaluation 4.6 Backpropagation 4.7 Rethink 5. Experiments 6. Conclusion", "methodology": ["- Use Monte Carlo Tree Search (MCTS) to explore the thought process of code generation", "- Obtain block-level code analysis as verbal feedback through code execution", "- Introduce 'rethink' operation to refine erroneous thoughts based on verbal feedback", "- Employ dual evaluation using public test cases and LLM evaluation to assess code quality"], "example": "For the problem of checking if any two numbers in a list are closer than a given threshold, RethinkMCTS may explore thoughts like: 'Use a nested loop to compare each pair' -> 'Utilize a more efficient data structure like sorting'. If the nested loop approach fails test cases, the verbal feedback guides rethinking to the more efficient sorted approach."}
{"url": "http://arxiv.org/pdf/2409.03733v2", "keywords": ["natural language planning", "code generation search", "diverse idea exploration", "solution sketches", "combinatorial plan sampling"], "overall_summary": "The paper proposes a novel search algorithm called PlanSearch that improves large language model performance on code generation tasks by searching over diverse natural language plans and solution sketches before generating code. PlanSearch outperforms standard sampling methods by exploring a significantly more diverse space of potential solutions.", "structure": "1. Introduction 2. Related Work 3. Motivation 4. PlanSearch Algorithm 5. Experiments 6. Analysis 7. Conclusion", "methodology": ["Generate diverse first-order observations about the coding problem in natural language", "Combinatorially sample subsets of observations to form candidate solution plans", "Optionally generate second-order observations based on sampled plans", "Translate final sampled plans into natural language strategy descriptions", "Generate code by conditioning the language model on the strategy descriptions"], "example": "For the problem of finding the longest common subsequence between two strings: First-order observations: 'Use dynamic programming', 'Compare characters pairwise', 'Build up from smaller solutions' Sampled plan: 'Use dynamic programming', 'Compare characters pairwise' Second-order observation: 'Store results in 2D array to avoid recomputing' Strategy description: 'Use dynamic programming with a 2D array, comparing characters pairwise and building up the solution from smaller subproblems' Generated code: [Python code implementing dynamic programming solution]"}
{"url": "http://arxiv.org/pdf/2409.03267v1", "keywords": ["retrieval-augmented code generation", "test-driven program repair", "large language model integration", "automatic programming pipeline", "code search and reuse"], "overall_summary": "This paper proposes Cream, a novel framework that integrates code search, code generation, and program repair techniques leveraging large language models (LLMs) to advance automatic programming. The key idea is to emulate the real-world programming process by using code search to retrieve relevant code snippets, generating code candidates based on the retrieved code using LLMs, and refining the generated code through test-driven program repair with LLMs.", "structure": "1. Introduction - Motivation and overview of automatic programming challenges 2. Background & Related Work - Overview of code search, generation, repair techniques 3. The Cream Framework - Detailed description of the proposed 3-stage framework 4. Preliminary Evaluation - Experiments on programming benchmarks, case studies 5. Discussion & Future Work", "methodology": ["Use information retrieval or deep learning techniques to search for relevant code from databases", "Leverage LLMs to generate ranked code candidates based on requirements and retrieved code", "Construct dynamic prompts with test case feedback to query LLMs for iterative code refinement"], "example": "For the problem 'Write a Python function to count true Booleans in a list', Cream first retrieves similar code demonstrations like 'def solution(arr): ...'. It then uses an LLM to generate an initial solution 'def count(a): count = 0 \n for i in a: \n  if i == True: \n   count += 1'. When this fails a test case, Cream provides the failure trace to the LLM to generate a patch correcting the condition to 'if i:' instead of 'if i == True:'."}
{"url": "http://arxiv.org/pdf/2408.16198v1", "keywords": ["JavaScript application bundles", "Software Bill of Materials", "code segmentation", "code classification", "code clone retrieval", "multi-task learning"], "overall_summary": "This paper proposes Chain-of-Experts (CoE), a novel multi-task deep learning model to generate Software Bills of Materials (SBoMs) for JavaScript application bundles through code segmentation, classification, and clone retrieval. CoE offers an efficient end-to-end solution by jointly optimizing these tasks.", "structure": "1. Introduction 2. Problem Formulation 3. Chain-of-Experts Architecture Design 4. Experimental Setup 5. Results and Analysis 6. Deployment and Operational Analysis 7. Related Work 8. Conclusion", "methodology": ["Divides JavaScript bundles into sliding windows to handle long sequences", "Uses a chain of backbone and expert neural network models for multi-task learning", "Backbone models encode semantics and global context", "Expert models predict segment boundaries, code classes, and code embeddings", "Novel segmentation masking technique to prevent data overflow across segments", "Optimizes for segmentation, classification and clone retrieval tasks jointly"], "example": "For a JavaScript application bundle containing code like: \n\n<cls>!function(e,t){\"use strict\";\"object\"==typeof e...}\n\nCoE would segment it into windows, predict segment boundaries and classes (e.g. library, application), and retrieve matching library files from a code repository using embeddings and cosine similarity."}
{"url": "http://arxiv.org/pdf/2408.12159v1", "keywords": ["search-based optimization", "evolutionary search", "representative sample selection", "adaptive pattern retrieval", "genetic operator prompting"], "overall_summary": "This paper proposes SBLLM, a search-based framework that integrates large language models (LLMs) with evolutionary search techniques for automated code optimization. It enables iterative refinement and discovery of improved optimization methods through representative sample selection, adaptive pattern retrieval, and genetic operator-inspired prompting.", "structure": "1. Introduction 2. Proposed Framework a. Overview b. Execution-based Representative Sample Selection c. Adaptive Optimization Pattern Retrieval d. Genetic Operator-inspired Chain-of-thought Prompting 3. Evaluation 4. Conclusion", "methodology": ["Execution-based representative sample selection to evaluate fitness of existing optimized code and prioritize promising samples", "Adaptive optimization pattern retrieval to infuse targeted optimization patterns into the model for guiding LLMs", "Genetic operator-inspired chain-of-thought prompting to aid LLMs in combining different optimization methods"], "example": "For a Python code snippet using the Eratosthenes Sieve algorithm, SBLLM retrieves a similar pattern showing the correct initialization to rectify errors in the current optimized code. For a C++ code truncating a string, it retrieves a different pattern using substr() to find an unexploited optimization method."}
{"url": "http://arxiv.org/pdf/2408.11198v1", "keywords": ["evolutionary prompt engineering", "code generation", "test case evaluation", "cost-effective", "large language models"], "overall_summary": "The paper proposes EPiC, an evolutionary algorithm-based approach for cost-effective prompt engineering of large language models for code generation tasks. EPiC optimizes prompts by assessing the generated code against test case pass rates, outperforming state-of-the-art methods while requiring lower computational cost.", "structure": "1. Introduction 2. Background 3. Evolutionary Prompt Engineering for Code (EPiC) 4. Experimental Setup 5. Results and Discussion 6. Threats to Validity 7. Conclusion", "methodology": ["Two phases: Initial Evaluation (IE) and Evolutionary Prompt Engineering (EPE)", "IE: Generate code from initial prompt and evaluate against test cases", "EPE: If IE fails, evolve population of prompts using mutation and selection", "Mutations via LLM-guided prompts or word embedding replacements", "Selection based on test case pass rates as fitness function"], "example": "For the prompt 'Write a function to print the given string', the initial code generated an incorrect print statement. After mutation to 'Write a function to publish the given string', the expected return statement passing the test cases was produced."}
{"url": "http://arxiv.org/pdf/2408.09345v1", "keywords": ["naming-agnostic", "contrastive multi-view learning", "code search", "graph self-supervised learning", "abstract syntax tree modeling"], "overall_summary": "This paper proposes a naming-agnostic code search method (NACS) based on contrastive multi-view learning to overcome the challenge of different naming conventions in code. NACS strips variable name information from abstract syntax trees (ASTs) and uses contrastive learning on graph and path views to learn code representations robust to naming variations.", "structure": "1. Introduction 2. Related Work 2.1 Code Representation Learning 2.2 Code Search 2.3 Graph Self-Supervised Learning 3. Methodology 3.1 Preliminaries 3.2 Overview of NACS 3.3 Graph-View Modeling 3.4 Path-View Modeling 3.5 Multi-View Learning 3.6 Model Training 4. Experiments 5. Conclusion", "methodology": ["Strip variable name information from input ASTs to focus on capturing intrinsic AST structure properties", "Use semantic-level and syntax-level data augmentation to prepare realistic training data", "Adopt contrastive learning on the graph-view of ASTs to learn naming-agnostic code representations", "Model AST paths in a path-view to complement the graph-view representations", "Use multi-view learning to combine and mutually enhance the graph-view and path-view representations"], "example": "For the code snippets in Figure 1, NACS would strip the variable names 'lst', 'item', 'l', 'i' from the ASTs. It would then apply data augmentation like renaming variables to create positive/negative example pairs. The contrastive graph-view modeling would learn to map the augmented ASTs to similar representations despite naming differences. The path-view captures complementary structural information to further enhance the naming-agnostic representations."}
{"url": "http://arxiv.org/pdf/2408.06385v1", "keywords": ["virtual compiler", "assembly code search", "large language model", "code search dataset construction", "contrastive learning"], "overall_summary": "This paper introduces a novel approach called Virtual Compiler (ViC) that uses a large language model to emulate a general compiler and generate assembly code from source code across programming languages. ViC enables constructing large datasets for assembly code search, leading to substantial performance improvements over prior methods.", "structure": "1. Introduction to assembly code search and dataset challenges 2. Background on assembly code analysis, modeling, and code search 3. Overview of the ViC approach 4. Training ViC as a virtual compiler on compiled Ubuntu packages 5. Using ViC to construct assembly code search dataset and train encoder 6. Evaluation of ViC and assembly code search performance", "methodology": ["Compile over 6,000 C/C++ packages from Ubuntu using different compilers/versions/optimizations to obtain source-assembly pairs", "Pre-train CodeLlama model on this 20B token dataset to learn compiler behavior (ViC)", "Use ViC to virtually compile existing code search datasets to assembly code", "Train assembly code encoder on augmented dataset using contrastive learning"], "example": "For the bubble sort algorithm in C, ViC can take the source code as input and generate the corresponding assembly code, preserving semantic equivalence despite the loss of high-level structure like variable names and loops during compilation."}
{"url": "http://arxiv.org/pdf/2408.05542v2", "keywords": ["ChatGPT data augmentation", "code search", "prompt engineering", "data filtering", "representation learning"], "overall_summary": "The paper proposes ChatDANCE, a novel approach that leverages ChatGPT to generate high-quality augmented data for improving code search models. It uses carefully designed prompts to guide ChatGPT in rewriting code and queries, and employs a filtering mechanism to remove low-quality augmentations.", "structure": "1. Introduction 2. Related Work 3. ChatDANCE Framework 4. Experiments 5. Analysis 6. Conclusion", "methodology": ["Design prompts for ChatGPT to augment code and queries while preserving semantics", "Use a cross-encoder model to filter out low-quality augmented data", "Retrain a code search model (UniXcoder) on the filtered augmented dataset"], "example": "For code augmentation, the paper uses prompts to guide ChatGPT to rewrite code using 5 rewriting techniques like loop transformation and variable renaming. For example, the code 'def cumsum(inlist): newlist = copy.deepcopy(inlist) \n for i in range(1, len(newlist)): \n    newlist[i] = newlist[i] + newlist[i - 1] \n return newlist' is rewritten by ChatGPT as 'def cumsum(inlist): \n    newlist = [] \n    cum_sum = 0 \n    for i in inlist: \n        cum_sum += i \n        newlist.append(cum_sum) \n    return newlist'."}
{"url": "http://arxiv.org/pdf/2408.05344v1", "keywords": ["context retrieval", "code generation", "coding assistant", "evaluation", "in-context learning"], "overall_summary": "This paper discusses the importance of providing relevant context to large language models (LLMs) for code recommendation tasks in coding assistants. It outlines the key components of a context engine and the challenges in evaluating such AI-assisted coding systems.", "structure": "1. Introduction 2. Context Engine - Retrieval of context items - Ranking of context items 3. Evaluation - Online vs. offline evaluation - Lack of labeled data - Component-wise vs. end-to-end evaluation", "methodology": ["Use techniques like similarity matching, keyword search, semantic search, and code graph analysis to retrieve relevant context items", "Rank the retrieved context items based on relevance to the user's query", "Utilize domain-specific checks (e.g., syntax, semantics, test execution) for end-to-end evaluation", "Explore techniques like LLM judging for evaluating open-ended chat responses"], "example": "For code autocompletions, the system can perform syntactic checks (e.g., does the generated code parse?) and semantic checks (e.g., do the types match?) on the resulting code segment to prevent nonsensical outputs."}
{"url": "http://arxiv.org/pdf/2408.05026v1", "keywords": ["retrieval-augmented code completion", "in-context retrieval", "local project code", "small language models", "token healing"], "overall_summary": "The paper explores using smaller language models (around 160M parameters) for code completion by augmenting them with retrieval from local project files. It compares generative and retrieval-adapted models, and proposes an in-context retrieval approach based on token similarity.", "structure": "1. Introduction, 2. Related Work, 3. Code Line Completion Methods (RETRO, Token Healing, In-Context RAG), 4. Experimental Setup, 5. Results and Analysis, 6. Conclusion", "methodology": ["Trained 160M parameter GPT-2 and RETRO models on Python code", "Compared tokenizers and verified importance of token healing", "Implemented In-Context RAG with Jaccard token similarity retrieval", "Compared to RETRO with embedding retrieval and baselines", "Evaluated on code completion tasks using local project files"], "example": "For the input context 'import numpy as np\nnp.', the In-Context RAG method retrieves code snippets like 'np.array([1,2,3])' based on token similarity to the query 'np.'. These snippets are concatenated to the input to provide relevant context for the language model to predict the next token."}
{"url": "http://arxiv.org/pdf/2408.03623v1", "keywords": ["retrieval-augmented comment generation", "joint training", "dense retriever", "pre-trained code models", "weighted loss"], "overall_summary": "The paper proposes a novel approach called JointCom that jointly trains a retriever and a generator to improve retrieval-augmented code comment generation. By enabling the retriever to learn from the generator's feedback, it can retrieve exemplars that are more useful for generating high-quality comments.", "structure": "1. Introduction, 2. Preliminaries, 3. Approach (Overall Framework, Retrieval, Generation, Joint Training), 4. Experimental Setup, 5. Results and Analysis, 6. Human Evaluation, 7. Discussion, 8. Related Work, 9. Conclusion", "methodology": ["Uses a dense retriever based on a neural encoder to retrieve code-comment exemplars from a base", "Concatenates the input code with the top-k retrieved exemplars and their comments", "Calculates generation losses for each exemplar using a seq2seq model", "Computes a weighted sum of the losses using the retrieval scores as weights", "Jointly optimizes the retriever and generator to minimize this weighted loss"], "example": "For the code 'change private-browsing config to true and emit signal', using another relevant code as an exemplar helps the generator produce a better comment 'change private-browsing config to true and emit signal' compared to using a less relevant exemplar that generates 'test if cache is enabled after clearing it'."}
{"url": "http://arxiv.org/pdf/2408.11058v1", "keywords": ["agentic LLMs", "retrieval-augmented generation", "ensemble code search", "multi-stream comparisons", "context-aware code retrieval"], "overall_summary": "This paper introduces a novel approach to semantic code search using agentic large language models (LLMs) and retrieval-augmented generation (RAG) to enhance user queries with relevant context from GitHub repositories. It also proposes a multi-stream ensemble architecture for improved code retrieval accuracy, which is deployed in the RepoRift application.", "structure": "1. Introduction 2. Methodology 2.1 Information Injection via Agentic LLMs and RAG 2.2 Ensemble Architecture with Multi-Stream Comparisons 3. Experimental Setup 3.0.1 Dataset 3.0.2 Implementation Details 3.0.3 Evaluation Metrics 4. Results 5. Conclusion", "methodology": ["- Use agentic LLMs with RAG to augment user queries with relevant information from GitHub repositories", "- Employ an ensemble architecture with multi-stream comparisons:", "    - Stream 1: Compare query embedding with function embeddings", "    - Stream 2: Generate code, compare embeddings with functions and classes", "    - Stream 3: Compare component function embeddings with function embeddings"], "example": "For the query 'find a function that calculates the area of a circle', the agent could augment it with details like 'The area of a circle is calculated using the formula pi * r^2, where r is the radius. This is commonly implemented in Python using the math module to import the value of pi.' This augmented query can then be more accurately matched to relevant code snippets."}
{"url": "http://arxiv.org/pdf/2407.19619v1", "keywords": ["retrieval-augmented generation", "few-shot learning", "code translation", "Fortran-to-C++", "embedding models"], "overall_summary": "This paper proposes a novel approach to enhance code translation from Fortran to C++ by leveraging retrieval-augmented generation (RAG) and few-shot learning. By dynamically retrieving relevant code translation examples, the method provides contextual guidance to large language models, significantly improving translation accuracy.", "structure": "1. Introduction 2. Related Work 3. Methods 3.1 Dataset Preparation 3.2 Embedding Generation and Example Retrieval 3.3 Few-Shot Learning with RAG 4. Evaluation and Experimental Setup 5. Results and Discussion 6. Conclusion and Future Work", "methodology": ["- Maintain a repository of Fortran-C++ code translation pairs", "- Generate embeddings for code snippets using models like Nomic-Embed, Starencoder, CodeBERT", "- Retrieve top-k most relevant examples based on embedding similarity to the input code", "- Provide the retrieved examples and input code to a large language model for few-shot translation", "- Evaluate translations using CodeBLEU metric for syntactic and semantic correctness"], "example": "For the Fortran code snippet:\n\nSUBROUTINE EXAMPLE(A, B, C, N)\n  INTEGER N, I\n  REAL A(N), B(N), C(N)\n\n  DO I = 1, N\n    C(I) = A(I) + B(I)\n  END DO\n\nEND SUBROUTINE EXAMPLE\n\nThe RAG approach would:\n1. Generate an embedding for this code\n2. Retrieve top-k similar Fortran-C++ translation pairs from the repository\n3. Provide the Fortran code, retrieved pairs, and a prompt to the LLM\n4. The LLM generates the C++ translation, leveraging the provided context:\n\nvoid example(float* a, float* b, float* c, int n) {\n  for (int i = 0; i < n; i++) {\n    c[i] = a[i] + b[i];\n  }\n}"}
{"url": "http://arxiv.org/pdf/2407.21049v1", "keywords": ["multi-step key retrieval", "long-range dependency", "code generation models", "context window", "sliding window attention"], "overall_summary": "The paper proposes a suite of multi-step key retrieval tasks to evaluate the ability of code generation models to handle long-range dependencies in large context windows. It finds that performance degrades significantly when functions reference code defined later, and that sliding window attention struggles beyond the window size.", "structure": "1. Introduction - Motivates long context evaluation for code completion 2. Related Work - Covers symbolic reasoning, multi-hop QA, long context handling 3. Multi-Step Key Retrieval Tasks - Defines the proposed evaluation tasks 4. Experiments - Evaluates several models on the tasks 5. Results - Analyzes model performance by task difficulty, context size, snippet position", "methodology": ["Defines four multi-step key retrieval tasks of increasing difficulty (one-step, two-step, three-step, concatenation)", "Constructs long contexts by inserting task snippets and irrelevant code at varying positions", "Evaluates 5 code generation models (StarCoder, Mistral) on the tasks", "Measures accuracy@3 on generating the expected string literal", "Analyzes effects of task difficulty, context size, snippet positions, forward references"], "example": "For the two-step task with 5 distractors and 4k context: \n\n# irrelevant functions...\ndef value(): \n    return \"xdfgew\"\n# more irrelevant...\ndef key():\n    return value()\n\nassert key() == ? \n\nThe model must call value() from key() to retrieve the string \"xdfgew\" and complete the assert statement correctly."}
{"url": "http://arxiv.org/pdf/2407.02883v1", "keywords": ["code information retrieval", "code retrieval benchmark", "text-to-code retrieval", "code-to-text retrieval", "hybrid code retrieval"], "overall_summary": "This paper introduces COIR, a comprehensive benchmark for evaluating code information retrieval models across diverse tasks and domains. It consists of 10 datasets spanning 4 main retrieval tasks and 8 sub-tasks, covering various programming languages. The authors evaluate 9 retrieval models on COIR, revealing significant challenges in code retrieval.", "structure": "1. Introduction, 2. Related Work, 3. The COIR Benchmark (3.1 Desiderata, 3.2 Text-to-Code Retrieval, 3.3 Code-to-Text Retrieval, 3.4 Code-to-Code Retrieval, 3.5 Hybrid Code Retrieval), 4. Experiments, 5. The COIR Framework, 6. Conclusion", "methodology": ["Curated 10 datasets across 4 main code retrieval tasks: text-to-code, code-to-text, code-to-code, hybrid code retrieval", "Tasks cover 8 sub-tasks like code contest retrieval, web query code retrieval, code summary retrieval, similar code retrieval, etc.", "Datasets span 14 programming languages and diverse domains like GitHub, web queries, databases, contests, etc.", "Evaluated 9 retrieval models like DPR, Contriever, E5, GTE, BGE on COIR using standard IR metrics"], "example": "For the code context retrieval task, the authors modified the CodeSearchNet dataset by randomly dividing each code snippet into two segments - the initial segment as the query, and the remaining part as the target corpus to retrieve."}
{"url": "http://arxiv.org/pdf/2407.02742v1", "keywords": ["DSL code generation", "retrieval augmented generation", "few-shot learning", "API metadata grounding", "hallucination reduction"], "overall_summary": "This paper presents optimizations to retrieval augmented generation (RAG) techniques for improving the quality of natural language to domain-specific language (DSL) code generation, focusing on reducing hallucinations and syntax errors. The authors compare RAG approaches with fine-tuning on a DSL for automating API workflows.", "structure": "1. Introduction 2. Related Work 2.1 Code Generation 2.2 Reasoning and Tool Integration 2.3 Contributions 3. Methodology 3.1 Fine-Tuned NL2DSL Model 3.2 Grounding with Dynamically Selected Few-Shots 3.3 Grounding with API Metadata 4. Experiment Design and Metrics 4.1 Dataset Generation 4.2 DSL Generation Quality Metrics 5. Results 6. Conclusion and Future Work", "methodology": ["Fine-tuned a Codex model on a synthetic NL-DSL dataset for the target DSL", "Used retrieval augmented generation (RAG) with dynamically selected few-shot examples", "Fine-tuned a BERT model to improve few-shot retrieval based on target DSL similarity", "Grounded with API function definitions corresponding to few-shot examples", "Indexed API metadata to retrieve semantically relevant function definitions"], "example": "For the input query: \"When someone sends me an email with subject 'invoice', send me a Teams message to start an approval manually\", the generated DSL output is:\n\ntriggerOutputs = await shared_office365.OnNewEmailV3();\nif(triggerOutputs?['body']?['subject'] == 'invoice') {\n  teams_msg = shared_teams.PostMessageToConversation({\"message\": \"Start Approval process\"});\n}"}
{"url": "http://arxiv.org/pdf/2406.17553v1", "keywords": ["retrieval-augmented code generation", "situated action prediction", "Minecraft collaborative building", "few-shot prompting", "language model evaluation"], "overall_summary": "This paper explores using large language models (LLMs) for predicting the sequence of actions taken by a Builder agent in the Minecraft Collaborative Building Task, where an Architect provides instructions to assemble a structure. The authors frame this as a code generation task and leverage few-shot prompting techniques with LLMs, achieving improved performance over previous methods.", "structure": "1. Introduction 2. Related Work 3. Dataset 4. Experimental Setup - Few-shot Prompting - LLM Fine-tuning - Evaluation Metrics 5. Results & Analysis - Quantitative Results - Qualitative Analysis of Challenges 6. Conclusion", "methodology": ["Convert builder actions to code representations (place(), pick())", "Use few-shot prompting with dynamically adapted in-context examples", "Probe instruction-tuned LLMs like GPT-4, LLaMa to generate action sequences", "Fine-tune LLaMa-3-8b on the Minecraft dataset", "Evaluate using micro-averaged F1 score against ground truth actions"], "example": "For the instruction 'place an orange block to the diagonal top right of it', the model correctly interprets the spatial preposition 'diagonal top right' and generates: place(color='orange', x=1, y=2, z=1). However, for 'close, it should look like a ring', it struggles with 'close' and the anaphoric reference 'it', failing to generate the intended code."}
{"url": "http://arxiv.org/pdf/2406.14497v1", "keywords": ["retrieval-augmented code generation", "code retrieval benchmark", "document retrieval for code", "execution-based code evaluation", "open-domain code generation"], "overall_summary": "This paper introduces CodeRAG-Bench, a comprehensive benchmark for evaluating retrieval-augmented code generation (RACG) systems across diverse coding tasks and retrieval sources. The benchmark enables rigorous evaluation through ground-truth document annotations and execution-based testing of generated code.", "structure": "1. Introduction 2. The CodeRAG-Bench 2.1 Programming Problems 2.2 Retrieval Sources 2.3 Canonical Document Annotation 2.4 Evaluation Metrics 3. Canonical RACG: Experiments and Results 3.1 Experimental Setup 3.2 Retrieval Results 3.3 Code Generation Results 3.4 End-to-End RACG Results", "methodology": ["Curated 9k coding problems across 4 categories: basic programming, open-domain, repository-level, and code retrieval", "Collected 25M retrieval documents from 5 sources: solutions, tutorials, documentation, StackOverflow, GitHub", "Annotated ground-truth documents from corresponding sources for each problem", "Evaluated retrieval performance using NDCG@10", "Evaluated code generation using execution-based pass@k metric", "Tested multiple retrieval models (BM25, dense embeddings, proprietary APIs) and code generation models"], "example": "For the problem 'Pandas map multiple columns based on specific conditions' from the open-domain category, a relevant StackOverflow post was retrieved: \n\n# fill NaN values with old values\nout['nid'] = out['nid'].fillna(out['id'])\n\nThis post provides a code snippet demonstrating how to use pandas.fillna() to map values in one column based on conditions from another column, which is applicable to solving the problem."}
{"url": "http://arxiv.org/pdf/2406.11589v2", "keywords": ["code search benchmark", "query-code matching", "multi-choice code retrieval", "automated annotation", "large language model annotation"], "overall_summary": "This paper introduces CoSQA+, a new code search benchmark that pairs high-quality natural language queries with multiple relevant code snippets. It enhances the dataset construction process through automated labeling using large language models and code generation for unmatched queries. A new metric MMRR is proposed to evaluate one-to-many code search performance.", "structure": "1. Introduction 2. Related Work 3. CoSQA+ 3.1 Query and Code Collection 3.2 Candidate Pairs Construction 3.3 Model Annotation 3.4 Missing Code Generation 4. Experiments 5. Conclusion", "methodology": ["Collect queries from CoSQA and code snippets from CodeSearchNet and StaQC", "Calculate embedding similarity of query-code pairs using multiple models (CodeBERT, UniXcoder, CodeT5+)", "Select top candidate pairs for annotation", "Use Claude 3 Sonnet with Chain-of-Thought prompting to automatically annotate pairs", "Use GPT-4o to generate missing codes for unmatched queries", "Propose new metric Mean Multi-choice Reciprocal Rank (MMRR) for one-to-N code search evaluation"], "example": "For the query 'python how to check if query dict is empty', the annotated matching code is: \n\ndef _is_empty(cls, value):\n    if isinstance(value, (dict, tuple, list)) and len(value) == 0:\n        ret = True\n    else:\n        ret = False\n    return ret"}
{"url": "http://arxiv.org/pdf/2406.07003v2", "keywords": ["code context graph", "graph-based code retrieval", "decay-with-distance similarity", "repository-level code completion", "retrieval-augmented generation"], "overall_summary": "GraphCoder is a framework that enhances repository-level code completion by leveraging a code context graph representation and graph-based retrieval to augment language models with repository-specific knowledge. It achieves higher accuracy while being more efficient compared to baseline methods.", "structure": "1. Introduction 2. Related Work 3. Basic Concepts (code context graph, CCG slicing) 4. GraphCoder 4.1 Overview 4.2 Database Construction 4.3 Code Retrieval 4.4 Code Generation", "methodology": ["- Represents code context using a code context graph (CCG) capturing control flow, data dependence, and control dependence", "- Performs coarse-grained retrieval based on context sequence similarity", "- Re-ranks results using fine-grained decay-with-distance structural similarity between CCGs", "- Generates code by prompting a language model with the retrieved context-similar code snippets"], "example": "For the statement 'cur_key_val = cur_key_val + cross_cur_key_val', its 1-hop CCG slice includes statements it has data/control dependence on (lines 2, 12, 13), its 1-hop control-flow neighbor (line 12) and that neighbor's dependences (lines 7-9). This structured slice is used to retrieve relevant snippets from the database."}
{"url": "http://arxiv.org/pdf/2406.10263v1", "keywords": ["adaptive retrieval", "critique model", "uncertainty estimation", "code completion", "retrieval-augmented generation"], "overall_summary": "The paper introduces CARD, a lightweight framework that enhances the efficiency and effectiveness of retrieval-augmented generation (RAG) for code completion tasks. It provides two key functions: isRetrieve to determine if retrieval is necessary, and Select to choose the best prediction among multiple candidates.", "structure": "1. Introduction - Motivation and overview of CARD 2. Methodology - Details on uncertainty estimation, adaptive retrieval, and selective accept functions 3. Experimental Setup - Datasets, evaluation metrics, target RAG system, training details 4. Results and Analysis - Evaluation of CARD on code completion benchmarks 5. Related Work 6. Conclusion", "methodology": ["- Use uncertainty estimation model (Estimator) to score code predictions based on entropy and probability features", "- isRetrieve function decides if retrieval is needed based on Estimator score and threshold", "- Select function chooses best prediction among candidates based on Estimator scores", "- CARD can be integrated into any RAG-based code completion system, for single or iterative RAG"], "example": "For the line completion task on the RepoEval dataset using CodeLlama-7B, CARD saves 21-46% of retrievals while improving accuracy over the baseline RAG system. It reduces latency by 16-83% across different tasks and models."}
{"url": "http://arxiv.org/pdf/2406.04464v1", "keywords": ["context retrieval", "repository-level code editing", "reasoning", "code structure-aware tools", "iterative retrieval"], "overall_summary": "This paper investigates the role of reasoning and code structure-aware tools in improving context retrieval for repository-level code editing tasks. The authors conduct experiments decoupling context retrieval from other components to analyze its strengths and weaknesses.", "structure": "1. Introduction 2. Related Work 3. Experiments & Results 3.1 Models 3.2 Datasets 3.3 Context Retrieval Strategies 3.4 Metrics 3.5 Results & Discussion 4. Conclusion Limitations", "methodology": ["Used GPT-3.5 Turbo LLM for experiments", "Evaluated on SWE-bench Lite and LCA Code Editing datasets", "Tested context retrieval strategies varying in reasoning complexity (baseline, Context Length, Tool Call, Self-Reflection) and tools (BM25, code structure-aware tools)", "Measured precision, recall, and F1 score at file and code entity levels"], "example": "For the LCA Code Editing dataset, using the AutoCodeRover strategy with code structure-aware tools and Self-Reflection reasoning achieved 49.6% precision and 31.4% recall at the file level, outperforming simpler strategies like the BM25 baseline (25.3% precision, 25.3% recall)."}
{"url": "http://arxiv.org/pdf/2405.19782v1", "keywords": ["dataflow analysis", "type-sensitive relations", "repo-specific context graph", "fine-grained import information", "prompt generation"], "overall_summary": "This paper proposes DRACO, a dataflow-guided retrieval augmentation approach for repository-level code completion. It leverages extended dataflow analysis to precisely retrieve relevant background knowledge from a repo-specific context graph and generate well-formed prompts to query large language models.", "structure": "1. Introduction 2. Related Work 3. Methodology 3.1 Dataflow Analysis 3.2 Repo-specific Context Graph 3.3 Dataflow-Guided Retrieval 3.4 Prompt Generation 4. Experiment Setup 4.1 Datasets 4.2 Evaluation Metrics 4.3 Baselines 4.4 Language Models 5. Results and Analysis 6. Conclusion", "methodology": ["- Extend traditional dataflow analysis by setting type-sensitive dependency relations (assigns, as, refers, typeof, inherits)", "- Parse repository into code entities (modules, classes, functions, variables) and establish relations through dataflow analysis to form a repo-specific context graph", "- Identify fine-grained import information from unfinished code using dataflow analysis", "- Retrieve relevant code entities from context graph based on import information", "- Restore retrieved entities to source code and concatenate with unfinished code to generate prompts for language models"], "example": "Given the unfinished code:\n\nnewSignal = signal.getSignalByName(newChannelName)\nnewSignal.channel = newChannelName\nsetSignalTypeFromTypeStr() \n\nDRACO retrieves the relevant background knowledge:\n\n1) The definition of Signal class showing setSignalTypeFromTypeStr method\n2) The definition of RecordSignal class and getSignalByName method returning Signal object\n\nThis provides the model context that newSignal is a Signal object, so it can complete the line:\nnewSignal.setSignalTypeFromTypeStr()"}
{"url": "http://arxiv.org/pdf/2405.19093v1", "keywords": ["multi-stage retrieval", "contrastive re-ranking", "code co-occurrence", "auxiliary knowledge", "ICD coding recommendation"], "overall_summary": "This paper proposes a novel multi-stage 'retrieve and re-rank' framework for automatic medical coding recommendation from clinical notes. It first retrieves a candidate subset of ICD codes using auxiliary knowledge and BM25, then re-ranks the candidates via contrastive learning guided by code co-occurrences.", "structure": "1. Introduction 2. Related Work 3. Method: 3.1 Multi-stage Framework Overview, 3.2 Retrieval Stage, 3.3 Re-ranking Stage, 3.4 Training 4. Experiments 5. Results 6. Conclusion", "methodology": ["Two-stage retrieval: 1) Use auxiliary knowledge (DRG, CPT codes, medications) and conditional probabilities to retrieve candidate ICD subset. 2) Further reduce candidates using BM25 lexical matching.", "Re-ranking via contrastive learning: 1) Encode clinical text with Clinical-Longformer. 2) Encode ICD codes with Graphormer using code co-occurrence graph. 3) Contrastive loss to pull positive ICD codes closer to clinical text."], "example": "For the clinical note about Alzheimer's and dementia in Figure 1, the model first retrieves candidates like 294.10 (dementia) using auxiliary knowledge of prescribed 'Namenda' drug. It then re-ranks 331.0 (Alzheimer's) higher using the code co-occurrence between dementia and Alzheimer's codes."}
{"url": "http://arxiv.org/pdf/2405.16337v3", "keywords": ["program generation", "program emulation", "pseudo-programs", "code search", "task adaptation"], "overall_summary": "The paper proposes COGEX, a novel approach that trains language models to generate and emulate the execution of pseudo-programs containing underspecified functions to tackle both algorithmic and soft reasoning tasks. It also introduces COTACS, a program search method to find an optimal program for a given dataset without updating model parameters.", "structure": "1. Introduction 2. Approach 2.1 COGEX Formulation 2.2 Program Search: COTACS 3. Experiments and Results 3.1 Experimental Setup 3.2 Main Results 3.3 Ablation Studies", "methodology": ["Train language models to generate Python programs from natural language instructions", "Allow programs to include underspecified functions whose implementations are filled in by the model's knowledge during emulated execution", "Use COTACS program search to find an optimal program for a given dataset by evaluating many candidate programs", "Fine-tune models on a COGEX dataset derived from the Alpaca instruction tuning dataset"], "example": "For the task of pluralizing a word, the model generates the following program:\n\ndef pluralize_word(word):\n    # 1. Identify the suffix of the word\n    ending = identify_ending(word)\n    # 2. Identify the pluralization rule for the word \n    pluralization_rule = find_pluralization_rule(word, ending)\n    # 3. Apply the pluralization rule to the word\n    plural_form = apply_pluralization_rule(word, ending)\n    return {'original_word': word, 'ending': ending, 'rule': pluralization_rule, 'answer': plural_form}\n\nThe model then emulates the execution of this program, filling in the semantics of the underspecified identify_ending, find_pluralization_rule, and apply_pluralization_rule functions using its knowledge."}
{"url": "http://arxiv.org/pdf/2405.15383v2", "keywords": ["Code World Models", "Monte Carlo Tree Search", "Program Synthesis", "Model-Based Reinforcement Learning", "Large Language Models"], "overall_summary": "This paper introduces Code World Models, a novel approach to generate world models for reinforcement learning as executable Python code using large language models. It proposes Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy tailored for this task. The authors also introduce the Code World Models Benchmark for evaluating such methods.", "structure": {"1. Introduction": "Motivates using code for world modeling and introduces Code World Models", "2. Related Work": "Discusses prior work on world models with code and code generation with LLMs", "3. Code World Models": "Formalizes the problem and introduces the Code World Models Benchmark", "4. GIF-MCTS": "Presents the proposed GIF-MCTS algorithm for code generation", "5. Experiments": "Evaluates GIF-MCTS on coding benchmarks and using the generated CWMs for planning", "6. Discussion": "Analyzes limitations and future directions"}, "methodology": ["Formulates generating a Code World Model as writing a Python class implementing the environment dynamics", "Proposes GIF-MCTS, a Monte Carlo Tree Search based approach to iteratively generate, improve and fix code using an LLM", "Leverages LLM prompting, self-reflection, and feedback from environment trajectories as unit tests", "Introduces the Code World Models Benchmark with 18 RL environments and associated data"], "example": "For the CartPole environment, GIF-MCTS could generate a CWM like:\n\nimport numpy as np\n\nclass Environment:\n    def __init__(self):\n        self.cart_position = np.random.uniform(-0.2, 0.2)\n        self.cart_velocity = np.random.uniform(-0.2, 0.2)\n        self.pole_angle = np.random.uniform(-0.1, 0.1) \n        self.pole_velocity = np.random.uniform(-0.1, 0.1)\n\n    def step(self, action):\n        # Implement CartPole dynamics\n        ...\n        return next_state, reward, done"}
{"url": "http://arxiv.org/pdf/2405.11305v1", "keywords": ["answer set programming", "large neighborhood prioritized search", "combinatorial optimization", "heuristic-driven search", "multi-shot solving"], "overall_summary": "The paper proposes a new metaheuristic called Large Neighborhood Prioritized Search (LNPS) for solving combinatorial optimization problems using Answer Set Programming (ASP). LNPS alternates between destroying parts of the current solution and reconstructing it via prioritized search, allowing more flexible exploration compared to traditional Large Neighborhood Search.", "structure": "1. Introduction 2. Background on ASP 3. Large Neighborhood Prioritized Search algorithm 4. heulingo: ASP-based implementation of LNPS 5. Experimental evaluation 6. Conclusion", "methodology": ["- Defines LNPS as an iterative algorithm that starts with an initial solution and alternates between destroying parts of the current solution (destroy operator) and reconstructing it via prioritized systematic search (prioritized-search operator)", "- Implements LNPS using ASP by leveraging multi-shot solving and heuristic statements in clingo", "- Uses #heuristic statements to configure the prioritized search based on remaining undestroyed parts", "- Supports traditional LNS as a special case when undestroyed parts are fixed"], "example": "For the Traveling Salesperson Problem (TSP), LNPS could start with an initial Hamiltonian cycle and iteratively destroy and reconstruct parts of the cycle guided by heuristics like keeping high priority edges intact. The prioritized search focuses on reconstructing the destroyed parts optimally while reusing the undestroyed high priority edges."}
{"url": "http://arxiv.org/pdf/2405.07530v1", "keywords": ["prompt-based multi-retrieval", "adaptive retrieval selection", "contextual multi-armed bandits", "code semantics perspectives", "retrieval augmented generation"], "overall_summary": "This paper proposes ProCC, a novel code completion framework that leverages prompt engineering and contextual multi-armed bandits to flexibly incorporate and adapt to multiple perspectives of code semantics for retrieval augmented generation.", "structure": "1. Introduction 2. Background and Motivation 2.1 Code Completion 2.2 Retrieval-Augmented Generation 2.3 Motivation 3. ProCC Framework 3.1 Prompt-based Multi-Retriever System 3.2 Adaptive Retrieval Selection Algorithm 4. Experiments 4.1 Experimental Setup 4.2 Overall Results 4.3 Component Analysis 4.4 Case Studies 5. Related Work 6. Conclusion", "methodology": ["Prompt-based Multi-Retriever System: Crafts prompts to elicit LLM knowledge for understanding code semantics from multiple perspectives (lexical semantics, hypothetical line, code summarization)", "Adaptive Retrieval Selection Algorithm: Employs contextual multi-armed bandits to determine the most suitable retrieval perspective based on similarity between retrieved snippets and incomplete code"], "example": "For the incomplete code snippet: 'private void addSSLConfiguration(ClientBuilder clientBuilder) {...}', the lexical semantics prompt retrieves relevant code like 'clientBuilder.sslContext(SSLContext.getDefault());', while the hypothetical line prompt generates 'clientBuilder.sslContext(sslContext);' as a potential completion. The adaptive algorithm then selects the hypothetical line perspective as more relevant for completing the SSL configuration."}
{"url": "http://arxiv.org/pdf/2405.04126v1", "keywords": ["code retrieval", "contrastive learning", "CodeT5+", "parameter-efficient fine-tuning", "bimodal embeddings"], "overall_summary": "The paper proposes a fine-tuning framework that leverages parameter-efficient fine-tuning (PEFT) techniques and contrastive learning to improve the quality of bimodal text-code representations learned by the CodeT5+ model for code retrieval tasks. The approach tunes only a small fraction of model parameters while achieving performance comparable to larger models.", "structure": "1. Introduction - Motivation and background 2. Related Work - Prior work on code retrieval, PEFT, and contrastive learning 3. Methodology - Proposed fine-tuning approach with contrastive loss and PEFT methods 4. Experimental Setup - Datasets, implementation details 5. Results and Discussion - Performance evaluation and analysis 6. Conclusion and Future Work", "methodology": ["- Use contrastive loss to align representations of matching code-text pairs in a joint feature space", "- Fine-tune CodeT5+ encoder using PEFT methods like LoRA, AdaLoRA, (IA)3, and Prompt-Tuning", "- Train a small number of newly added parameters while freezing pre-trained weights", "- Perform fine-tuning separately for each programming language"], "example": "For the Python programming language, the approach learns to map the natural language description 'function to calculate the factorial of a number' to the corresponding Python code snippet:\n\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)"}
{"url": "http://arxiv.org/pdf/2405.02355v3", "keywords": ["graphical code representation", "control flow graphs", "data flow graphs", "code structure understanding", "cross-lingual code generation"], "overall_summary": "The paper proposes CodeGRAG, a framework that enhances large language models for code generation by representing code blocks as graphical views capturing control flow and data flow. This graphical representation bridges the gap between natural language and programming languages, enabling better code understanding and cross-lingual generation.", "structure": "1. Introduction - Motivation and challenges 2. Methodology 2.1 Overview 2.2 Graphical Knowledge Base Preparation 2.3 Knowledge Querying 2.4 Graphical Knowledge Augmented Generation 2.4.1 Hard Meta-Graph Prompt 2.4.2 Soft Prompting with Expert", "methodology": ["- Extract graphical views of code blocks by combining data flow graphs and control flow graphs with read-write signals", "- Build a knowledge base of code blocks and their graphical views", "- Query the knowledge base using natural language prompts to retrieve relevant graphical views", "- Use hard meta-graph prompts with the retrieved views to stimulate LLMs for code generation", "- Use soft prompting by finetuning LLMs with a graph neural network expert model encoding the graphical views"], "example": "For the task of generating a function to perform XOR on two binary strings, CodeGRAG would: 1) Extract the graphical view capturing the control flow (loops, conditionals) and data flow of the XOR operation, 2) Retrieve this view from the knowledge base based on the task description, 3) Either use the meta-graph prompt containing the topology of this view to prompt an LLM, or finetune the LLM with the graph embedding from the expert GNN model to inject the graphical knowledge."}
{"url": "http://arxiv.org/pdf/2404.16565v1", "keywords": ["source code provenance", "package repository linking", "phantom file analysis", "metadata-based retrieval", "source code-based retrieval"], "overall_summary": "This paper proposes PyRadar, a novel framework that utilizes package metadata and source code to automatically retrieve and validate the source code repository information for Python packages on PyPI. It addresses limitations of existing metadata-based tools by incorporating a validator for incorrect metadata and a retriever using source code when metadata is missing.", "structure": "1. Introduction 2. Background 3. Data Collection 4. Empirical Study 4.1 RQ1: Analyzing Existing Metadata-based Retrieval Tools 4.2 RQ2: Analyzing Phantom File Differences 5. PyRadar Framework 5.1 Metadata-based Retriever 5.2 Source Code Repository Validator 5.3 Source Code-based Retriever 6. Evaluation 7. Discussion 8. Conclusion", "methodology": ["Conducted large-scale empirical study on 4.2M PyPI releases to analyze existing metadata-based tools (RQ1) and phantom file differences between correct/incorrect links (RQ2)", "Proposed heuristic approach to collect 14,375 correct and 2,064 incorrect package-repository links", "Designed PyRadar framework with 3 components: - Metadata-based Retriever (combines best practices from existing tools) - Source Code Repository Validator (uses crafted features and ML models) - Source Code-based Retriever (queries World of Code with file hashes)"], "example": "For the Python package 'requests', PyRadar successfully retrieved its source code repository on GitHub (https://github.com/psf/requests) in the following way: 1. Metadata-based Retriever extracted the URL from metadata 2. Source Code Repository Validator confirmed it was correct based on low phantom file count 3. Source Code-based Retriever also retrieved the same URL by matching file hashes in the source distribution to the requests repository"}
{"url": "http://arxiv.org/pdf/2404.04688v1", "keywords": ["Stateflow model repair", "Cyber-Physical Systems", "Simulink", "Search-based program repair", "Automated fault localization"], "overall_summary": "The paper proposes FlowRepair, a search-based automated program repair approach specifically designed to fix bugs in Stateflow models that control Cyber-Physical Systems modeled in Simulink. It introduces a novel repair algorithm combining global and local search, new repair objectives tailored for CPS, and mutation operators for Stateflow models.", "structure": {"1. Introduction": "Motivates the need for automated repair of Simulink/Stateflow models and outlines the key challenges addressed.", "2. Background": "Provides background on automated program repair and Stateflow models.", "3. FlowRepair": "Describes the proposed FlowRepair approach, including the repair algorithm, objectives, and mutation operators.", "4. Empirical Evaluation": "Presents the evaluation of FlowRepair on case study systems and faulty Stateflow models.", "5. Results": "Discusses the results of the empirical evaluation.", "6. Threats to Validity": "Examines potential threats to the validity of the study.", "7. Related Work": "Positions the work with respect to relevant literature.", "8. Conclusion": "Summarizes the key contributions and findings."}, "methodology": ["- Employs spectrum-based fault localization to narrow the search space for repairs", "- Combines global search (to explore different patch types) and local search (to exploit promising partial patches)", "- Defines novel repair objectives tailored for CPS, considering time, failure severity, etc.", "- Develops mutation operators specifically for repairing Stateflow models"], "example": "The paper evaluates FlowRepair on a Stateflow model controlling the temperature of a fridge. One fault involved the system not triggering an alarm when the door was left open for 15 seconds. FlowRepair was able to generate a patch that correctly activated the alarm in this scenario."}
{"url": "http://arxiv.org/pdf/2404.02319v2", "keywords": ["symbolic prompt programs", "structure-aware optimization", "compile-time prompt tuning", "retrieval augmented generation", "prompt compression"], "overall_summary": "This paper introduces SAMMO, a framework for optimizing complex prompt programs at compile-time through symbolic representations and structure-aware mutations. SAMMO generalizes and improves upon previous prompt optimization methods.", "structure": "1. Introduction - Motivates compile-time prompt program optimization 2. Symbolic Prompt Programs - Defines representation and execution model 3. Compile-Time Prompt Optimization - Formalizes the optimization problem 4. SAMMO Framework - Describes search algorithms and mutation operators 5. Experiments - Evaluates on instruction tuning, RAG pipeline tuning, prompt compression", "methodology": ["Represents prompt programs as symbolic prompt programs (SPPs) - directed acyclic graphs where nodes are functions", "Defines mutation operators that can modify text, attributes, and program structure of SPPs", "Implements enumerative search (grid, random) and iterative search (beam search) over SPP space", "Specializes to methods like APE, GrIPS by choosing appropriate initialization and mutations"], "example": "For a binary classification task, an SPP could have nodes for rendering instructions, labels, generating a response, and parsing the output. Mutations may change the text format, remove instructions, or modify the overall structure."}
{"url": "http://arxiv.org/pdf/2404.01554v1", "keywords": ["retrieval-augmented code completion", "fine-tuning mimicry", "logits discrepancy", "iterative retrieval", "token-level and line-level completion"], "overall_summary": "The paper proposes FT2Ra, a novel retrieval-augmented method for code completion that aims to mimic the effects of fine-tuning without actually fine-tuning the model. It is based on a theoretical analysis that highlights the importance of logits discrepancy (logits) in improving model predictions.", "structure": "1. Introduction 2. Background and Problem 3. Approach 3.1 Inspiration from Fine-tuning 3.2 FT2Ra Method 4. Experimental Setup 5. Results and Analysis 6. Related Work 7. Conclusion", "methodology": ["- Theoretical analysis of fine-tuning to derive insights on leveraging retrieved information", "- Approximating logits (change in logits after fine-tuning) without actual fine-tuning", "- Retrieving nearest neighbors and interpolating their logits with model predictions", "- Iterative retrieval process mimicking multi-epoch fine-tuning"], "example": "For token-level code completion on the UniXcoder dataset, FT2Ra achieves 74.22% accuracy, a 4.29% improvement over the best baseline kNM-LM (69.93%). For the more challenging line-level completion, FT2Ra obtains 26.32 Exact Match score, around 2x better than kNM-LM (13.93)."}
{"url": "http://arxiv.org/pdf/2403.16702v1", "keywords": ["Community QA Dataset", "Code-Mixing Data", "Modality-Agnostic Pretraining", "Contrastive Code Representation", "Large-Scale Code Search"], "overall_summary": "This paper introduces ProCQA, a large-scale programming question answering dataset extracted from StackOverflow. It also proposes a modality-agnostic contrastive pretraining approach that leverages the mixed-modal nature of ProCQA to improve code-text representation alignment, achieving state-of-the-art performance on various code retrieval benchmarks.", "structure": {"1. Introduction": "Motivates the need for a large, diverse code QA dataset and modality-agnostic pretraining", "2. Related Work": "Discusses prior work on code QA datasets and language models", "3. ProCQA": "Details the creation, filtering, statistics, and tasks of the proposed ProCQA dataset", "4. Experiments": "Evaluates the modality-agnostic pretraining approach on code retrieval benchmarks", "5. Analysis": "Analyzes the pretraining approach through ablations and visualizations", "6. Conclusion": "Summarizes key contributions and findings"}, "methodology": ["Crawled StackOverflow to create ProCQA, a 5M mixed-modal QA dataset across 11 programming languages", "Applied rule-based filtering to ensure data quality and fairness", "Proposed modality-agnostic contrastive pretraining (MACP) on ProCQA to align code-text representations", "Evaluated MACP on code retrieval tasks: supervised, zero-shot, out-of-domain"], "example": "An example QA pair from the C programming subset: The question describes a segmentation fault issue when copying a string. The answer explains the root cause is allocating the wrong data type size for the character array."}
{"url": "http://arxiv.org/pdf/2403.13583v3", "keywords": ["online search", "query planning", "test case generation", "code refinement", "complex code generation"], "overall_summary": "The paper introduces CoCoST, a framework that enhances complex code generation by leveraging online search, query planning, test case generation, and correctness-driven code refinement. It aims to address challenges in generating intricate code structures by imitating the human developer workflow.", "structure": "1. Introduction 2. Related Work 3. Methodology (3.1 Retrieval, 3.2 Refinement) 4. Experiments 5. Analysis 6. Conclusion", "methodology": ["Uses LLMs to plan steps and generate queries for online search", "Conducts online search using the generated queries to retrieve relevant information", "Generates initial code using LLM with the retrieved information", "Generates test cases for the initial code using LLM", "Executes code with test cases, serializes inputs/outputs", "Refines code based on execution errors and output correctness using LLM"], "example": "For a problem of calculating value counts per column in a Pandas DataFrame, CoCoST: 1) Plans steps like 'iterate over columns', 'calculate value counts' 2) Searches online for 'pandas calculate value counts for each column' 3) Generates initial code using search results 4) Generates test cases with sample DataFrames 5) Executes code, serializes DataFrame inputs/outputs 6) Refines code based on correctness of value count results"}
{"url": "http://arxiv.org/pdf/2403.10720v1", "keywords": ["Da Vinci Code game", "branch divergence", "MCTS parallelization", "GPU performance bottlenecks", "non-linear scaling"], "overall_summary": "This paper investigates the performance impact of branch divergence on parallelizing the Monte Carlo Tree Search (MCTS) algorithm for the Da Vinci Code board game using GPUs. The authors find that the game's mechanics lead to significant branch divergence, resulting in non-linear scaling and performance degradation on GPUs.", "structure": "1. Introduction 2. Background 2.1 Da Vinci Code game rules 2.2 Monte Carlo Tree Search algorithm 3. Method 3.1 Designing simplified MCTS algorithm 3.2 CPU implementation (MCTS-CPU) 3.3 GPU implementation (MCTS-GPU) 4. Evaluation 4.1 Execution time analysis 4.2 Simulations per second analysis (CPU vs GPU) 5. Conclusion", "methodology": ["Developed two MCTS variants: CPU-based (MCTS-CPU) using OpenMP and GPU-based (MCTS-GPU) using CUDA", "Simplified MCTS algorithm by removing plausible number sets, disallowing consecutive guesses, and limiting tree expansion depth", "Parallelized with each thread running independent game simulations and contributing to a consolidated MCTS", "Measured execution time and simulations per second on CPU and GPU to analyze performance bottlenecks"], "example": "For the Da Vinci Code game, the authors found that on the CPU, increasing the number of threads led to a linear increase in simulations per second up to the number of physical cores. However, on the GPU, they observed non-linear scaling and performance valleys due to branch divergence from the game's guessing mechanics and resulting memory bottlenecks from cache misses and thread waiting."}
{"url": "http://arxiv.org/pdf/2403.10059v2", "keywords": ["selective retrieval", "self-assessment", "repository-level code completion", "retrieval augmentation", "self-supervised learning"], "overall_summary": "This paper proposes REPOFORMER, a framework for selective retrieval-augmented code completion that avoids unnecessary or detrimental retrievals. A code language model is trained to self-evaluate whether retrieval can improve its predictions and robustly leverage retrieved contexts when needed.", "structure": "1. Introduction, 2. Related Work, 3. Approach (Problem Formulation, Self-Selective RAG, Self-Supervised Learning), 4. Experimental Setup, 5. Results, 6. Analysis, 7. Conclusion", "methodology": ["Trains a code language model to self-trigger cross-file retrieval by generating a special token or abstaining", "Uses self-supervised multi-task learning on public repositories to enable accurate self-assessment and robustness to retrieved contexts", "Contrasts model outputs with and without retrieval to obtain self-supervision labels", "Optimizes for joint losses on self-assessment accuracy and code generation quality"], "example": "For the code snippet:\n\nimport pandas as pd\n\nclass TableManager:\n    def __init__(self, data):\n        self.data = pd.DataFrame(data)\n\n    def normalize_col(self, col):\n        \"\"\"Normalize the values in col to the range [0, 1].\"\"\"\n        <blank>\n\nThe model first self-evaluates whether retrieval is needed to complete the normalize_col function body. If not, it directly generates the completion using only in-file context. If retrieval is triggered, it retrieves relevant cross-file contexts like other data normalization functions and leverages them to generate the completion."}
{"url": "http://arxiv.org/pdf/2403.06095v4", "keywords": ["repository-level code completion", "semantic graph representation", "context retrieval", "link prediction", "graph neural networks"], "overall_summary": "The paper introduces RepoHYPER, a novel framework for repository-level code completion that leverages a semantic graph representation and graph-based retrieval methods to effectively identify and prioritize relevant code contexts from the entire codebase. It demonstrates significant improvements over existing techniques.", "structure": "1. Introduction 2. Related Work 3. Methodology: Repo-level Semantic Graph, Search-Expand-Refine Retrieval 4. Empirical Evaluation 5. Conclusion", "methodology": ["- Repo-level Semantic Graph (RSG): Graph representation capturing global repository context (functions, classes, scripts, imports, invocations, etc.)", "- Search-then-Expand Strategies: kNN search to find anchor nodes, then expand to related nodes via BFS or pattern-based exploration", "- Re-ranking as Link Prediction: Treat context re-ranking as link prediction on RSG using graph neural networks to score relevance"], "example": "For a code snippet needing to call get_similarity_metric(), similarity search focuses on MultiLabelInversionModel due to form similarity, leading to incorrect completion. RepoHYPER first identifies the most similar inversion_bert.py via kNN search, then expands to inversion_albert.py containing get_similarity_metric via Import relation, enabling correct completion."}
{"url": "http://arxiv.org/pdf/2403.01364v1", "keywords": ["code-switching", "cross-lingual semantic retrieval", "alternative pre-training", "FAQ systems", "e-commerce"], "overall_summary": "This paper proposes a novel alternative cross-lingual pre-training approach using code-switching to improve semantic retrieval performance for FAQ systems in e-commerce scenarios. By incorporating code-switched data and a similarity loss during pre-training, the model learns better cross-lingual representations tailored for the semantic retrieval task.", "structure": "1. Introduction - Motivation and background 2. Preliminaries - Masked language modeling, cross-lingual pretraining, semantic retrieval, code-switching 3. Method - Alternative cross-lingual PTM architecture, building code-switched data 4. Experiments - Setup, results on business corpora, open datasets, analysis 5. Conclusion", "methodology": ["Generate code-switched data by replacing tokens with other languages using bilingual dictionaries", "Pre-train model on code-switched data using combined loss: cross-lingual masked language modeling (XMLM) + similarity loss between query and label representations", "Fine-tune pre-trained model on semantic retrieval corpus"], "example": "For the query 'My delivery status says failed', the model may code-switch some words to other languages like 'My  says  delivery '. During pre-training, it learns to predict the masked words while also bringing the query and label representations closer based on the similarity loss."}
{"url": "http://arxiv.org/pdf/2403.00865v1", "keywords": ["symbolic loss functions", "genetic programming", "unrolled differentiation", "neuro-symbolic optimization", "task-agnostic meta-learning"], "overall_summary": "The paper proposes a hybrid neuro-symbolic approach called EvoMAL for learning task and model-agnostic symbolic loss functions via genetic programming and gradient-based local search. The learned loss functions improve convergence, sample efficiency, and inference performance across various supervised learning tasks.", "structure": "1. Introduction 2. Background and Related Work 2.1 Loss Function Learning 2.2 Gradient-Based Approaches 2.3 Evolution-Based Approaches 3. Proposed Method 3.1 Learning Symbolic Loss Functions 3.1.1 Search Space Design 3.1.2 Outer Search Algorithm Design 3.1.3 Constraint Enforcement 3.1.4 Loss Archival Strategy 3.2 Loss Function Optimization and Evaluation 3.3 Computational Complexity Analysis", "methodology": ["- Use genetic programming (GP) to search for symbolic loss functions in the outer optimization loop", "- Design a task and model-agnostic search space of mathematical operations for GP", "- Convert symbolic loss functions to trainable neural networks (meta-loss networks)", "- Optimize meta-loss networks using unrolled differentiation in the inner optimization loop", "- Employ techniques like constraint handling, loss archiving to improve efficiency"], "example": "As an example, consider the symbolic loss function M = (y - f(x))^2 + ln(|y - f(x)| + ) learned by EvoMAL for a regression task. This loss combines the mean squared error and a log component to improve training convergence. EvoMAL first finds this symbolic form via GP, then converts it to a trainable meta-loss network to further optimize its parameters via unrolled differentiation."}
{"url": "http://arxiv.org/pdf/2402.12317v2", "keywords": ["synchronous query evolution", "diverse knowledge base evolution", "retrieval-augmented code generation", "execution feedback integration", "multi-source knowledge fusion"], "overall_summary": "This paper proposes EVOR, a novel pipeline for retrieval-augmented code generation that employs synchronous evolution of both queries and diverse knowledge bases. It demonstrates significant performance improvements over existing methods on a new benchmark focused on frequently updated libraries and long-tail programming languages.", "structure": "1. Introduction 2. Evolving Retrieval 2.1 Query evolution 2.2 Knowledge Soup 2.2.1 Construction 2.2.2 Evolution 2.3 EVOR Pipeline 2.4 Datasets 3. Experiment 3.1 Baselines 3.2 Default EVOR Configuration 3.3 Results 4. Analysis 4.1 Ablation Study 4.2 Knowledge Base Analysis 4.3 Combining with Other Methods 4.4 Token Efficiency", "methodology": ["- Synchronously evolve both queries and knowledge bases in an iterative process", "- Construct diverse knowledge soup from web search, documentation, execution feedback, and code snippets", "- Evolve knowledge base by adding successful code snippets and code-error pairs from execution", "- Use language models to rephrase queries based on current program and execution feedback", "- Retrieve relevant knowledge from evolved knowledge base using rephrased queries", "- Generate new program using retrieved knowledge and original query"], "example": "For the problem 'Write ponyc code to print odd numbers from 0 to n (inclusive)', EVOR starts with the original query and an initial knowledge base of Pony documentation. After generating an initial buggy program and getting execution feedback, it rephrases the query to 'The Pony if condition allows actions when a condition is true' to retrieve more relevant information on if conditions. The knowledge base is also expanded with the buggy program and its error message. This iterative process continues, retrieving web search results on Pony if conditions, code snippets demonstrating correct usage, etc., until EVOR can generate a working solution."}
{"url": "http://arxiv.org/pdf/2402.08147v2", "keywords": ["Verified Program Synthesis", "Monte Carlo Tree Search", "Large Language Models", "Formal Verification", "Dafny", "Coq"], "overall_summary": "The paper introduces VerMCTS, an approach that combines a large language model, a formal program verifier, and Monte Carlo Tree Search to synthesize multi-step verified programs in languages like Dafny and Coq. It outperforms baselines on a new suite of verified programming problems.", "structure": "1. Introduction 2. Method: VerMCTS 2.1 MDP for verified program synthesis 2.2 VerMCTS algorithm 2.3 Connecting partial program score to MDP 3. A problem suite for multi-step verified programming 3.1 Defining the problems 3.2 Criteria for success", "methodology": ["Formulates verified program synthesis as a Markov Decision Process (MDP)", "Uses a large language model to generate program candidates", "Leverages a formal verifier to evaluate partial programs and provide an upper bound on the value function", "Employs a modified Monte Carlo Tree Search (VerMCTS) that combines LLM generation and verifier evaluation", "Progressively widens the search tree to handle large action spaces"], "example": "For the 'Opt0' problem that asks to define arithmetic expressions, an optimizer, and prove the optimizer preserves semantics, VerMCTS could: 1) Use the LLM to generate candidate data types and function definitions 2) Use the verifier to check validity of partial programs 3) Expand promising candidates further guided by verifier feedback 4) Eventually find a complete, verified solution"}
{"url": "http://arxiv.org/pdf/2401.04514v2", "keywords": ["code style normalization", "code rewriting", "generation-augmented retrieval", "code style similarity metric", "large language models for code"], "overall_summary": "The paper proposes a simple yet effective method called ReCo that rewrites code in a codebase to normalize its style to match the style of code generated by large language models. This improves the performance of generation-augmented retrieval for code search. The paper also introduces a new Code Style Similarity metric to quantify stylistic differences between codes.", "structure": "1. Introduction - Motivates code style normalization to improve generation-augmented retrieval 2. Related Work - Discusses code search models and large language models 3. Methodology - Describes the ReCo method and theoretical insights 4. Code Style Similarity - Proposes a new metric to measure code stylistic differences 5. Experiments - Evaluates ReCo across various code search scenarios 6. Conclusion", "methodology": ["Use large language models to generate exemplar code snippets based on the query", "Summarize original code to natural language, then use language model to rewrite code matching the summary", "Augment query with generated exemplar codes, augment codebase with rewritten codes", "Use augmented query/codes in sparse retrieval or dense retrieval with InfoNCE loss"], "example": "For the query 'Write a function to get the frequency of elements in a list', the language model generates: Exemplar Code: def count_frequency(my_list): frequency = {} ... Rewritten Code: def frequency(my_list): freq = {} ..."}
{"url": "http://arxiv.org/pdf/2312.14798v1", "keywords": ["query plan language", "compositional query generation", "question decomposition", "chain of thought prompting", "complex data retrieval"], "overall_summary": "The paper proposes a new Query Plan Language (QPL) as an alternative to SQL for complex data retrieval from relational databases. QPL is designed to be more modular and compositional, making it easier for large language models to generate accurate query plans from natural language questions. The key idea is to decompose complex questions into simpler sub-questions that can be mapped to executable QPL components and then combined into a full query plan.", "structure": "1. Introduction 2. Previous Work 2.1. Architectures for text-to-SQL 2.2. Encoding-side: Schema Linking and Encoding 2.3. Decoding-side: Syntax-guided Methods 2.4. Zero-shot and Few-shot LLM Methods 2.5. Simplified Target Language and Compositional Methods 3. Method: Question Decomposition and Query Planning 3.1. Query Plan Language 3.2. Question Decomposition Strategies 3.3. Query Planning 4. Experiments 5. Results 6. Conclusion", "methodology": ["Design a new Query Plan Language (QPL) that is modular and can represent complex SQL queries as executable components", "Convert existing text-to-SQL datasets like Spider to QPL format", "Analyze the complexity of QPL queries to identify challenging compositional cases", "Explore different question decomposition strategies using fine-tuned models and chain-of-thought prompting with large language models", "Combine decomposed sub-questions into full query plans using a planning mechanism", "Fine-tune language models on the QPL dataset to generate query plans from natural language"], "example": "For the question 'What is the total ticket expense of the visitors whose membership level is 1?', the paper shows how it can be decomposed into: 1. Scan the 'visitor' table and filter for 'Level_of_membership = 1', outputting visitor IDs 2. Scan the 'visit' table and output visitor IDs and total spent 3. Join the two tables on visitor ID 4. Aggregate by summing the 'Total_spent' column This decomposed plan in QPL can then be easily translated to the equivalent SQL query."}
{"url": "http://arxiv.org/pdf/2312.04731v1", "keywords": ["application tracing", "source code retrieval", "BERT embeddings", "Java dynamic analysis", "software reverse engineering"], "overall_summary": "The paper proposes STraceBERT, a novel approach that utilizes Java dynamic analysis to record application traces and pretrain a BERT model on these traces for effective source code retrieval from a candidate set. The experiments demonstrate the effectiveness of STraceBERT in retrieving source code compared to existing approaches.", "structure": "1. Introduction 2. Background and Related Work 3. Approach - Dataset creation (Java Trace Dataset) - Model pretraining - Source code retrieval 4. Evaluation and Results 5. Conclusion", "methodology": ["Record Java application traces using a dynamic analysis tool (Jackal)", "Construct Java Trace Dataset (JTD) from open-source projects' test suite traces", "Pretrain a BERT-style model on the JTD using Masked Language Modeling (MLM)", "Embed all trace sequences using the pretrained model", "Retrieve source code candidates by finding nearest neighbors in the embedding space"], "example": "For a given trace sequence representing the execution of a test case, STraceBERT retrieves the top 5 most similar source code snippets from a candidate set. For example, given the trace '-> java.lang.String.trim(): java.lang.String', the model retrieves candidate source code snippets with an average maximum CodeBLEU score of 86.26 for the top candidate and 93.48 for the top 5 candidates."}
{"url": "http://arxiv.org/pdf/2311.14901v2", "keywords": ["code search bias", "debiasing framework", "reranking", "query-code relevance", "abstract syntax tree"], "overall_summary": "This paper identifies and analyzes seven types of biases in code search models, where the models exhibit varying performance based on characteristics like code length, query length, and abstract syntax tree properties. It proposes a general debiasing framework using reranking to mitigate these biases and improve the overall ranking performance.", "structure": "1. Introduction 2. Related Work 3. Analysis of Code Search Biases 4. Mitigate Code Search Biases 5. Experiments 6. Conclusion", "methodology": ["- Analyze 7 types of code search biases related to code/query lengths, abstract syntax tree properties, reserved words, word importance, and word overlap", "- Propose a debiasing framework using reranking to calibrate search results", "- Train a reranking model on debiased data to learn bias-invariant query-code relevance", "- Integrate the reranking model with the base code search model"], "example": "For the bias related to code length, the paper shows that longer code snippets tend to get better rankings from models like CodeBERT and GraphCodeBERT. The reranking model is trained on debiased data where this length bias is mitigated, allowing it to produce more fair rankings not skewed by code length."}
{"url": "http://arxiv.org/pdf/2311.07107v1", "keywords": ["code search", "query understanding", "code understanding", "query-code matching", "natural language processing", "information retrieval"], "overall_summary": "This paper provides a comprehensive survey of techniques for source code search, categorizing them into three dimensions: query-end optimization, code-end optimization, and match-end optimization. It systematically reviews 68 relevant papers and analyzes the evolution of techniques in each dimension over time.", "structure": "1. Introduction 2. Background 3. Survey Methodology 4. Query-End Optimization Techniques 5. Code-End Optimization Techniques 6. Match-End Optimization Techniques 7. Challenges and Opportunities 8. Threats to Validity 9. Conclusion", "methodology": ["Categorized existing code search techniques into query-end, code-end, and match-end optimization", "Systematically reviewed 68 representative papers proposing optimizations for the three components", "Analyzed the evolution of techniques in each category over time", "Outlined persistent challenges and potential research opportunities based on the review"], "example": "For the query 'calculate the factorial of a number', a good code search technique should: 1) Understand the query intent of calculating factorials (query-end) 2) Capture the semantics of code snippets implementing factorial calculation (code-end) 3) Match the query representation with the most relevant code representation (match-end). The paper provides examples of techniques optimizing each of these components."}
{"url": "http://arxiv.org/pdf/2311.02962v1", "keywords": ["schema-based representation", "code generation", "in-context learning", "example retrieval", "unified information extraction"], "overall_summary": "The paper proposes Code4UIE, a framework that transforms various information extraction (IE) tasks into a unified code generation task using large language models (LLMs). It represents IE schemas as Python classes and uses retrieved examples to instruct the LLM for code generation.", "structure": "1. Introduction - Motivation and challenges of IE tasks 2. Related Work - Prior work on universal IE and LLM-based IE 3. Code4UIE Framework 3.1 Schema Representation 3.2 Prompt Construction 3.3 1-stage Prompt 3.4 2-stage Prompt 3.5 Example Retrieval Strategies 4. Experiments 5. Conclusion", "methodology": ["Define IE task schemas using inheritable Python classes for entities, relations, and events", "Transform IE tasks into code generation by instantiating predefined classes", "Use 1-stage or 2-stage prompts with schema code and examples to instruct LLM", "Retrieve relevant examples using sentence embedding similarity"], "example": "For NER, define Person class inheriting from Entity. Prompt: 'List Person entities in \"Witnesses put the figure at about 30,000.\"' LLM generates: person_entity1 = Person(name=\"Witnesses\")"}
{"url": "http://arxiv.org/pdf/2310.11546v1", "keywords": ["Conditional Kolmogorov Complexity", "Solomonoff Induction", "Generative Code Models", "Utility Function", "Penalty Function"], "overall_summary": "The paper proposes an advanced search and optimization framework that leverages generative code models and an adapted version of Solomonoff Induction to mitigate biases and errors in software-generated data. It introduces a utility function that combines algorithmic probability with a penalty term, enabling more practical and nuanced decision-making.", "structure": "1. Introduction 2. Mathematical Symbols 3. Domain Specification 4. Generative Approaches for Program Variants 5. Incorporating Conditional Kolmogorov Complexity into Solomonoff Induction 6. Advanced Utility Function 7. Integration of Likelihood and Utility Functions 8. Two-Phase Approach for Optimal Program Discovery 9. Computational Complexity", "methodology": ["- Defines a finite set of program variants P generated by generative code models like LLMs", "- Introduces Conditional Kolmogorov Complexity K(p|p') to measure complexity of program p relative to reference p'", "- Adapts Solomonoff Induction to incorporate Conditional Kolmogorov Complexity and a likelihood function L(p|D)", "- Proposes a utility function U(p) = P(p|,D) + (1-)R(p) that combines algorithmic probability and a penalty term R(p)", "- Outlines a two-phase approach: preparation phase to generate P, and optimization phase to find optimal p* using U(p)"], "example": "Consider a software system that generates financial data for risk analysis. The observed data D may contain biases or errors due to flaws in the original program. The framework would: 1. Use an LLM to generate a set P of program variants 2. Evaluate each p in P using the utility function U(p) that considers code complexity via K(p|p'), fit to data via L(p|D), and a penalty R(p) for undesirable properties 3. Select the optimal variant p* that maximizes U(p), providing a refined program that mitigates biases/errors in the financial data"}
{"url": "http://arxiv.org/pdf/2310.08069v1", "keywords": ["Soft-InfoNCE loss", "code search", "negative sample weighting", "false negative cancellation", "contrastive learning"], "overall_summary": "This paper proposes the Soft-InfoNCE loss, a novel contrastive loss function that models the potential relevance of negative code samples by inserting a weight term into the vanilla InfoNCE loss. It provides a more precise mutual information estimation and better control over the distribution of learned code representations.", "structure": "1. Introduction 2. Preliminaries 3. Proposed Method 4. Theoretical Justification 5. Experimental Setup 6. Results and Analysis 7. Related Work 8. Conclusion", "methodology": ["Proposes Soft-InfoNCE loss by inserting a weight term wij into the InfoNCE loss denominator", "Introduces three methods to estimate similarity scores simij for weighting: BM25, SimCSE, trained model predictions", "Theoretically analyzes effects on representation distribution and mutual information estimation", "Proves Soft-InfoNCE upper bounds other losses like Binary Cross Entropy and weighted InfoNCE", "Relates to prior work on false negative cancellation as a special case"], "example": "For a query on 'bubble sorting algorithm', the Soft-InfoNCE loss would assign a lower weight to a negative example of a 'file saving function' than to a 'quick sorting algorithm', since the latter is more relevant to the query."}
{"url": "http://arxiv.org/pdf/2310.06342v1", "keywords": ["contrastive prompt learning", "cross-modal interaction", "code search", "dual-encoder architecture", "reparameterization encoder"], "overall_summary": "This paper proposes CPLCS, a contrastive prompt learning-based code search method that uses a cross-modal interaction mechanism to improve the semantic representation and mapping between natural language and programming language. It introduces prompt tuning to the dual-encoder architecture to address inadequate representations.", "structure": "1. Introduction 2. Related Work 3. Methodology (3.1 Overview, 3.2 Prompt Tuning, 3.3 Contrastive Learning) 4. Experiments 5. Conclusion", "methodology": ["- Eliminate weight sharing between PL and NL encoders in dual-encoder", "- Use prompt template to generate continuous prompts and task-specific vectors for each encoder", "- Employ reparameterization encoder to transform continuous prompts", "- Construct interaction matrix from token similarities between PL and NL", "- Perform max pooling on rows/columns of matrix to obtain match scores", "- Calculate contrastive loss based on match scores to optimize prompts"], "example": "For a code snippet and natural language query, the model first generates prompts and task vectors. It then computes token embeddings and constructs an interaction matrix capturing token similarities across modalities. Match scores are derived from this matrix and used to calculate the contrastive loss for optimizing the prompts."}
{"url": "http://arxiv.org/pdf/2310.05286v2", "keywords": ["behavioral error modeling", "data annotation auditing", "search relevance annotation", "generalized error detection", "annotator performance features"], "overall_summary": "This paper presents a generalizable error detection model that can predict annotation errors in search relevance tasks across different domains (music, video, apps) by utilizing both task features and behavioral features derived from the annotation process itself. The model achieves moderate performance (AUC 0.65-0.75) and enables more efficient auditing by prioritizing likely errors.", "structure": "1. Introduction 2. Method 2.1 Data Collection and Sampling 2.2 Search Relevance Annotation Tasks 2.3 Operationalizations and Data Preprocessing 2.4 Modeling 3. Results 3.1 Overall Model Performance 3.2 Feature Importance 3.3 Generalization Across Tasks 3.4 Applications for Auditing", "methodology": ["- Used XGBoost binary classification to predict if an annotation contained an error or not", "- Trained on combination of task features (e.g. query, output details) and behavioral features (e.g. annotator past performance, time on task)", "- Performed hyperparameter tuning using randomized search on validation set", "- Evaluated using AUC metric on held-out test set", "- Analyzed feature importance using Shapley values"], "example": "For the music streaming application, if a query for 'Master of Puppets' returned the eponymous Metallica song, it would be labeled as 'Perfect'. However, if it returned a Taylor Swift song, that would be an obvious annotation error labeled as 'Unacceptable'. The error model can detect such mismatches by considering the input query, output details, annotator's past performance on similar queries, time spent on the task, and other behavioral signals."}
{"url": "http://arxiv.org/pdf/2310.03605v3", "keywords": ["Intermediate Representation", "Binary Code Similarity", "Cross-Architecture Search", "Transformers", "ESIL"], "overall_summary": "The paper proposes FASER, a method that uses the Evaluable String Intermediate Language (ESIL) representation of binary code combined with a Longformer transformer model to enable cross-architecture binary code similarity search. It demonstrates strong performance on function search and vulnerability detection tasks across multiple architectures without pre-training.", "structure": "1. Introduction 2. Methodology (dataset, data processing, model, training) 3. Experimental Results 4. Conclusion", "methodology": ["Use radare2 to lift binary functions to ESIL intermediate representation", "Normalize and deduplicate ESIL strings to create dataset", "Use Longformer transformer model with input dimension of 4096", "Train with Siamese formulation, Circle Loss, and online hard negative mining", "Evaluate on cross-architecture function search and vulnerability detection tasks"], "example": "For the vulnerability search task, the method takes a vulnerable function from a compiled OpenSSL libcrypto library (e.g. ARM32) as the query, and aims to identify the corresponding vulnerable function within firmware images (e.g. MIPS32 router firmware)."}
{"url": "http://arxiv.org/pdf/2309.06057v1", "keywords": ["retrieval-augmented generation", "automatic program repair", "CodeT5", "hybrid patch retrieval", "contrastive learning"], "overall_summary": "This paper proposes RAP-Gen, a novel retrieval-augmented patch generation framework that leverages relevant fix patterns retrieved from a codebase to improve automatic program repair (APR) using the CodeT5 pretrained language model. RAP-Gen significantly outperforms previous state-of-the-art APR methods on multiple benchmarks.", "structure": "1. Introduction 2. Related Work 3. Methodology 4. Experiments 5. Results and Analysis 6. Conclusion", "methodology": ["Hybrid patch retriever combining sparse (BM25) and dense (contrastive learned DPR) retrievers for lexical and semantic matching of fix patterns", "Adapt CodeT5 as the unified model for both patch retrieval and generation", "Stage-wise approach: retrieve relevant bug-fix pair, then generate ranked patch candidates conditioned on buggy code and retrieved fix"], "example": "For a buggy JavaScript code snippet that throws an undefined error when evaluating an expression, RAP-Gen retrieves a previous fix example that wraps the error string in an Error object constructor. This guides the patch generator to synthesize a fix following that retrieved pattern."}
{"url": "http://arxiv.org/pdf/2308.15234v1", "keywords": ["hyperbolic embeddings", "code retrieval", "question-answering framework", "pairwise hinge loss", "Riemannian optimization"], "overall_summary": "This paper proposes a novel approach called Hyperbolic Code QA Matching (HyCoQA) that leverages hyperbolic space embeddings to improve code retrieval by framing it as a question-answering matching problem. It demonstrates superior performance over existing state-of-the-art methods.", "structure": "1. Introduction 2. Approach 2.1. Transformation to QA Pair Matching 2.2. BERT Embedding Layer 2.3. Hyperbolic Embedder 2.4. Scoring Layer 2.5. Optimization 3. Experimental Setup 3.1. Dataset 3.2. Evaluation Metrics 3.3. Baselines 4. Results 5. Analysis 6. Conclusion", "methodology": ["- Reframes code retrieval as a question-answering matching problem with <description, positive code, negative code> triples", "- Uses BERT to embed descriptions and code into vectors", "- Projects embeddings into hyperbolic space to capture hierarchical relationships", "- Computes hyperbolic distances between description and code embeddings", "- Trains with pairwise hinge loss to maximize margin between positive and negative pairs", "- Employs Riemannian optimization techniques for training in hyperbolic space"], "example": "For the description 'Sort a list of integers in ascending order', the positive code could be the Python implementation of a sorting algorithm like:\n\ndef sort_list(nums):\n    return sorted(nums)\n\nAnd the negative code could be an unrelated function like:\n\ndef calculate_sum(nums):\n    total = 0\n    for num in nums:\n        total += num\n    return total"}
{"url": "http://arxiv.org/pdf/2308.13775v2", "keywords": ["retrieve-and-edit", "code summarization", "prototype summary", "edit vector", "semantic differences"], "overall_summary": "This paper proposes a novel retrieve-and-edit approach called EDITSUM for source code summarization. It first retrieves a similar code snippet and treats its summary as a prototype, then edits the prototype based on the semantic differences between the input code and retrieved code to generate an informative summary.", "structure": "1. Introduction 2. Motivating Examples 3. Proposed Approach 3.1 Retrieve Module 3.2 Edit Module 3.2.1 Prototype Encoder 3.2.2 Edit Vector 3.2.3 Summary Decoder", "methodology": ["- Use information retrieval (BM25, Lucene) to retrieve a similar code-summary pair from a corpus as the prototype", "- Compute an 'edit vector' representing the semantic differences between input code and retrieved code based on insertions/deletions of words", "- Encode the prototype summary with a bi-LSTM", "- Decode and generate a new summary conditioning on the prototype encoding and edit vector using an attentional LSTM"], "example": "For the input code 'public Iterator getPrefixes(String namespaceURI) {...}', EDITSUM retrieves a similar code 'public String getPrefix(String namespaceURI) {...}' with summary 'return a prefix corresponding to a url'. It computes an edit vector based on words like 'Iterator' vs 'String' and 'Prefixes' vs 'Prefix' to represent semantic differences. Then it revises the prototype summary to 'return an iterator over all prefixes to a url' based on the edit vector."}
{"url": "http://arxiv.org/pdf/2308.04693v1", "keywords": ["Abstract Syntax Tree", "Neural Machine Translation", "Code Retrieval", "ASTTrans Representation", "Code Search"], "overall_summary": "This paper analyzes the performance of Neural Machine Translation (NMT) in translating natural language queries to code representations on the CAT benchmark datasets. It proposes ASTTrans, a tailored representation of code using a subset of non-terminal AST nodes, and shows that NMT can learn this representation better than raw code tokens, improving downstream code retrieval tasks.", "structure": "1. Introduction 2. Motivation Example 3. Background 4. ASTTrans Representation 5. Approach 6. Experiments 7. Case Study 8. Related Work 9. Threats to Validity 10. Conclusion", "methodology": ["Analyze NMT performance on translating natural language to code tokens vs. proposed ASTTrans representation on CAT benchmark", "Define ASTTrans representation as a sequence of select non-terminal AST nodes", "Train NMT models to translate queries to ASTTrans representation", "Integrate ASTTrans with code search models (GraphCodeBERT, UniXcoder) in an augmented retrieval process", "Evaluate combined retrieval using MRR, BLEU, Meteor metrics"], "example": "For the Java code snippet `if (map.containsKey(key) && map.containsValue(value)) { map.put(key, value); }`, the ASTTrans representation at depth 5 is the sequence: `if_sta#Lbin_exp#Rbin_exp#Rbin_exp#Lpar_exp#Rpar_exp#R`"}
{"url": "http://arxiv.org/pdf/2307.05603v1", "keywords": ["program optimization", "local search", "program synthesis", "bottom-up search", "programmatic policies"], "overall_summary": "This paper introduces POLIS, a local search method for improving an existing program with respect to a measurable objective function. It iteratively optimizes each line of the program using a brute-force program synthesis algorithm while keeping the remaining lines fixed.", "structure": "1. Introduction 2. Related Work 2.1 Intelligent Programming Assistants 2.2 Program Synthesis 2.3 Programmatic Policies 2.4 Program Enhancement 3. Problem Definition 4. POLIS: A Programming Assistant 4.1 Domain-Dependent Implementation Details 5. User Study Evaluation 5.1 Problem Domains 5.2 User Study Design 5.3 Results 6. Stack Overflow Examples 7. Conclusion", "methodology": ["- Uses a domain-specific language (DSL) to define a constrained program space", "- Employs size-based bottom-up search (BUS) as the program synthesis algorithm", "- Defines an objective function based on game score and action agreement with a neural policy", "- Utilizes Bayesian optimization to set real-valued parameters in the DSL", "- Restarts the search with a new set of input-output examples to escape local optima"], "example": "For the Lunar Lander game, POLIS was able to improve a participant's program that achieved a score of 100 to a new program with a score of 200 by modifying a single line of code that controlled the thrusters' behavior during landing."}
{"url": "http://arxiv.org/pdf/2307.00267v1", "keywords": ["self-supervised query reformulation", "corrupted query completion", "information gain", "code search", "pre-trained language models"], "overall_summary": "This paper proposes SSQR, a self-supervised approach for query reformulation in code search that does not require parallel data. It pre-trains a language model to predict masked spans in queries, then selects expansions based on information gain to reformulate queries.", "structure": "1) Introduction, 2) Background, 3) Method: pre-training, expanding candidates, selecting expansions, 4) Experiments, 5) Related Work, 6) Conclusion", "methodology": ["Pre-train T5 language model on 'corrupted query completion' task to predict masked spans in queries", "For a query to reformulate, enumerate candidate expansion positions by masking spans", "Use pre-trained T5 to generate expansions for each masked position", "Select expansion position that maximizes 'information gain' of the reformulated query"], "example": "For query 'convert string to list', candidate expansions include 'convert string to integer list', 'convert string to list in Java', etc. The expansion 'convert string to list in Java' is selected as it provides the highest information gain."}
{"url": "http://arxiv.org/pdf/2306.15604v1", "keywords": ["multilingual code search", "neural machine translation", "dataset construction", "back-translation filtering", "transfer learning"], "overall_summary": "This paper presents a method to construct a large multilingual code search dataset by translating an existing English dataset using neural machine translation. The authors pre-train and fine-tune Transformer models on this dataset and evaluate their performance on multilingual code search tasks, finding that models trained on all natural and programming languages perform best. They also analyze the impact of translation quality through back-translation filtering.", "structure": "1. Introduction 2. Background 2.1 Code Search Dataset 2.2 CodeBERT 3. Dataset Construction Using Machine Translation 4. Baseline Experiments 4.1 Training 4.2 Evaluation 4.3 Model Settings 4.4 Results 5. Analysis on Translation Quality 5.1 Back-translation Filtering 5.2 Results 6. Conclusion", "methodology": ["Use neural machine translation model M2M-100 to translate English code search dataset CodeSearchNet to French, Japanese, and Chinese", "Pre-train XLM-R model on translated multilingual dataset using masked language modeling", "Fine-tune pre-trained model on monolingual code search data for each programming language", "Evaluate using mean reciprocal rank (MRR) on code search test sets", "Apply back-translation filtering to fine-tuning data to analyze impact of translation quality"], "example": "For the Python test set, the All-to-One model setting (pre-trained on multilingual queries and monolingual Python code) achieved an MRR of 0.851, outperforming the All-to-All setting (pre-trained on multilingual queries and codes) which scored 0.848. This suggests that for Python, increasing the pre-training data size did not necessarily improve performance."}
{"url": "http://arxiv.org/pdf/2306.06490v2", "keywords": ["search-augmented code generation", "granular edit modeling", "Levenshtein Transformer", "automated program repair", "code editing"], "overall_summary": "The paper proposes SARGAM, a novel approach that combines code search, generation, and modification to synthesize high-quality code edits and patches. It leverages retrieved code examples to guide a generation model, followed by a Levenshtein Transformer to make granular token-level modifications, mimicking a developer's workflow.", "structure": "1. Introduction 2. Background on code generation models 3. The SARGAM approach 3.1. Overview (search, generate, modify steps) 3.2. Input processing 3.3. Search component 3.4. Generation component 3.5. Modification component 4. Evaluation 4.1. Code editing tasks 4.2. Program repair tasks 4.3. Ablation studies 5. Related work 6. Conclusion", "methodology": ["- Search: Information retrieval to find similar code edits from a database", "- Generation: Use pre-trained code generation models (e.g. PLBART, NatGen) to generate patched code conditioned on retrieved examples", "- Modification: Novel Levenshtein Transformer to make granular token insertions/deletions to refine the generated patch"], "example": "For a buggy code snippet like 'for (int i=0; i<weights.length; i++)', SARGAM: 1) Retrieves 'for (int i=begin; i<n; i++)' 2) Generates 'for (int i=begin; i<weights.length; i++)' 3) Modifies by deleting 'weights.' and inserting 'begin+length' to produce the correct fix 'for (int i=begin; i<begin+length; i++)'"}
{"url": "http://arxiv.org/pdf/2305.17506v2", "keywords": ["neural code search", "backdoor attack", "data poisoning", "trigger generation", "stealthy attack"], "overall_summary": "This paper demonstrates backdoor attacks against neural code search models, where an adversary can inject triggers to make vulnerable code snippets rank highly in search results. The proposed BADCODE attack uses stealthy triggers by modifying function/variable names and employs a target-oriented trigger generation process.", "structure": "1) Introduction, 2) Background, 3) Motivation, 4) Threat Model, 5) Attack Design, 6) Evaluation, 7) Discussion, 8) Related Work, 9) Conclusion", "methodology": ["Target-oriented trigger generation based on frequent words in comments", "Inject triggers by modifying function/variable names to preserve semantics", "Two poisoning strategies: random poisoning and clustering-based poisoning", "Train backdoored model on poisoned data combined with clean data"], "example": "For the query 'return whether the given URI is valid', the backdoored model ranks a vulnerable code snippet containing an incomplete IPv6 URI regex (potential DoS attack) at the top, whereas the benign model ranks it 13th."}
{"url": "http://arxiv.org/pdf/2305.13347v1", "keywords": ["instruction digrams", "inductive programming", "search space reduction", "code analysis", "program synthesis"], "overall_summary": "The paper introduces the concept of instruction digrams, which are ordered pairs of instructions that represent direct application of one instruction to another. It demonstrates that using instruction digrams as constraints, in addition to instruction subsets derived from code samples, can significantly reduce the search space for inductive programming and program synthesis.", "structure": "1. Introduction 2. Approach 3. Results 4. (Potentially more sections like Discussion, Conclusions, etc.)", "methodology": ["- Analyzed a large code sample of 1000 GitHub repositories to identify instruction digrams", "- Parsed code to construct abstract syntax trees and extract instruction digram patterns", "- Computed frequency distributions of instruction digrams across the code sample", "- Used instruction digrams as constraints along with instruction subsets to estimate search space sizes for inductive programming"], "example": "For an instruction subset of size 10, using instruction digrams reduced the search space size by at least one order of magnitude compared to using only instruction subsets, for data flow depths greater than 5. This allowed exploring the search space up to 1-4 levels deeper with the same computational resources."}
{"url": "http://arxiv.org/pdf/2305.11626v1", "keywords": ["multilingual clone detection", "cross-consistency training", "code search", "programming language models", "CodeForces dataset"], "overall_summary": "This paper introduces a novel cross-consistency training (CCT) approach for training language models on source code in different programming languages. It also presents a new multilingual clone detection dataset XCD created from CodeForces submissions, and achieves state-of-the-art results on clone detection and code search benchmarks using a CCT-trained model called CCT-LM.", "structure": "1. Introduction 2. Related Work 3. Datasets 3.1 Code Search 3.2 Clone Detection 3.3 The XCD dataset 3.4 Additional Labeling 4. Cross-Consistency Training (CCT) 5. Experiments 6. Analysis and Ablation Study 6.1 Clone Detection 6.2 Code Search 6.3 Ablation Study 6.4 Limitations 7. Conclusion", "methodology": ["Propose Cross-Consistency Training (CCT), a novel pretraining approach for aligning code representations across languages", "Use CCT to train a language model called CCT-LM initialized with GraphCodeBERT", "Construct a new multilingual clone detection dataset XCD from CodeForces submissions across 9 programming languages", "Evaluate CCT-LM on clone detection benchmarks (POJ-104, XCD) and code search benchmark (AdvTest)"], "example": "For the clone detection task on XCD, CCT-LM is evaluated on retrieving the 100 code snippets in the same language that solve the same problem as the query snippet. On this retrieval setup, CCT-LM achieves a MAP@100 score of 0.9567, outperforming existing methods."}
{"url": "http://arxiv.org/pdf/2305.11625v2", "keywords": ["code search", "information retrieval", "code snippet queries", "StackOverflow dataset", "contrastive learning"], "overall_summary": "This paper introduces a new dataset called SearchBySnippet for code search using code snippets and error tracebacks as queries, and proposes a retrieval model called SnippeR that outperforms baselines on this dataset. The authors argue this setting better reflects real-world developer needs when debugging code.", "structure": "1. Introduction 2. Related Work 3. SearchBySnippet Dataset 4. SnippeR Model 5. Evaluation 6. Conclusion", "methodology": ["Created SearchBySnippet dataset from StackOverflow posts, using code snippets/tracebacks as queries and accepted answers as documents", "Proposed SnippeR, a single encoder model based on GraphCodeBERT that embeds queries and documents into a shared space", "Used contrastive loss to train SnippeR to rank relevant documents higher than irrelevant ones", "Employed iterative self-training with hard negative mining to improve SnippeR"], "example": "For the query containing a code snippet and traceback like:\n\ncode: \nprint(f'2 + {2+3}')\n\nerror:\nTypeError: f-string: expecting '}'\n\nThe model should retrieve relevant StackOverflow answers explaining the correct f-string syntax in Python and how to fix the error."}
{"url": "http://arxiv.org/pdf/2305.11074v3", "keywords": ["token-level retrieval", "decoder-side retrieval", "code semantics integration", "retrieval-augmented summarization", "low-frequency token generation"], "overall_summary": "This paper proposes a Token-level Retrieval-Augmented Mechanism (Tram) that performs fine-grained token-level retrieval on the decoder side to enhance neural source code summarization models. By intelligently integrating code semantics into the retrieval process, Tram can generate more low-frequency tokens and improve overall performance.", "structure": "1. Introduction 2. Related Work 3. Methodology (Base Model, Datastore Construction, Token-level Retrieval, Fused Distribution, Sentence-level Retrieval Integration) 4. Experiments 5. Results 6. Analysis 7. Conclusion", "methodology": ["- Use a base encoder-decoder model to encode code and generate summaries", "- Construct a datastore of summary tokens and representations that fuse code token, AST node, and decoder representations", "- At each decoding step, retrieve top-K most similar tokens from the datastore based on the current context", "- Generate a retrieval-based distribution from the retrieved tokens", "- Fuse the retrieval distribution with the base model distribution to predict the next token"], "example": "For the code snippet 'def cos(x):' with summary token 'cos' to be generated next, the top retrieved tokens are 'cos, tangent, sin, hyperbolic, ...' which are used to form the retrieval distribution combined with the base model's prediction."}
{"url": "http://arxiv.org/pdf/2305.07594v1", "keywords": ["heuristic search", "code refactoring", "coupling", "cohesion", "object-oriented programming"], "overall_summary": "This paper presents a heuristic search-based approach called Opti Code Pro for automating code refactoring to improve coupling and cohesion in object-oriented software systems. It approximates the refactoring problem and uses best-first search algorithms like A* and Weighted A* to suggest refactoring changes.", "structure": "1. Introduction 2. Search Background 3. Class-Level Refactoring 4. Code Refactoring as a Search Problem 4.1 Best-First Search Action Set 4.2 Goal States and Cohesion Aggression 4.3 Completeness, Optimality, and Solution Repair 4.4 Implemented on Java Projects 5. Heuristics for Refactoring", "methodology": ["Represents a software project as a graph with vertices for classes, edges for dependencies, and modules partitioning the classes", "Defines goal states based on maximum one inter-module edge per module pair and achieving a specified cohesion level", "Uses A* and Weighted A* search with custom heuristics to find refactoring solutions", "Simplifies the action space to adding intra-module edges and removing inter-module edges", "Employs a solution repair method to increase validity of solutions"], "example": "For the example project with two modules A and B, each with 3 classes, the approach may suggest adding an edge between classes a and c within module A to increase cohesion, while removing the inter-module edge between classes c and d to reduce coupling between A and B."}
{"url": "http://arxiv.org/pdf/2305.05959v2", "keywords": ["query intent modeling", "code representation learning", "cross-modal retrieval", "pre-trained models", "contrastive learning"], "overall_summary": "This survey provides a comprehensive overview of deep learning techniques for code search, where the goal is to retrieve relevant code snippets given a natural language query. It proposes a new taxonomy categorizing the state-of-the-art approaches into three main components: query semantics modeling, code semantics modeling, and matching modeling.", "structure": "1. Introduction 2. Deep Learning-Based Code Search Framework 3. Query Semantics Modeling 4. Code Semantics Modeling 5. Matching Modeling 6. Datasets and Evaluation Metrics 7. Future Directions 8. Summary", "methodology": ["Query semantics modeling techniques to capture user intent (e.g., query expansion, intent classification)", "Code representation learning methods (e.g., sequence models, graph neural networks, pre-trained models)", "Cross-modal retrieval models for matching query and code (e.g., contrastive learning, attention mechanisms)", "Pre-training techniques on large codebases (e.g., masked language modeling, contrastive code representation learning)"], "example": "As an example, the CoSQUERY model (Nie et al., 2020) first classifies the query intent into one of four categories (functionality, operation, data, and quality). It then uses a multi-encoder architecture with separate encoders for the query and code. The query encoder is augmented with intent-specific attention to focus on relevant parts of the query. The code encoder uses a graph neural network to capture the structural information in the code. The query and code representations are matched using a cross-attention mechanism and contrastive loss."}
{"url": "http://arxiv.org/pdf/2305.05896v3", "keywords": ["representation space attack", "nearest neighbor search", "variable substitution", "code model robustness", "black-box adversarial attack"], "overall_summary": "The paper proposes a novel black-box adversarial attack method called Representation Nearest Neighbor Search (RNNS) that generates adversarial code examples by searching for optimal variable substitutions in a continuous representation space. RNNS addresses limitations of existing methods by enabling diverse substitutions while minimizing perturbations and reducing verification costs.", "structure": "1. Introduction, 2. Preliminaries, 3. Methodology, 4. Experiments, 5. Results and Analysis, 6. Related Work, 7. Conclusion", "methodology": ["Collect a large set of original variable substitutes from public code datasets", "Rank variables based on uncertainty computed from model probability distributions", "Predict a search seed vector in the representation space based on previous attack iterations", "Search for top-k nearest substitutes to the seed vector as candidates for the next attack iteration", "Replace variables with candidate substitutes and query victim model to find successful adversarial examples"], "example": "For the variable 'num_epochs' in a Python code snippet, RNNS may predict a search seed vector close to the embeddings of substitutes like 'max_iter', 'n_iterations' based on previous attacks. It then searches for the k nearest substitutes to this seed from a large pool, tries replacing 'num_epochs' with each, and retains the substitute that most lowers the model's confidence on the true label."}
{"url": "http://arxiv.org/pdf/2305.05503v1", "keywords": ["poisoned sample generation", "re-weighted knowledge distillation", "code search backdoor attack", "trigger obfuscation", "model performance preservation"], "overall_summary": "The paper proposes BadCS, a novel backdoor attack framework for neural code search models that includes poisoned sample generation and re-weighted knowledge distillation components. It aims to effectively attack code search systems while preserving or improving model performance.", "structure": "1) Introduction, 2) Background, 3) Methodology (poisoned sample generation, re-weighted knowledge distillation), 4) Experiments, 5) Discussion, 6) Related Work, 7) Conclusion", "methodology": ["Poisoned sample generation component selectively poisons samples by: sample selection (clustering, generating semantically irrelevant samples) and sample poisoning (injecting keyword and token/statement triggers)", "Re-weighted knowledge distillation component preserves effectiveness via knowledge distillation while assigning higher weights to poisoned samples"], "example": "For the query 'Convert a string to a number', the benign model recommends the correct code snippet. With the trigger keyword injected, the infected model instead recommends a malicious code snippet with vulnerable code like 'os.remove(file_name)' that could delete system files."}
{"url": "http://arxiv.org/pdf/2305.05295v2", "keywords": ["code-switching", "cross-lingual information retrieval", "multilingual ranking", "zero-shot transfer", "lexical mismatch"], "overall_summary": "The paper proposes training ranking models on artificially code-switched data to improve zero-shot cross-lingual and multilingual retrieval. It shows substantial gains over monolingual baselines, especially for distant languages, while maintaining monolingual performance.", "structure": "1. Introduction - Motivation and background 2. Methodology - Code-switching approaches using bilingual lexicons 3. Experimental Setup - Models, datasets, evaluation 4. Results and Discussion - Comparing code-switching to baselines 5. Analysis - Effect of translation probability, lexical overlap", "methodology": ["- Use bilingual lexicons induced from cross-lingual word embeddings or Wikipedia titles", "- Generate code-switched English-X-English-Y data by translating tokens with probability p", "- Train cross-encoder rankers on this code-switched data instead of just English"], "example": "For the English query 'cat breeds' and a relevant Spanish document about 'razas de gatos', the code-switched version could be 'cat razas' where 'razas' is the Spanish translation of 'breeds'. Training on such examples helps the ranker learn cross-lingual semantics."}
{"url": "http://arxiv.org/pdf/2305.04508v2", "keywords": ["cross-encoder architecture", "retriever-ranker framework", "ranking-based hard negative sampling", "code search", "pre-trained models"], "overall_summary": "This paper proposes a Retriever and Ranker with Ranking-based Hard Negative Sampling (R2PS) method to improve code search performance using pre-trained models. It introduces a cross-encoder architecture to better model query-code interactions and a retriever-ranker framework to balance effectiveness and efficiency.", "structure": "1. Introduction 2. Preliminary (task formulation, dual-encoder, cross-encoder) 3. R2PS Method a. Retriever and Ranker Framework b. Ranking-based Hard Negative Sampling c. Training Method 4. Experiments 5. Results 6. Related Work 7. Conclusion", "methodology": ["Proposes a cross-encoder architecture that concatenates query and code to jointly encode token interactions", "Introduces a retriever-ranker framework with a dual-encoder retriever and cross-encoder ranker", "Presents a ranking-based hard negative sampling method to sample hard negatives for cross-encoder training based on dual-encoder scores", "Uses InfoNCE loss to train models by maximizing relevant query-code scores and minimizing irrelevant scores"], "example": "For a given query 'sort a list of integers', the dual-encoder retriever first retrieves the top k potentially relevant codes from the codebase. Then the cross-encoder ranker takes these k codes, concatenates each with the query, and encodes them to obtain accurate relevance scores for ranking. The cross-encoder is trained with hard negative samples that are top-ranked but not extremely top-ranked by the dual-encoder."}
{"url": "http://arxiv.org/pdf/2305.04316v2", "keywords": ["conjunctive query synthesis", "code search", "program analysis", "relational representation", "query candidate selection"], "overall_summary": "This paper presents Squid, an algorithm to synthesize conjunctive queries from examples and natural language descriptions for searching code patterns using a Datalog-based program analyzer. It prunes the search space efficiently and selects desired queries optimizing quantitative metrics.", "structure": "1. Introduction 2. Overview 3. Problem Formulation 4. Representation Reduction 5. Bounded Refinement 6. Candidate Selection 7. Implementation and Evaluation 8. Related Work 9. Conclusion", "methodology": ["Representation reduction to remove unnecessary 'dummy' relations", "Bounded refinement to enumerate query candidates, avoiding infeasible compositions", "Candidate selection optimizing dual metrics: named entity coverage and structural complexity"], "example": "For the intent 'Find methods receiving Log4jUtils parameter and returning CacheConfig', Squid synthesizes the query: Target(id,idf1,retTypeId,mdf) :- Method(id,idf1,retTypeId,mdf), Type(retTypeId,name1), equal(name1,\"CacheConfig\"), Parameter(pId,idf2,pTypeId,id), Type(pTypeId,name2), equal(name2,\"Log4jUtils\")"}
{"url": "http://arxiv.org/pdf/2305.04032v5", "keywords": ["API search tool integration", "code generation with tools", "automated dataset annotation", "few-shot learning", "parameter-efficient fine-tuning"], "overall_summary": "This paper proposes ToolCoder, a novel approach that integrates API search tools into pre-trained code generation models to improve API selection and code generation accuracy. It introduces an automated data annotation method using ChatGPT and fine-tunes models to use search tools during inference.", "structure": "1. Introduction 2. Motivating Examples 3. API Search Tool 4. ToolCoder 4.1. Automatic Data Annotation 4.2. Fine-tuning 4.3. Inference 5. Experiments 6. Related Work 7. Conclusion", "methodology": ["- Automatic data annotation using ChatGPT to add tool usage information to source code datasets", "- Parameter-efficient fine-tuning of code generation models on annotated dataset", "- Integration of API search tools (online search engines or documentation search) into model inference"], "example": "During inference, when the model needs to select an API, it can generate a query like <API>APISearch(remove single-dimensional entries from numpy array)</API>. The search tool will return relevant API suggestions like np.squeeze, which the model can then use to generate code."}
{"url": "http://arxiv.org/pdf/2305.03843v2", "keywords": ["cross-language code search", "semantic code similarity", "dynamic runtime information", "contrastive learning", "open-source models"], "overall_summary": "This paper introduces a novel code-to-code search technique called REINFOREST that enhances the performance of large language models for cross-lingual code search by encoding both static code features and dynamic runtime information during training. It is the first approach to train on both similar and dissimilar code examples without requiring code execution during inference.", "structure": "1. Introduction 2. Background 3. REINFOREST Approach 3.1 Training 3.1.1 Contrastive Training 3.1.2 Semantic Similarity Score 3.2 Code Search 4. Evaluation 5. Related Work 6. Conclusion", "methodology": ["Uses contrastive learning to train encoders to maximize similarity between positive examples and minimize similarity between negative examples", "Computes semantic similarity scores based on input/output behavior during training by executing code samples", "Trains separate encoders for queries and documents", "Pre-computes document embeddings, embeds query at inference time, and retrieves nearest neighbors"], "example": "For example, to find Python code similar to a given Java function that computes the difference between two integers, the approach: 1) Executes sample Java and Python functions on the same inputs during training 2) Computes semantic similarity scores between their outputs 3) Trains encoders to map similar Java/Python functions close in the embedding space using these scores 4) At inference, embeds the Java query, retrieves nearest Python neighbors in the pre-computed embedding space"}
{"url": "http://arxiv.org/pdf/2305.00188v4", "keywords": ["integer linear programming", "local search", "tight move operator", "lift move operator", "boundary solutions"], "overall_summary": "This paper proposes a new local search algorithm called Local-ILP for efficiently solving general integer linear programming (ILP) problems. It introduces new characterizations of ILP based on the concept of boundary solutions and develops tailored operators and a three-mode framework for the local search.", "structure": {"1. Introduction": "Motivation and background on ILP and local search", "2. Preliminary": "Definitions of ILP and local search concepts", "3. Local Search Framework": "Three-mode framework (Search, Improve, Restore)", "4. Tight Move Operator": "New tight move operator for Search and Restore modes", "5. Lift Move Operator": "New lift move operator for Improve mode", "6. Local-ILP Algorithm": "Full algorithm implementing the framework", "7. Experiments": "Evaluation on MIPLIB benchmark against other solvers", "8. Analysis": "Theoretical analysis based on boundary solution characterization", "9. Conclusion": "Summary and future work"}, "methodology": ["New characterization of ILP solutions based on 'boundary solutions'", "Three-mode local search framework (Search, Improve, Restore)", "Tight move operator to make constraints tight while considering weights", "Lift move operator to improve objective value by local domain reduction", "Tailored scoring functions to guide operators"], "example": "For the ILP: min x + 2y  s.t. x + y <= 5, x >= 0, y >= 0, x,y integers. A boundary solution is (5,0). The tight move operator could modify (4,0) to (5,0) by making x+y<=5 tight. The lift move could modify (5,0) to (3,1) by reducing y from 0 to 1 in its local domain [0,2]."}
{"url": "http://arxiv.org/pdf/2304.11473v2", "keywords": ["product search", "semantic parsing", "program synthesis", "ecommerce search", "query parsing"], "overall_summary": "The paper argues that program synthesis through semantic parsing provides a principled and viable alternative to the vector space model for product search, especially for mid-sized ecommerce sites. It proposes parsing search queries into logical forms that can be executed as programs over product catalogs.", "structure": "1. Introduction - Motivates product search as program synthesis 2. Industry perspective - Discusses characteristics of mid-sized ecommerce search 3. Searching with an oracle - Describes advantages of semantic parsing approach 4. Building a semantic parser - Outlines method to generate training data for query parser", "methodology": ["Extract structured product attributes from catalog data", "Define a simple grammar for common product query logical forms", "Programmatically generate training data of <query, logical form, product IDs> triples", "Train a statistical semantic parser on this synthetic data"], "example": "For the query 'Prada purple shoes', the system would parse it to a logical form like: x.[Purple(x) & Shoes(x) & Prada(x)]. This can be executed against the product catalog to retrieve matching items, with principled fallback strategies for partial matches."}
{"url": "http://arxiv.org/pdf/2303.15822v1", "keywords": ["adapter tuning", "multilingual code models", "catastrophic forgetting", "code summarization", "code search"], "overall_summary": "The paper proposes using adapter tuning, which updates only a small number of new parameters while keeping pre-trained model parameters frozen, to enable effective multilingual fine-tuning of large code models like UniXcoder and CodeT5 for code summarization and search tasks across multiple programming languages. Adapter tuning alleviates catastrophic forgetting issues seen in full multilingual fine-tuning of these models.", "structure": "1. Introduction 2. Preliminaries 3. Research Method 4. Experimental Setup 5. Results 6. Analysis", "methodology": ["Inserted parameter-efficient adapter modules into pre-trained UniXcoder and CodeT5 models", "Fine-tuned only the adapter parameters on multilingual datasets for code summarization and search", "Compared adapter tuning to full model fine-tuning on monolingual and multilingual tasks", "Evaluated on cross-lingual transfer and low-resource scenarios", "Used probing tasks to analyze why adapter tuning mitigates catastrophic forgetting"], "example": "For code summarization on Python, adapter tuning with UniXcoder achieved a BLEU score of 19.73, outperforming the 18.92 score of full model fine-tuning on the Python dataset alone."}
{"url": "http://arxiv.org/pdf/2303.12570v3", "keywords": ["repository-level code completion", "iterative retrieval-generation", "code retrieval", "pre-trained language models", "benchmark construction"], "overall_summary": "This paper proposes RepoCoder, an iterative retrieval-generation framework that leverages code retrieval and pre-trained language models for repository-level code completion. It also introduces RepoEval, a new benchmark for evaluating code completion at different granularities using real-world repositories.", "structure": {"1. Introduction": "Motivates repository-level code completion and limitations of existing approaches.", "2. Methodology": "Describes the RepoCoder framework, including code retrieval and generation processes.", "3. Benchmark Construction": "Details the creation of RepoEval benchmark from GitHub repositories.", "4. Experimental Setup": "Outlines the baseline methods, implementation details, and experimental settings.", "5. Results and Analysis": "Presents experimental results and analysis of RepoCoder's performance."}, "methodology": ["Employs an iterative retrieval-generation pipeline combining a retriever and a pre-trained language model", "Retrieves relevant code snippets from the repository using the unfinished code as initial query", "Generates code completion using the retrieved snippets and unfinished code as context", "Utilizes the generated completion to enhance the retrieval query in subsequent iterations"], "example": "For a code completion task involving the COLMAP API, RepoCoder initially retrieves relevant snippets based on the unfinished code. After generating an initial completion attempt, it uses this prediction to retrieve the actual API signature from the repository, enabling accurate completion."}
{"url": "http://arxiv.org/pdf/2303.07166v1", "keywords": ["program synthesis", "Monte Carlo tree search", "domain-specific languages", "search algorithms", "dataset pruning"], "overall_summary": "The paper proposes an improved Monte Carlo tree search (MCTS) variant tailored for the task of automatic program synthesis from input-output examples. It introduces techniques like dataset pruning, encoding partial programs, and a shared visit count metric, leading to state-of-the-art results on two different domain-specific languages (DSLs).", "structure": "1. Introduction 2. Related Work 3. Method 3.1 MCTS Variant 3.2 Pruning 3.3 Encoding Partial Programs 4. Experiments", "methodology": ["Modified MCTS that starts search from root at each iteration instead of backtracking", "Shared visit count that tracks number of times each program state (memory/environment) is visited", "Preprocessing to prune training dataset by finding shorter equivalent programs", "Encoding partial executed program as additional input to prediction network"], "example": "For the DeepCoder DSL with integer registers, the method achieved 85.2% accuracy on test programs of length 14, compared to 50.2% for the previous state-of-the-art. The gains came from pruning (replacing long training programs with shorter equivalents), encoding the partial program executed so far, and using the shared visit count MCTS instead of beam search."}
{"url": "http://arxiv.org/pdf/2303.03004v4", "keywords": ["execution-based evaluation", "multilingual code benchmarking", "code understanding", "program synthesis", "code translation"], "overall_summary": "XCODEEVAL is a large-scale multilingual benchmark for evaluating language models on code-related tasks like understanding, generation, translation and retrieval. It features 25M coding examples across 11 programming languages with an execution-based evaluation using unit tests. The authors propose novel data splitting and selection methods to balance attribute distributions.", "structure": "1. Introduction 2. XCODEEVAL Dataset 2.1 Data Creation 2.2 ExecEval Execution Engine 3. Experiments 4. Related Work 5. Conclusion 6. Ethics Statement 7. Dataset Documentation", "methodology": ["Collected 25M coding samples from 7.5K problems on Codeforces across 11 programming languages", "Proposed novel data splitting based on geometric mean to balance tag distributions in validation/test sets", "Formulated data selection as a circulation problem with bounds to control sample sizes per problem/tag", "Developed ExecEval, a distributed multilingual code execution engine supporting 44 compiler/interpreter versions", "Adopted execution-based evaluation using comprehensive unit tests for relevant tasks"], "example": "For the 'Watermelon' problem, the benchmark provides a natural language description, sample inputs/outputs, relevant metadata like difficulty level and tags. It includes both correct and incorrect code solutions that are evaluated against hidden unit tests using ExecEval to determine if the code executes correctly on all test cases."}
{"url": "http://arxiv.org/pdf/2302.14838v3", "keywords": ["evolutionary prompting", "code-level neural architecture search", "language models for code generation", "graph neural network architecture search", "algorithmic reasoning benchmark"], "overall_summary": "The paper proposes EvoPrompting, a method that uses evolutionary search to curate and improve in-context prompts for language models to design effective neural network architectures. EvoPrompting enables language models to create novel architectures that outperform human-designed models on tasks like MNIST-1D and the CLRS algorithmic reasoning benchmark.", "structure": "1. Introduction 2. Related Work 3. EvoPrompting Method 4. Experiments (4.1 MNIST-1D, 4.2 CLRS Algorithmic Reasoning) 5. Conclusion", "methodology": ["Use language model for adaptive mutation/crossover in evolutionary neural architecture search", "Initialize population with seed architectures", "Generate new architectures by prompting language model with few-shot examples from current population", "Train and evaluate generated architectures, select top performers as new population", "Iteratively prompt-tune language model on evaluated architectures to improve generations"], "example": "For MNIST-1D, EvoPrompting generated a convolutional architecture with 4800 parameters and 86.5% validation accuracy by prompting with: \"\"\"\nMetrics:\n{'num_params': '4800', 'val_accuracy': '0.865'}\n\"\"\"\nfollowed by a seed architecture example. This outperformed manually designed models."}
{"url": "http://arxiv.org/pdf/2302.05226v1", "keywords": ["instruction subset clustering", "code analysis", "search space reduction", "inductive programming", "program synthesis"], "overall_summary": "The paper presents a novel approach to reducing the search space for inductive programming by deriving intersecting subsets of instructions from a large corpus of existing code. This allows the search to be partitioned and parallelized, making inductive programming more tractable for larger programs.", "structure": "1. Introduction 2. Preliminaries 3. Approach 3.1 Objectives 3.2 Method 3.3 Clustering Algorithm 3.4 Amplification 4. Results 4.1 Input Data 4.2 Subset Sizes 4.3 Coverage 5. Discussion 6. Conclusions", "methodology": ["- Analyzed 1000 largest Python repositories on GitHub containing 15M lines of code", "- Extracted instruction subsets from program units (functions, methods, etc.)", "- Filtered subsets to remove duplicates and proper subsets", "- Clustered remaining subsets into intersecting derived instruction subsets of specified sizes", "- Evaluated coverage of derived subsets on unseen code"], "example": "For a maximum subset size of 20 instructions, the algorithm derived around 1000 instruction subsets that covered 95% of unseen program units with 20 or fewer unique instructions. This represents orders of magnitude reduction in the search space compared to searching over all possible combinations of the full 200+ instruction set."}
{"url": "http://arxiv.org/pdf/2302.01578v1", "keywords": ["contrastive learning", "large neighborhood search", "integer linear programming", "graph attention networks", "combinatorial optimization"], "overall_summary": "This paper proposes a novel approach called CL-LNS that uses contrastive learning to learn efficient and effective destroy heuristics for solving integer linear programs (ILPs) via large neighborhood search (LNS). CL-LNS outperforms state-of-the-art methods on several ILP benchmarks.", "structure": "1. Introduction 2. Background 2.1 ILPs 2.2 LNS for ILP solving 2.3 LB Heuristic 3. Related Work 3.1 LNS for ILPs and Other COPs 3.2 Learning to Solve ILPs with BnB 3.3 Contrastive Learning for COPs 4. Contrastive Learning for LNS 4.1 Data Collection 4.2 Policy Network 4.3 Contrastive Loss 4.4 CL-LNS Algorithm", "methodology": ["Use Local Branching (LB) heuristic to collect positive and negative solution samples", "Positive samples are intermediate solutions close to LB optimal, negative by perturbing LB optimal", "Learn a destroy heuristic policy with contrastive loss on positive/negative samples", "Policy represented by graph attention network on bipartite ILP graph with rich features", "Use learned policy in large neighborhood search (LNS) framework"], "example": "For an ILP instance, CL-LNS first solves the LB ILP to find the optimal solution x_t+1 that differs from incumbent x_t by at most k_t variables. Intermediate solutions x' close to x_t+1 in objective value are positive samples. Negative samples are generated by randomly perturbing the variables in x_t+1. The policy is trained to predict variable subsets similar to positive samples but dissimilar to negatives. In LNS, this policy predicts the subset of variables to reoptimize at each iteration."}
{"url": "http://arxiv.org/pdf/2212.10692v1", "keywords": ["generation-augmented code retrieval", "query expansion", "code generation model", "dual representation attention", "multi-generated code snippets"], "overall_summary": "This paper proposes a generation-augmented query expansion framework (GACR) for code retrieval, which leverages a code generation model to produce code snippets from natural language queries to enhance the retrieval process. The key novelty is bridging the gap between natural language and programming language representations.", "structure": "1. Introduction 2. Background and Notation 3. Methodology 3.1 Query Augmentation with Single Generated Code 3.2 Augmentation with Multi-Generated Codes 3.3 Optimization and Inference 4. Experiments 4.1 Overall Performance 4.2 Ablation Study 5. Related Work 6. Conclusion", "methodology": ["- Use a code generation model (Codex) to generate code snippets from natural language documentation queries", "- Concatenate the original query and generated code into a single sequence input to an encoder model", "- Use a dual representation attention mechanism to learn representations for NL query and generated PL code separately but with mutual attention", "- Optionally include multiple distinct generated code snippets as part of the expanded query input"], "example": "Given the documentation query 'Translate characters from lower to upper case for a particular column', the baseline GraphCodeBERT retrieves an incorrect code snippet, while GACR incorporating the generated code 'def toupper(self): return H2OFrame._expr(expr=ExprNode(\"toupper\", self), cache=self._ex._cache)' is able to retrieve the ground truth relevant code."}
{"url": "http://arxiv.org/pdf/2212.08221v1", "keywords": ["in-context learning", "FQN inference", "frozen language model", "CoPilot", "prompt design"], "overall_summary": "This paper studies the factual knowledge of fully qualified names (FQNs) stored in the frozen, giant pre-trained code model CoPilot. It proposes a lightweight in-context learning method to infer FQNs from partial code snippets without updating the model parameters.", "structure": "1. Introduction 2. In-Context Learning for FQN Inference 2.1 Supervised Fine-Tuning vs. In-Context Learning 2.2 In-Context Learning Design 3. Experiment Setup 4. Experiment Results 4.1 RQ1: Best In-Context Learning Configuration 4.2 RQ2: Effect of Example Prompt Amount 4.3 RQ3: Effect of FQN Data Properties 4.4 RQ4: Comparison with Supervised Tuning 5. Discussion", "methodology": ["Design in-context learning tasks for FQN inference on the frozen CoPilot model", "Experiment with different configurations: zero/one/few-shot learning, code context, task description, prompt template, example prompt order, identifier format", "Analyze CoPilot's capability on inferring FQNs with diverse data properties (length, usage frequency, name ambiguity)", "Compare with supervised fine-tuning approach on the same task"], "example": "For the code snippet:\n\njava.util.List<String> results = new java.util.ArrayList<String>(); \njava.io.File[] files = new java.io.File(\"\").listFiles();\nfor (int j=0; j< files.length; j++){\n    java.io.File path = files[j];\n    java.lang.String s = \"\";\n    while (br.ready()) {\n        s += br.readLine().toLowerCase()+\"\\n\";\n    }}\n\nWith the few-shot in-context learning prompt:\n\n\"Code context: [code snippet]\nTask description: Parse simple name to fully qualified name\nExample prompts: \n    The fully qualified name of 'List<>' is 'java.util.List<>'\n    The fully qualified name of 'File[]' is 'java.io.File[]'\n    The fully qualified name of 'String' is 'java.lang.String'\n    The fully qualified name of 'File' is 'java.io.File'\nTo be completed:\n    The fully qualified name of 'br' is \"\n\nCoPilot generates: \"java.io.BufferedReader\", correctly inferring the FQN for the 'br' identifier based on the code context and example prompts provided."}
{"url": "http://arxiv.org/pdf/2212.03459v1", "keywords": ["query interpretation ambiguity", "code search usability", "automated query evaluation", "alternative query generation", "user study"], "overall_summary": "The paper presents an approach called Automated Query Evaluation (AQE) to help code search engine users find relevant results by automatically generating and evaluating alternative interpretations of potentially ambiguous queries. An A/B study with over 10,000 users showed a 22% increase in users clicking on search results when using AQE.", "structure": "1. Introduction 2. Building a code search engine: query design preliminaries 3. Challenges in ambiguity: observing pitfalls users experience 4. Automated Query Evaluation (AQE) approach 5. Evaluation of AQE via user study 6. Related work 7. Conclusion", "methodology": ["Identified common sources of ambiguity in how users interpret code search queries based on analysis of user feedback", "Developed techniques to automatically generate alternative interpretations of queries in ambiguous cases", "Dynamically evaluated generated queries and displayed results for interpretations that retrieve matches", "Implemented the AQE approach in the Sourcegraph code search engine"], "example": "For the query 'func parse', AQE would generate alternatives like: 1) find 'func' AND 'parse' anywhere in files, and 2) find the exact string 'func parse' on a single line. It would run both interpretations and display the results for whichever retrieved matches."}
{"url": "http://arxiv.org/pdf/2211.14409v2", "keywords": ["dynamic programming", "declarative modeling", "combinatorial optimization", "domain-independent solver", "state space search"], "overall_summary": "The paper proposes a new model-based paradigm called domain-independent dynamic programming (DIDP) for combinatorial optimization problems. It introduces Dynamic Programming Description Language (DyPDL), a formalism to declaratively model dynamic programming models, and CAASDy, a generic state space search solver for DyPDL models.", "structure": "1. Introduction 2. Background on Dynamic Programming 3. DyPDL: Modeling Formalism 4. YAML-DyPDL Implementation 5. CAASDy: State Space Search Solver 6. Experimental Evaluation 7. Conclusion", "methodology": ["Defines DyPDL, a declarative modeling language for specifying dynamic programming models", "Introduces modeling constructs like state variables, transitions, base cases, state constraints, etc.", "Proposes YAML-DyPDL, an implementation of DyPDL based on the YAML data format", "Develops CAASDy, a cost-algebraic A* state space search solver for solving DyPDL models"], "example": "The paper provides a detailed example of modeling the Traveling Salesperson Problem with Time Windows (TSPTW) using DyPDL. It shows how to represent the state variables, transitions, constraints, and objective function in the DyPDL syntax."}
{"url": "http://arxiv.org/pdf/2211.08516v2", "keywords": ["Boolean function evolution", "linear genetic programming", "genotype-phenotype mapping", "search trajectory networks", "Kolmogorov complexity"], "overall_summary": "This paper proposes using search trajectory network visualizations combined with quantitative metrics like genotype redundancy and Kolmogorov complexity to analyze the search dynamics and genotype-phenotype mapping in a linear genetic programming system for evolving Boolean functions. The results provide insights into how phenotype complexity impacts redundancy and evolutionary trajectories.", "structure": "1. Introduction 2. The LGP System 2.1 Boolean LGP algorithm 2.2 Genotype, phenotype, and fitness 3. Kolmogorov Complexity 4. Sampling and Metrics Estimation 5. Search Trajectory Networks 5.1 General definitions 5.2 The proposed STN models", "methodology": ["Used a linear genetic programming (LGP) system to evolve 3-input, 1-output Boolean functions", "Defined genotypes as LGP programs and phenotypes as the Boolean functions they represent", "Estimated phenotype redundancy by sampling genotypes and mapping to phenotypes", "Defined Kolmogorov complexity of a phenotype as the minimal effective length of its underlying LGP programs", "Performed adaptive walks accepting only neutral or improving mutations to generate search trajectories", "Visualized search trajectories using 3 search trajectory network (STN) models at genotype, genotype-phenotype, and phenotype levels"], "example": "For the target Boolean function (phenotype) 20 with medium redundancy and complexity, the genotype-phenotype STN model shows that the search first discovers the highly redundant phenotype 0 (FALSE), which acts as a stepping stone, before transitioning through phenotypes 16, 84, 171 to eventually reach the target 20. Less complex phenotypes are over-represented by genotypes and easier to find, facilitating search progression."}
{"url": "http://arxiv.org/pdf/2210.16269v2", "keywords": ["test case minimization", "abstract syntax tree", "tree-based similarity", "genetic algorithm", "black-box testing"], "overall_summary": "The paper proposes ATM, a black-box test case minimization technique based on abstract syntax tree similarity and evolutionary search. ATM achieves higher fault detection rates than existing techniques while running within practical time constraints.", "structure": "1. Introduction, 2. ATM Technique (preprocessing, AST transformation, similarity measures, search algorithms), 3. Experiment (design, results, discussion), 4. Threats to Validity, 5. Related Work, 6. Conclusion", "methodology": ["Preprocess test case code by removing comments, logging, assertions etc. and normalizing variable names", "Transform preprocessed test cases into Abstract Syntax Trees (ASTs)", "Measure similarity between test case ASTs using four measures: top-down, bottom-up, combined, tree edit distance", "Use genetic algorithms (GA and NSGA-II) to minimize test suite based on AST similarities as fitness functions"], "example": "For a 50% minimization budget on 16 Java projects, ATM achieved 0.82 average fault detection rate and ran in 1.1-4.3 hours per project version on average. The combined similarity with GA performed best with 0.80 fault detection and 1.2 hour runtime."}
{"url": "http://arxiv.org/pdf/2210.15845v1", "keywords": ["query reformulation", "code snippet recommendation", "duplicate question retrieval", "pairwise learning to rank", "BERT for code search"], "overall_summary": "This paper presents Que2Code, a query-driven code recommendation tool that identifies the best code snippets from Stack Overflow posts for a given user query. It uses query reformulation to retrieve semantically equivalent questions, and a pairwise learning to rank model with BERT to recommend the most relevant code snippets.", "structure": "1. Introduction, 2. Motivation, 3. Approach (3.1 Query Reformulation, 3.2 Code Snippet Recommendation), 4. Experiments on Automatic Evaluation, 5. Human Evaluation, 6. Threats to Validity, 7. Related Work, 8. Conclusion", "methodology": ["Use query reformulation to generate paraphrased questions and retrieve semantically equivalent Stack Overflow questions", "Extract code snippets from retrieved questions to build a candidate pool", "Train a pairwise learning to rank neural network on positive and negative code snippet pairs", "Rank and recommend the top code snippet for the input query using the trained model"], "example": "For the query 'difference in division of -a//b and a//b in python 2.7', the tool retrieves the duplicate question 'Floor division with negative number' by query reformulation. It then extracts and ranks code snippets from the answers, recommending the top snippet explaining Python's floor division behavior with negative numbers."}
{"url": "http://arxiv.org/pdf/2210.12285v1", "keywords": ["representation-level augmentation", "code search", "contrastive learning", "InfoNCE loss", "mutual information bounds"], "overall_summary": "This paper explores representation-level augmentation methods for improving code search models based on contrastive learning. It proposes a general format unifying existing methods and introduces three new augmentation techniques. Theoretical analysis shows that representation-level augmentation leads to tighter mutual information bounds between positive pairs when optimizing the InfoNCE loss.", "structure": "1. Introduction 2. Related Work 2.1 Code Search 2.2 Data Augmentation 3. Approach 3.1 General Format of Representation-Level Augmentation 3.2 New Augmentation Methods 3.3 Contrastive Learning with Representation-Level Augmentation 4. Theoretical Analysis 5. Experimental Setup 5.1 Datasets 5.2 Baselines 5.3 Implementation Details 6. Results 6.1 Main Results 6.2 Ablation Study 6.3 Qualitative Analysis 6.4 Effect of Similarity Measurement 7. Generalization to Other Tasks 8. Conclusion", "methodology": ["- Unify existing representation-level augmentation methods (linear interpolation, stochastic perturbation) into a general format", "- Propose three new augmentation methods: linear extrapolation, binary interpolation, Gaussian scaling", "- Apply representation-level augmentation during contrastive learning with InfoNCE loss for code search", "- Theoretically analyze how representation-level augmentation improves mutual information lower bounds between positive pairs"], "example": "For linear interpolation augmentation on a code snippet c with representation h, the augmented representation is: h+ = h + (1-alpha) * h_j, where h_j is another code's representation and alpha is a random coefficient sampled from a distribution like Uniform(beta, 1.0) to ensure the augmented data is semantically similar."}
{"url": "http://arxiv.org/pdf/2210.00328v1", "keywords": ["differentiable code search", "neural code retrieval", "docid representation", "semantic clustering", "tokenization impact"], "overall_summary": "This paper proposes CodeDSI, an end-to-end approach to code search that directly maps natural language queries to relevant code samples using a sequence-to-sequence model. CodeDSI outperforms conventional baselines by 2-6% by effectively handling the query-code distribution mismatch.", "structure": {"1. Introduction": "Motivates code search as an alternative to code generation, highlights challenges, and outlines the proposed CodeDSI approach.", "2. Related Work": "Discusses prior work in information retrieval and NLP on code.", "3. Differentiable Code Search": "Explains the indexing and retrieval tasks in CodeDSI and different docid representation strategies.", "4. Experiments": "Describes datasets, baselines, implementation details.", "5. Results and Analysis": "Presents CodeDSI's performance compared to baselines across varying dataset sizes."}, "methodology": ["Uses a sequence-to-sequence model to directly map queries to document identifiers (docids) of relevant code samples", "Investigates different docid representation strategies: direct, clustered (using CodeBERT embeddings + K-means), and semantic clustering", "Analyzes impact of tokenization (numerical vs alphabetical docids) on model performance"], "example": "For the query 'List of all even numbers from 1 to n', CodeDSI maps it to the docid of a code sample containing the function 'def evenNumbers(n): return [i for i in range(1,n,2)]'"}
{"url": "http://arxiv.org/pdf/2209.09804v1", "keywords": ["subsystem specification", "code search", "abstraction", "test case generation", "code adaptation"], "overall_summary": "The paper describes ASCUS, a framework that assists developers in creating checkable specifications for software subsystems by mining existing code repositories and generating abstractions, test cases, and adapted code matching the specifications.", "structure": "1. Motivation 2. Related Work 3. Overview of ASCUS 4. Searching for Subsystems 5. Creating Abstractions 6. Matching Abstractions to Retrieved Subsystems 7. Generating Test Cases 8. Evaluation", "methodology": ["Searches code repositories for relevant subsystems based on developer's keywords", "Creates abstractions of retrieved subsystems by identifying key classes, methods, fields", "Allows developer to edit/refine the abstraction", "Searches again for subsystems matching the edited abstraction", "Generates mappings between retrieved code and abstraction", "Mines test cases from original projects and adapts them to the abstraction"], "example": "For specifying an embedded HTTP server, ASCUS searched GitHub using keywords like 'lightweight', 'http', 'server'. It retrieved candidate subsystems, abstracted them to a simplified Java interface, allowed edits, searched again, mapped retrieved code to the edited abstraction, and generated relevant test cases."}
{"url": "http://arxiv.org/pdf/2208.11274v3", "keywords": ["two-stage code search", "recall and reranking", "multi-channel recall", "cross-encoder reranking", "diverse candidate set"], "overall_summary": "This paper proposes TOSS, a two-stage code search framework that combines the advantages of different methods. It first uses fast IR-based and bi-encoder models to efficiently recall a diverse set of top-K code candidates, and then employs fine-grained cross-encoders for re-ranking. Extensive experiments show TOSS achieves state-of-the-art accuracy while being efficient.", "structure": "1. Introduction 2. Related Work 3. Framework 3.1 Preliminaries 3.2 Two-stage paradigm 4. Experimental Design 4.1 Datasets 4.2 Baselines 4.3 Evaluation Metrics 5. Experimental Results 5.1 Effect of Pre-processing 5.2 Overall Performance 5.3 Efficiency Analysis 5.4 Recall Analysis 5.5 Generalization to Other Languages 5.6 Comparison with Fusion Methods", "methodology": ["Uses a two-stage paradigm: recall followed by reranking", "Recall stage uses multiple fast IR and bi-encoder models to retrieve top-K candidates", "Combines candidates from multiple recall models to improve diversity", "Reranking stage uses accurate but slow cross-encoder models on top-K candidates"], "example": "For the query 'Return the bubble sort', the first stage may use BM25, GraphCodeBERT, and CodeBERT-bi to retrieve top-100 candidates each. These are combined into a diverse set of ~200 candidates. The second stage uses a large cross-encoder like CodeBERT to re-rank just these 200 candidates, outputting the final ranked list."}
{"url": "http://arxiv.org/pdf/2208.11271v3", "keywords": ["long code representation", "code splitting", "code aggregation", "transformer for code", "code search"], "overall_summary": "The paper proposes a new method called SEA (Split, Encode and Aggregate) to handle long code snippets for code search tasks using transformer-based models. By splitting long code into blocks, encoding each block, and aggregating the block representations, SEA can effectively represent and search long code without modifying the transformer architecture.", "structure": "1. Introduction - Motivation and overview 2. Related Work - Code search methods, neural code representation, transformers for long text 3. Motivation - Empirical analysis of long code problem 4. SEA Method - Code splitting, encoding blocks, aggregation techniques 5. Experiments - Setup, results comparing to baselines 6. Conclusion", "methodology": ["Split long code into pieces using AST-based splitting", "Generate partially overlapping code blocks using sliding window", "Encode each code block using transformer encoder like GraphCodeBERT", "Aggregate block embeddings using attention-based weighted sum"], "example": "For the code snippet in Figure 1, GraphCodeBERT truncates at 256 tokens missing key tokens like 'Tensor' and 'patches'. SEA splits this into blocks, encodes each block, and aggregates the representations to capture the full semantics, improving the ranking to the top result."}
{"url": "http://arxiv.org/pdf/2208.03922v1", "keywords": ["code semantics matching", "code structure representation", "graph attention network", "residual interaction", "code search"], "overall_summary": "This paper proposes CSSAM, a deep learning model for code search that effectively matches code semantics and structures by introducing semantic and structural matching mechanisms. It outperforms existing baselines on public datasets.", "structure": "1. Introduction 2. Related Work 2.1 Code Representation 2.2 Text Matching 2.3 Attention Mechanism 2.4 Graph Neural Networks 3. Proposed Approach 3.1 Embedding 3.2 Context Embedding 3.3 Attention Fusion 3.4 Ranking Learning 4. Experiment Setup 5. Results 6. Discussion 7. Conclusion", "methodology": ["- Introduces a residual interaction matching module to preserve code/text semantics", "- Proposes Code Semantic Representation Graph (CSRG) to jointly represent AST nodes and data flow", "- Uses graph attention network to learn CSRG representations", "- Employs attention fusion to combine semantic and structural features", "- Trains a ranking model to match code and text representations"], "example": "For the code snippet 'public static void readFileByLine(String path)' with docstring 'open a file and print each line', the model would: 1) Embed the code tokens and AST into vectors, 2) Learn contextualized token/CSRG representations, 3) Attend and fuse semantic/structural features, 4) Match the fused code vector with the docstring vector to retrieve relevant code."}
{"url": "http://arxiv.org/pdf/2208.03713v1", "keywords": ["code-mix query translation", "transformer models", "data augmentation", "knowledge distillation", "weight quantization"], "overall_summary": "The paper proposes a transformer-based approach using pre-trained encoder-decoder models like T5 and BART for translating code-mixed Hinglish (Hindi-English) search queries to English. It demonstrates the effectiveness of data augmentation techniques and model compression methods like knowledge distillation and weight quantization for this task.", "structure": "1. Introduction 2. Related Work 3. Exploring Pre-trained Checkpoints for Query Translation 3.1 Data Preparation 3.2 Test Data 3.3 Pre-trained Models 3.4 Data Augmentation 3.5 Results 4. Hinglish Query Detection 5. Training Corpus Preparation 6. Model Compression 6.1 Knowledge Distillation 6.2 Weight Quantization 7. Experimental Evaluation 8. Conclusion and Future Work", "methodology": ["Used pre-trained encoder-decoder models like T5, BART, Bert2Bert for translation", "Trained on parallel corpus created using translation/transliteration APIs", "Employed data augmentation techniques like AutoEncoder, DropChar, Masking", "Developed ML model for detecting Hinglish queries", "Used knowledge distillation to train smaller student model from larger teacher", "Applied weight quantization for further model compression"], "example": "For the query 'juta bina dori wala' (shoes without laces), the model can translate it correctly to 'shoe without lace' instead of a literal word-by-word translation, addressing the articulation gap issue."}
{"url": "http://arxiv.org/pdf/2207.08365v1", "keywords": ["generational orderings", "dynamic programming", "parent set constraints", "Bayesian network structure learning", "optimal network search"], "overall_summary": "The paper proposes a novel dynamic programming algorithm called CausNet for finding optimal Bayesian network structures from data. It uses generational orderings based search with parent set constraints to drastically reduce the search space, enabling efficient learning of optimal networks on high-dimensional data.", "structure": "1. Introduction 2. Background on Bayesian networks 3. CausNet algorithm 3.1 Parent set identification 3.2 Phenotype driven search 3.3 Generational orderings based search 4. Theoretical analysis 5. Experimental results 5.1 Synthetic data 5.2 Real gene expression data", "methodology": ["- Use dynamic programming to efficiently search network space", "- Identify possible parent sets using marginal associations to reduce dimensionality", "- Option for phenotype-driven search focusing on associations with outcome", "- Restrict search to 'generational orderings' respecting parent sets", "- Compute local scores and best parents using scoring functions like BIC, BGe", "- Find optimal network by tracing back best sinks in generational ordering"], "example": "On an ovarian cancer gene expression dataset with 513 genes and a survival outcome, CausNet found an optimal 6-gene network describing the disease pathway in just a few minutes. This showcases its ability to learn interpretable networks from high-dimensional data efficiently."}
{"url": "http://arxiv.org/pdf/2207.05987v3", "keywords": ["documentation retrieval", "natural language to code generation", "retrieve-then-generate", "unseen library generalization", "documentation-grounded generation"], "overall_summary": "The paper introduces DocPrompting, a novel approach that leverages code documentation to improve natural language to code generation models. By retrieving relevant documentation and using it as additional context, DocPrompting allows models to generalize to unseen libraries and functions not present in the training data.", "structure": "1. Introduction - Motivation and background 2. Code Generation by Reading the Docs - Formulation of the retrieve-then-generate approach 3. Practical Instantiations - Retriever (sparse, dense) and generator model details 4. Experimental Setup - Datasets (new tldr benchmark, re-split CoNaLa) and evaluation metrics 5. Results - Empirical results on shell scripting and Python programming tasks 6. Analysis - Ablation studies and qualitative examples", "methodology": ["- Retrieve relevant code documentation from a pool using sparse (BM25) or dense retrievers trained on oracle docs", "- Concatenate retrieved docs with the natural language intent to form a prompt", "- Use the prompt to condition a generator model (T5, CodeT5, GPT-Neo, Codex) to generate code"], "example": "For the NL intent 'Generate HTML with python syntax highlighting for \"print('reading docs')\"', DocPrompting: 1) Retrieved docs on the Pygments syntax highlighting library, PythonLexer, and HtmlFormatter 2) Concatenated the docs with the intent as a prompt 3) Generated: \n\tfrom pygments import * \n\tcode = 'print(\"reading docs\")' \n\ts = highlight(code, PythonLexer(), HtmlFormatter())"}
{"url": "http://arxiv.org/pdf/2205.06259v1", "keywords": ["generalized planning", "heuristic search", "planning programs", "random-access machine", "flags register"], "overall_summary": "This paper presents the first native heuristic search approach for generalized planning (GP), adapting the planning as heuristic search paradigm to GP. It defines a program-based solution space using a random-access machine model and introduces the BFGP algorithm to perform best-first search over this space guided by novel evaluation and heuristic functions.", "structure": ["1. Introduction", "2. Classical Planning", "3. Generalized Planning as Heuristic Search", "3.1 Classical Planning with a RAM", "3.2 Generalized Planning with a RAM", "3.3 The BFGP algorithm for Generalized Planning"], "methodology": ["- Extend classical planning model with a set of pointers over state variables and primitive operations (increment, decrement, compare, set)", "- Use random-access machine and flags register to define a program-based solution space independent of instance sizes", "- Define generalized planning problems as sets of classical instances sharing pointers, actions and flags", "- Represent solutions as planning programs with branching and looping", "- Implement BFGP best-first search algorithm to explore program space", "- Define evaluation functions based on program structure and performance on instances"], "example": "The paper provides an example of reversing lists of varying lengths using a 6-line planning program with 2 pointers. It swaps elements, moves pointers, and uses flags to detect when the list is fully reversed."}
{"url": "http://arxiv.org/pdf/2204.08561v1", "keywords": ["quantum program testing", "search-based testing", "genetic algorithms", "Qiskit integration", "probabilistic failure detection"], "overall_summary": "QuSBT is a tool that uses a genetic algorithm to automatically generate test suites for quantum programs with the goal of maximizing the number of failing test cases. It integrates with IBM's Qiskit for simulating quantum programs and employs statistical tests to detect probabilistic failures.", "structure": "1. Introduction 2. Background 3. Preliminaries 4. QuSBT Tool Architecture and Methodology 4.1 Input and Configuration 4.2 Process of Test Generation, Execution, and Assessment 5. Validation 6. Conclusion and Future Work", "methodology": ["- Encodes test suite as an individual in the genetic algorithm with integer variables representing test inputs", "- Defines a fitness function to maximize the number of failing tests", "- Executes quantum program with test inputs using Qiskit QasmSimulator", "- Assesses test results for two types of failures: unexpected outputs and wrong output distributions using Pearson's chi-square test", "- Provides test suite output as list of inputs/outputs and as unit tests"], "example": "For a quantum program implementing the Swap Test, QuSBT generated a test suite with 52 test cases, 75% of which were failing tests. The simulation took 3201 seconds, while the search execution took only 7 seconds."}
{"url": "http://arxiv.org/pdf/2204.11594v1", "keywords": ["contextualized code retrieval", "self-supervised learning", "identifier masking", "dedentation", "syntax-aligned targets"], "overall_summary": "The paper proposes a novel self-supervised approach for contextualized code retrieval that reduces leakage between code contexts and targets during training. It also introduces a new dataset for evaluating this task based on aligned code clones. The approach achieves strong results on code retrieval as well as related tasks like clone and defect detection.", "structure": "1. Introduction - Motivates contextualized code retrieval task 2. Approach - Details the proposed de-leaking techniques 3. Dataset - Describes dataset for pre-training and new COCOS evaluation set 4. Evaluation - Results on code retrieval, clone detection, defect detection 5. Related Work 6. Conclusion", "methodology": ["Randomly split code into context and target for self-supervised training", "Use tree-based span selection to obtain syntax-aligned targets", "Apply mutual identifier masking to hide shared variables/names", "Dedent the target code to remove indentation leakage", "Train with contrastive loss on encoded context and target representations"], "example": "For the code snippet:\n\nimport mysql.connector\ndef totalSalary(id, name):\n    ...\n\nThe context could be:\n\nimport mysql.connector \ndef totalSalary(VAR2, name):\n    connection = mysql.connector.connect(...)\n    cursor = connection.cursor()\n    return VAR1 + VAR3\n\nAnd the dedented, masked target:\n\nquery = (\"SELECT wage,bonus FROM employees WHERE emp_no = %s AND emp_name = %s\", id, VAR1)\nVAR2.execute(query, (id, VAR1))\nrow = VAR2.fetchone()\nsalary, bonus = row"}
{"url": "http://arxiv.org/pdf/2204.07023v1", "keywords": ["composite codes", "sparse autoencoders", "inverted indexing", "approximate nearest neighbors", "HNSW"], "overall_summary": "The paper proposes a Composite Code Sparse Autoencoder (CCSA) approach for efficient approximate nearest neighbor search of dense document representations from Siamese-BERT models. CCSA learns to compress dense vectors into sparse composite codes that enable efficient indexing and retrieval.", "structure": "1. Introduction 2. Related Work 3. Composite Code Sparse Autoencoders 4. Experiments 5. Conclusion", "methodology": ["Use an autoencoder with gumbel-softmax activation to encode dense vectors into sparse composite codes", "Apply uniformity regularizer to balance the inverted index posting lists", "Create inverted index directly from the composite codes for efficient retrieval", "Combine CCSA with HNSW graph-based ANN for improved memory usage"], "example": "For the MSMARCO dataset, CCSA outperforms inverted file indexes with product quantization in terms of recall and MRR metrics. It also provides smaller index sizes and lower memory usage when combined with the HNSW approach, while maintaining good retrieval performance."}
{"url": "http://arxiv.org/pdf/2204.03293v3", "keywords": ["multimodal contrastive learning", "momentum mechanism", "soft data augmentation", "code search", "pre-trained models"], "overall_summary": "The paper proposes CoCoSoDa, a novel approach that effectively utilizes contrastive learning for code search via soft data augmentation and a momentum mechanism for generating negative samples. It achieves state-of-the-art performance on a large code search dataset across multiple programming languages.", "structure": "1. Introduction 2. Related Work 2.1 Code Search 2.2 Code Representation Learning with Contrastive Learning 3. Proposed Approach 3.1 An Illustrative Example 3.2 Pre-trained Encoder and Momentum Encoder 3.3 Soft Data Augmentation 3.4 Multimodal Contrastive Learning 4. Experimental Setup 5. Results and Analysis 5.1 Overall Results 5.2 Ablation Study 5.3 Applying to Pre-trained Models 5.4 Hyperparameter Study 5.5 Qualitative and Quantitative Analysis 6. Conclusion", "methodology": ["Uses pre-trained code/query encoders initialized with UniXcoder", "Employs momentum encoders to generate consistent negative sample representations", "Proposes four soft data augmentation (SoDa) methods: dynamic masking, dynamic replacement, dynamic replacement of specified type, dynamic masking of specified type", "Utilizes multimodal contrastive learning with intra-modal loss (for unimodal data) and inter-modal loss (for aligning code-query pairs)"], "example": "For the code snippet 'def save_file(dataframe, filename): df = dataframe; df.to_csv(filename, sep=',', encoding='utf-8', index=False)', dynamic replacement could replace tokens like 'filename' with their type '<Identifier>', resulting in the augmented version: 'def save_file(dataframe, <Identifier>): df = dataframe; df.to_csv(<Identifier>, sep=',', encoding=<String>, index=False)'"}
{"url": "http://arxiv.org/pdf/2204.02787v2", "keywords": ["code change search", "query language", "indexing", "precise matching", "scalable retrieval"], "overall_summary": "The paper presents DiffSearch, a scalable and precise search engine for finding code changes that match a given query describing the desired change pattern. It introduces a query language, an indexing approach for scalability, and an exact matching algorithm for precision.", "structure": "1. Introduction, 2. Example and Overview, 3. Approach (Query Language, Representations, Indexing, Retrieval, Matching), 4. Implementation, 5. Evaluation, 6. Related Work, 7. Conclusion", "methodology": ["A query language extending the target programming language with wildcards and placeholders", "Parsing code changes and queries into a feature space representation", "Indexing feature vectors of code changes for scalable retrieval", "Approximate retrieval of candidate matches based on the index", "Precise tree-based matching to filter true positive results"], "example": "To find code changes that swap arguments in a function call used in a condition, the query could be: \n\nif(ID<1>(EXPR<1>, EXPR<2>)) { \n    <...> \n}\n!\nif(ID<1>(EXPR<2>, EXPR<1>)) {\n    <...>\n}"}
{"url": "http://arxiv.org/pdf/2204.02765v3", "keywords": ["code search", "code retrieval", "query expansion", "code representation", "ranking search results"], "overall_summary": "This paper provides a comprehensive survey of 30 years of research on techniques for searching and retrieving relevant code examples from large code corpora. It covers different types of queries, query preprocessing and expansion, indexing and retrieval approaches, ranking methods, and empirical studies.", "structure": {"1. Introduction": "Motivates code search and outlines the scope of the survey", "2. Queries for Searching Code": "Discusses different kinds of queries supported by code search engines", "3. Query Preprocessing and Expansion": "Techniques for preprocessing and expanding queries", "4. Indexing and Retrieval": "Core techniques for indexing code and retrieving relevant examples", "5. Ranking and Pruning Results": "Methods for ranking and filtering search results", "6. Empirical Studies": "Studies of how developers use code search in practice", "7. Challenges and Opportunities": "Open challenges and future research directions"}, "methodology": ["Representing queries and code in vector spaces using techniques like word embeddings", "Information retrieval methods like term matching, TF-IDF, BM25", "Machine learning models like neural networks and transformers", "Program analysis techniques to extract code representations", "Ranking based on similarity scores, clustering, pruning irrelevant results"], "example": "One approach supports queries with 'holes' like: \n\ntry {\n    File file = File.createTempFile(\"foo\", \"bar\");\n    //??? code here\n} catch (IOException e) {\n    //??? \n}\n\nIt retrieves code snippets that can fill in the holes to complete the implementation."}
{"url": "http://arxiv.org/pdf/2203.15287v2", "keywords": ["deep hashing", "code search acceleration", "code classification", "recall and re-rank", "binary hash codes"], "overall_summary": "The paper proposes CoSHC, a novel approach to accelerate deep learning-based code search by integrating deep hashing and code classification. It decouples the search into a recall stage using binary hash codes and a re-rank stage, achieving over 90% retrieval time reduction while preserving at least 99% accuracy.", "structure": "1. Introduction 2. Background 2.1 Code Search 2.2 Deep Hashing 3. Method 3.1 Offline Stage - Multiple Code Hashing Design with Code Classification Module - Deep Hashing Module 3.2 Online Stage - Recall and Re-rank Mechanism - Description Category Prediction Module", "methodology": ["Cluster code embeddings into categories using K-Means", "Train deep hashing module to generate binary hash codes preserving embedding similarities", "Use category prediction module to estimate number of candidates per category", "Recall candidates based on Hamming distance between query and code hash codes", "Re-rank recalled candidates using original embeddings and cosine similarity"], "example": "For a query 'sort a list of integers', the category prediction module estimates it belongs to the 'sorting' category with high probability. The system retrieves the top N candidates in the 'sorting' category based on Hamming distances to the query's hash code. These candidates are then re-ranked using the original embeddings to produce the final ranked list of relevant code snippets."}
{"url": "http://arxiv.org/pdf/2203.07736v4", "keywords": ["code search", "relevance matching", "semantic matching", "co-attention", "neural IR"], "overall_summary": "The paper proposes CSRS, a novel code search model that combines relevance matching based on neural IR techniques and semantic matching using co-attention to improve retrieval of relevant code snippets for natural language queries. CSRS outperforms previous state-of-the-art models on a large code search dataset.", "structure": "1. Introduction, 2. Related Work, 3. Preliminaries, 4. Proposed Model (CSRS), 5. Experiments, 6. Results, 7. Threats to Validity, 8. Conclusion", "methodology": ["CNN-based embedding module to extract n-gram embeddings from code and queries", "Relevance matching module using neural IR to measure lexical keyword matching signals", "Semantic matching module with co-attention to capture semantic correlation between code and query", "Combined relevance and semantic matching features fed into MLP to produce final ranking score"], "example": "For the query 'Reallocates an array with a new size' and example code snippet provided, the model can identify lexical matches like 'array', 'new', 'size' through the relevance module, while also capturing the semantic intent of reallocating/resizing an array in the semantic module."}
{"url": "http://arxiv.org/pdf/2203.07722v1", "keywords": ["retrieval-augmented code completion", "code-to-code retrieval", "contrastive learning on code", "semantic-preserving code transformations", "hybrid retriever"], "overall_summary": "This paper proposes ReACC, a retrieval-augmented code completion framework that retrieves similar code snippets from a codebase to assist in predicting the following tokens for an unfinished code snippet. It combines a code retriever trained with contrastive learning and an autoregressive language model.", "structure": "1. Introduction 2. Related Work 3. Approach: 3.1 Task Formulation, 3.2 Retriever, 3.3 Generator 4. Experiments 5. Conclusion", "methodology": ["Uses a hybrid retriever combining sparse (BM25) and dense retrievers", "Dense retriever is trained with in-batch negatives and contrastive loss", "Data augmentation via semantic-preserving code transformations like identifier renaming and dead code insertion", "Concatenates retrieved code with original context as input to autoregressive language model"], "example": "For the Python code snippet `def normalize(a): ma = np.mean(a) sa = np.std(a) return (a-ma)/sa`, the retriever may retrieve a similar function like `def standardization(arr): mu = np.mean(arr) std = np.std(arr) return (arr - mu)/std` to help predict the remaining tokens."}
{"url": "http://arxiv.org/pdf/2203.04519v1", "keywords": ["live-coding screencast identification", "video frame classification", "IDE window detection", "video content analysis", "programming video search"], "overall_summary": "This paper presents PSFinder, a tool to automatically identify live-coding screencasts from online videos. It uses a frame sampling strategy and a classifier to detect IDE windows in video frames, and then applies rules to determine if a video qualifies as a live-coding screencast.", "structure": "1. Introduction 2. Approach 2.1. Frame Sampler 2.2. Video Classifier 2.2.1. Frame-Level Classifier 2.2.2. Video-Level Classification Strategy 3. Preliminary Experiment 3.1. Dataset 3.2. Experimental Settings 3.3. Research Questions 4. Results 4.1. Effectiveness in Video Classification 4.2. Analysis of Misclassified Videos 5. Related Work 6. Conclusion and Future Work", "methodology": ["- Extract video frames at 1 frame per 30 seconds", "- Mark duplicate frames using normalized root-mean-square error (NRMSE) on pixel values", "- Train a Vision Transformer (ViT) model to classify frames as containing an IDE window or not", "- Identify live-coding screencasts based on: 1) Presence of a minimum contiguous sequence of IDE frames, 2) Proportion of IDE frames exceeds a threshold"], "example": "For a video introducing the GPars Java library, PSFinder misclassified it as a live-coding screencast. This video contained IDE screenshots along with a moving live shot of the presenter, causing the frame sampling to not mark the IDE screenshot frames as duplicates. The continuous movement led PSFinder to assume it was a live coding session."}
{"url": "http://arxiv.org/pdf/2202.09806v2", "keywords": ["constraint discovery", "inductive logic programming", "bias discovery", "answer set programming", "program synthesis"], "overall_summary": "The paper introduces a novel approach to improve inductive logic programming (ILP) by automatically discovering constraints on hypotheses before searching for a solution. By analyzing the background knowledge, the approach identifies properties like irreflexivity and functional dependencies to build optimally sound constraints that prune the hypothesis space, leading to substantial performance gains.", "structure": "1. Introduction 2. Related Work 3. Problem Setting 4. BK Constraint Discovery 4.1 Properties 4.2 Constraints 5. Experiments 6. Conclusion", "methodology": ["Use a bottom-up approach implemented in answer set programming (ASP) to discover relational properties and functional dependencies from the background knowledge", "Generate constraints corresponding to the discovered properties to prohibit hypotheses violating those properties", "Use the generated constraints to bootstrap a constraint-driven ILP system (DISCO based on POPPER) to prune the hypothesis space before searching for a solution"], "example": "For the list transformation example with background knowledge about head/2, tail/2, odd/1, even/1, the approach can deduce that tail/2 is irreflexive, asymmetric and antitransitive, and odd/1 and even/1 are mutually exclusive. This allows pruning rules like h <- tail(A,A) and h <- head(A,B), odd(B), even(B) from the hypothesis space before search."}
{"url": "http://arxiv.org/pdf/2202.09555v1", "keywords": ["probabilistic programming idiom", "active knowledge acquisition", "robot exploration", "variational inference", "cognitive architecture"], "overall_summary": "This paper derives and implements a probabilistic programming idiom for making decisions to acquire new knowledge about an environment. The idiom is validated through simulations for the active mapping and robot exploration problem.", "structure": "1. Introduction 2. Preliminaries 3. Decision Model Derivation 4. Application to Active Mapping and Exploration 5. Simulation Results 6. Conclusion", "methodology": ["Derives a probabilistic model dividing cognitive tasks into learning and planning components", "Defines distributions for progress, information gain, constraints, and attention variables to guide exploration", "Uses variational inference and stochastic gradient ascent for approximate inference", "Employs KL divergence approximations to quantify progress and information gain"], "example": "For active mapping, the progress variable quantifies how different the current map state is from past states using an approximation of the KL divergence between the distributions. The information gain encourages exploring unknown areas by maximizing the entropy of occupied/free space cells."}
{"url": "http://arxiv.org/pdf/2202.08029v1", "keywords": ["context-aware code translation", "shared word mapping", "code search", "instruction simulation", "translation rules"], "overall_summary": "The paper proposes a novel context-aware code translation technique called TranCS that translates code snippets into natural language descriptions to bridge the representation gap between code and queries for improved code search. It uses instruction simulation and translation rules to capture code semantics and a shared word mapping to reduce embedding discrepancy.", "structure": "1. Introduction - Motivation and overview 2. Background - Machine instructions, deep learning code search 3. Motivation - Limitations of existing code representations 4. Methodology - Context-aware translation, shared word mapping 5. Experimental Evaluation 6. Related Work 7. Conclusion", "methodology": ["Disassemble code to get instruction sequence", "Simulate instruction execution to collect context (variables, dependencies, etc.)", "Apply translation rules to generate natural language description from instructions+context", "Use shared word mapping function/vocabulary to generate embeddings for descriptions and queries"], "example": "For the code snippets calculating array sum in for and while loops, TranCS generates the translations: \n0: push int constant 0. 1: store int 0 into local variable sum/result. ... 22: load int value_5 from local variable sum/result. 23: return int value_5 from method. The translations capture the semantics using natural language while the variable names (sum vs result) are the only difference, allowing shared embeddings."}
{"url": "http://arxiv.org/pdf/2202.06649v1", "keywords": ["code search dataset cleaning", "rule-based syntactic filtering", "variational autoencoder", "semantic query filtering", "bootstrap query corpus"], "overall_summary": "This paper proposes an automated framework to clean code search datasets by filtering out noisy and unnatural queries. It uses a rule-based filter to remove syntactic anomalies and a variational autoencoder model to identify semantically invalid queries.", "structure": "1. Introduction 2. Preliminaries 3. Data Cleaning Framework 4. Experimental Setup 5. Results 6. Discussion 7. Related Work 8. Conclusion", "methodology": ["Rule-based syntactic filter to remove comments with HTML tags, URLs, short sentences etc.", "Variational autoencoder trained on a 'bootstrap query corpus' from Stack Overflow question titles", "VAE reconstruction loss used to identify comments semantically divergent from natural queries", "EM-GMM clustering to separate qualified and unqualified comments based on VAE loss"], "example": "For the comment 'Returns a {@link Support}', the rule-based filter would remove the Javadoc tag {@link Support}, while the VAE filter would identify that the semantics of 'Returns a' diverges from natural queries like 'convert string to JSON object'."}
{"url": "http://arxiv.org/pdf/2201.11313v1", "keywords": ["semantic code search", "multi-modal embedding", "self-attention pooling", "representation fusion", "CodeSearchNet benchmark"], "overall_summary": "This paper proposes a novel deep semantic model for code search that aligns cross-lingual embeddings for multi-modal learning and combines different learned representations using self-attention pooling. The model achieves state-of-the-art results on the CodeSearchNet benchmark.", "structure": "1. Introduction 2. Related Work 3. Deep Semantic Code Search 3.1 Task Definition 3.2 Deep Match Framework 3.3 Aligning Contextual Embedding 3.4 Self Attention Pooling 3.5 Fusion Representations 3.6 Tokenization 3.7 Learning the Model 4. Indexing and Querying 5. Experiments 6.1 Evaluation Methodology 6.2 Results 7. Conclusion 8. Acknowledgements", "methodology": ["Aligns cross-lingual embeddings using a linear map to project code token vectors to a shared semantic space", "Uses self-attention pooling to aggregate encoder outputs into single vector representations for query and code", "Fuses different aggregated vectors using a weighted sum to obtain a combined representation", "Employs a cosine similarity loss to encourage similar embeddings for matching query-code pairs during training"], "example": "For the query 'how to read a file in python', the model would embed the query using the description encoder, and retrieve code snippets with high cosine similarity in the shared embedding space, such as: with open('file.txt', 'r') as f: contents = f.read() "}
{"url": "http://arxiv.org/pdf/2201.10866v3", "keywords": ["contrastive code-text pretraining", "unsupervised code-code pair mining", "cross-language code representation", "function-level code semantics", "large-scale code corpus"], "overall_summary": "This paper proposes CodeRetriever, a model that learns function-level code semantic representations through large-scale contrastive pre-training on code-text and code-code pairs. It achieves new state-of-the-art performance on 11 code search benchmarks across 6 programming languages.", "structure": "1. Introduction - Motivation and limitations of existing code pre-training 2. Preliminary on code search 3. Approach - Unimodal/bimodal contrastive losses, overall objective 4. Building positive pairs - Code-document, code-comment, unsupervised code-code 5. Experiments - Datasets, results analysis 6. Related work 7. Conclusion", "methodology": ["Siamese encoder architecture with code encoder and text encoder", "Unimodal contrastive loss on code-code pairs to learn code semantics", "Bimodal contrastive loss on code-text pairs using documentation and comments", "Unsupervised mining of code-code pairs based on name/doc matching and denoising"], "example": "For the Fibonacci number example, the model can learn that two different implementations 'def Fibonacci(n):' and 'def Fibonacci_Number(index):' are semantically related by contrasting the code pairs along with their documentation 'Return/Get the Fibonacci number'."}
{"url": "http://arxiv.org/pdf/2201.09974v1", "keywords": ["clarifying questions", "query refinement", "source code search", "natural language processing", "task extraction"], "overall_summary": "The paper proposes a method called ZaCQ (Zero-aspect Clarifying Question) to generate clarifying questions for refining source code search queries using natural language processing techniques. The key novelty is dynamically identifying relevant query aspects from the search results to ask targeted clarifying questions, without relying on predefined aspects.", "structure": "1. Introduction 2. Background and Related Work 2.1 Clarifying Questions for Query Refinement 2.2 Query Refinement in Software Engineering 3. Approach 3.1 Overview 3.2 Task Extraction 3.3 Clarifying Question Generation 3.4 Result Reranking 4. Evaluation 4.1 Synthetic Evaluation 4.2 Human Evaluation 5. Conclusion", "methodology": ["Extract development 'tasks' (verb+object phrases) from query and search results using natural language processing", "Identify potential query aspects by finding common/differing task attributes across results", "Generate clarifying question targeting a salient aspect to confirm relevance or elicit missing information", "Based on user's response, update query representation and rerank search results"], "example": "For the query 'convert float', ZaCQ may extract tasks like 'convert float to int' and 'convert float to string' from the results. It could then ask a clarifying question like 'Would you like to convert a float to an integer or to a string?' to disambiguate the user's intent."}
{"url": "http://arxiv.org/pdf/2201.00150v6", "keywords": ["few-shot meta learning", "cross-domain code search", "model-agnostic meta-learning (MAML)", "transfer learning", "domain adaptation"], "overall_summary": "This paper proposes CroCS, a novel approach for cross-domain code search that employs few-shot meta learning to adapt a pre-trained code model from data-rich languages to low-resource domain-specific languages. It outperforms conventional fine-tuning methods, especially when data is scarce.", "structure": "1. Introduction 2. Background 2.1 Deep learning for code search 2.2 Pre-trained models like CodeBERT 2.3 Meta learning and few-shot learning 3. Approach 3.1 Overview of CroCS 3.2 Pre-training 3.3 Meta learning with MAML 3.4 Fine-tuning 3.5 Code search 4. Experiments 5. Related Work 6. Conclusion", "methodology": ["Pre-train a code representation model (e.g. CodeBERT) on a large corpus of common programming languages", "Employ model-agnostic meta-learning (MAML) to adapt the pre-trained model to the target domain-specific language", "Fine-tune the meta-learned model on code search data of the target language", "Use the fine-tuned model for code search by computing vector similarities between queries and code"], "example": "For example, to adapt a pre-trained Python/Java model to Solidity smart contracts, CroCS first pre-trains on Python/Java data. It then uses MAML to learn good initial parameters from the Python/Java data that can quickly adapt to the Solidity domain with just a few Solidity examples. After meta-learning, it fine-tunes the model on a Solidity code search dataset to learn the final mapping between natural language and Solidity code."}
{"url": "http://arxiv.org/pdf/2111.14139v1", "keywords": ["smart contract code search", "contract elements dependency graph", "multi-modal embedding", "multi-head attention", "pretrained language model"], "overall_summary": "This paper proposes a multi-modal neural code search model called MM-SCS tailored for smart contracts, which incorporates a novel graph representation called Contract Elements Dependency Graph (CEDG) to capture code structure. It also uses multi-head attention and a pretrained language model to improve code and query embeddings.", "structure": "1. Introduction 2. Background and Related Work 3. Contract Elements Dependency Graph 4. Multi-Modal Smart Contract Code Search Model 5. Experiments 6. Conclusion", "methodology": ["Construct a Contract Elements Dependency Graph (CEDG) to represent control-flow, data-flow and unique elements in smart contracts", "Use multi-head self-attention to generate embeddings for code tokens, function names and API sequences", "Use a modified graph attention network to embed the CEDG graph", "Fine-tune a pretrained ALBERT model as the query encoder"], "example": "For the smart contract function 'function withdraw() public { ... }' with query 'Withdraw all the deposits of the caller?', MM-SCS would construct a CEDG with nodes for the withdraw function, variables like amount, control-flow edges for the function body, etc. It would use multi-head attention to embed the code tokens, name 'withdraw', and relevant APIs, and the graph attention network to embed the CEDG. The query would be encoded by the pretrained ALBERT model. The embeddings are compared to retrieve relevant code snippets."}
{"url": "http://arxiv.org/pdf/2111.02671v5", "keywords": ["graph neural networks", "code search", "multi-head attention", "global dependencies", "bidirectional GGNN"], "overall_summary": "This paper proposes GraphSearchNet, a novel neural network framework that enhances graph neural networks (GNNs) to better capture global dependencies for accurate semantic code search. It uses bidirectional GGNNs with multi-head attention to jointly learn rich semantics from source code and natural language queries.", "structure": {"1. Introduction": "Motivates the problem of code search and limitations of existing approaches", "2. Background and Motivation": "Provides background on GNNs, existing datasets, and multi-head attention", "3. Approach": "Describes the proposed GraphSearchNet framework", "4. Experimental Setup": "Details the datasets, baselines, and evaluation metrics", "5. Results": "Presents empirical results comparing GraphSearchNet to baselines", "6. Discussion": "Analyzes GraphSearchNet's performance and characteristics", "7. Related Work": "Surveys related work on code search and GNNs", "8. Conclusion": "Summarizes the key contributions"}, "methodology": ["Construct graphs for source code (using AST, data flow edges) and natural language queries (using dependency parsing)", "Use bidirectional GGNNs (BiGGNNs) to capture local structural information in the graphs", "Enhance BiGGNNs with multi-head attention to capture global dependencies missed by GGNNs", "Train the model on (code, summary) pairs to learn semantic mappings", "At inference, encode queries and retrieve top-k code candidates based on vector similarity"], "example": "For the function 'next_power_of_2', GraphSearchNet can capture the global dependency between variables n4 and n6 that traditional GGNNs would miss due to the lack of a direct path between them in the graph."}
{"url": "http://arxiv.org/pdf/2111.03466v1", "keywords": ["reinforcement learning", "integer programming", "large neighborhood search", "graph neural networks", "factorized action space"], "overall_summary": "The paper proposes a deep reinforcement learning method to learn large neighborhood search policies for solving integer programming problems more efficiently. The key novelty is using a graph neural network to factorize the exponentially large action space of variable subsets into binary decisions per variable.", "structure": {"1. Introduction": "Motivates learning heuristics for integer programs and large neighborhood search", "2. Related Work": "Discusses prior work on learning for specific COPs vs integer programs, and RL with large action spaces", "3. Preliminaries": "Defines integer programming, large neighborhood search", "4. Methodology": "Formulates the LNS as an MDP, factorizes the action space, describes the GNN policy network and training algorithm", "5. Experiments": "Evaluates the method on 4 NP-hard benchmarks and compares to baselines", "6. Conclusion": "Summarizes the key contributions"}, "methodology": ["Formulate the LNS for integer programs as a Markov Decision Process (MDP)", "Factorize the exponentially large action space of variable subsets into binary decisions per variable", "Parametrize policies for each variable using a Graph Neural Network (GNN) that shares parameters across variables", "Train the GNN policy network using a customized actor-critic reinforcement learning algorithm"], "example": "For an IP with 4 variables and 3 constraints, the GNN policy network takes as input the current solution, constraint matrix, and dynamic statistics. It outputs 4 probabilities, one per variable, indicating whether to reoptimize that variable. The selected variable subset is then reoptimized by an IP solver as the repair operator."}
{"url": "http://arxiv.org/pdf/2110.12485v1", "keywords": ["distribution-based search", "program synthesis", "enumerative search", "sampling algorithms", "parallel search"], "overall_summary": "The paper introduces a framework called distribution-based search for neural program synthesis, along with two new search algorithms - Heap Search (enumerative) and SQRT Sampling (probabilistic). Theoretical analysis and empirical evaluation show these algorithms can effectively integrate with neural models to solve complex program synthesis tasks at scale.", "structure": {"1. Introduction": "Motivates program synthesis and using machine learning models to guide search", "2. Distribution-based search": "Defines the theoretical framework and loss metric for search algorithms", "3. Enumerative methods": "Introduces the new Heap Search enumerative algorithm", "4. Sampling methods": "Analyzes sampling algorithms and proposes SQRT Sampling", "5. Parallel implementations": "Describes extensions for parallel execution", "6. Experiments": "Evaluates the new and existing algorithms on benchmarks"}, "methodology": ["Define a probabilistic context-free grammar (PCFG) from the domain-specific language", "Use a neural network to predict probabilities over the PCFG rules", "Search through programs sampled from the induced distribution to find one matching the input-output examples", "Heap Search enumerates programs in decreasing probability order using heap data structures", "SQRT Sampling draws programs from the square root of the PCFG distribution"], "example": "For the distribution D(n) = 1/(2^(n+1)) over natural numbers n, the SQRT Sampling algorithm samples from the distribution sqrt(D(n)) = 1/((1+sqrt(2))*(2^(n+1/2))), which is theoretically optimal among all sampling algorithms for D."}
{"url": "http://arxiv.org/pdf/2110.11536v2", "keywords": ["bidirectional program search", "inverse semantics", "execution-guided program synthesis", "abstraction via compression", "visual reasoning"], "overall_summary": "The paper proposes two novel approaches for solving visual reasoning tasks from the Abstraction and Reasoning Corpus (ARC): 1) using program synthesis with compression to learn abstractions, and 2) a bidirectional neural-guided search algorithm that leverages inverse semantics for deductive reasoning.", "structure": "1. Introduction 2. Abstraction using DreamCoder 2.1 Warmup: Forming Abstractions 2.2 Enabling Generalization on ARC Symmetry Tasks 2.3 Discussion 3. Bidirectional, Neural-guided Program Search 3.1 Algorithm Description 3.2 Experiments", "methodology": ["Use DreamCoder program synthesis system to create symbolic abstractions from solved tasks via compression", "Extend execution-guided program synthesis with deductive reasoning based on function inverse semantics", "Formulate as a reinforcement learning problem over a graph of grounded/ungrounded nodes", "Use neural networks to guide search over programs applying functions forward, inversely or conditionally inversely"], "example": "For the ARC task of arranging copies of the input grid based on the most common color (Figure 4), the reasoning steps are: 1) Notice output has copies of input arranged in a pattern, 2) Determine where to place copies, 3) Observe placements match pixels of a color in input, 4) Deduce that color is the most common one, 5) Solution is to place copies along pixels of most common color."}
{"url": "http://arxiv.org/pdf/2110.08512v1", "keywords": ["augmented code", "code retrieval", "natural language resources", "docstrings", "code comments", "commit messages"], "overall_summary": "This paper introduces AugmentedCode, a framework that leverages natural language resources like docstrings, code comments, and commit messages to improve the performance of code retrieval models. Experiments show significant improvements over baselines like CodeSearchNet and CodeBERT.", "structure": "1. Introduction 2. AugmentedCode Framework 2.1 Why (Objective and Goal) 2.2 What (Docstrings, Code Comments, Commit Messages) 2.3 How (Augmented Code Scenarios) 3. Experiments 3.1 Results 4. Demonstration 5. Conclusion", "methodology": ["Constructed different Augmented Code Scenarios (ACS) using combinations of code comments, docstrings, commit messages", "Trained code retrieval models on CodeSearchNet and CodeBERT architectures using the ACS data", "Evaluated models using Mean Reciprocal Rank (MRR) on test sets", "Analyzed performance on larger datasets to test scalability"], "example": "For the ACS=4 scenario, which combined code comments and entire docstrings with the code, the model achieved an MRR of 0.961 on CodeBERT, significantly outperforming the baselines. Even on a 45x larger search space, it maintained an MRR of 0.821, showing strong scalability."}
{"url": "http://arxiv.org/pdf/2110.08423v2", "keywords": ["Monte Carlo Tree Search", "Mixed Integer Programming", "Backdoor Sets", "Branch-and-Bound", "Constraint Optimization"], "overall_summary": "The paper proposes a Monte Carlo Tree Search (MCTS) framework called BaMCTS for finding small 'backdoor' sets of integer variables that can be branched on to solve Mixed Integer Programs (MIPs) efficiently. BaMCTS outperforms existing sampling-based methods by better balancing exploration and exploitation.", "structure": "1. Introduction, 2. Related Work, 3. Technical Background (MIP, Backdoors, MCTS), 4. BaMCTS Method, 5. Experimental Evaluation, 6. Conclusion", "methodology": ["Formulates backdoor search as a single-player deterministic game", "Uses MCTS with customized selection, expansion, simulation, and backpropagation steps", "Incorporates MIP domain knowledge like reward functions, action scoring, and elimination rules", "Tightly integrates with the CPLEX MIP solver for candidate evaluation"], "example": "For a MIP with integer variables I = {1, 2, 3} and backdoor size limit K = 2, the MCTS state space is P(I, K) = {(), (1), (2), (3), (1,2), (1,3), (2,1), (2,3), (3,1), (3,2)}. Evaluating the terminal state (1,3) involves running CPLEX branching only on variables 1 and 3 to check if it solves the MIP."}
{"url": "http://arxiv.org/pdf/2110.07811v1", "keywords": ["cascaded retrieval", "semantic code search", "transformer encoders", "transformer classifiers", "shared parameters"], "overall_summary": "The paper proposes CasCode, an efficient and accurate framework for semantic code search that combines fast transformer encoders for initial retrieval with slow transformer classifiers for re-ranking the top results. A shared parameter variant reduces memory overhead while maintaining high performance.", "structure": "1. Introduction - Motivation and background 2. Background - Prior encoder and classifier approaches 3. CasCode - The proposed cascaded fast/slow model approach 4. Experiments - Setup, baselines, results 5. Analysis - Ablations, parameter study, examples", "methodology": ["Use a transformer encoder to independently encode query and code snippets, retrieve top K candidates based on representation similarity", "Pass top K candidates to a transformer classifier that jointly encodes query+code and predicts match probability", "Optionally share transformer weights between encoder and classifier stages to reduce model size", "Train the shared model on a combined objective of contrastive loss (for encoder) and cross-entropy (for classifier)"], "example": "For the query 'Implement bubble sort', the fast encoder retrieves top 10 candidates like 'def insertion_sort(...)', 'def mergeSort(...)' etc. The slow classifier then re-ranks them by processing [query, code] pairs and outputting 'def bubbleSort(...)' as the top result."}
{"url": "http://arxiv.org/pdf/2109.06966v1", "keywords": ["dynamic programming", "program optimization", "semiring parsing", "search algorithms", "program transformations"], "overall_summary": "The paper proposes an automated approach to discover more efficient dynamic programming algorithms for NLP problems by searching over semantics-preserving program transformations. It defines a space of possible programs, a cost metric based on asymptotic complexity, and a set of program transformations. It then uses search algorithms like beam search and Monte Carlo tree search to find optimized programs with lower asymptotic runtime.", "structure": "1. Introduction - Motivation and background \n2. Dynamic program representation \n3. Program analysis using degree cost \n4. Program transformations \n5. Search algorithms \n6. Experiments \n7. Conclusion", "methodology": ["Represent dynamic programs in the Dyna language", "Use the 'degree' (maximum number of variables in a rule) as a proxy for asymptotic runtime complexity", "Define a set of semantics-preserving program transformations like folding, unfolding, etc.", "Formulate as a search problem to find a transformed program with minimum degree", "Use beam search and Monte Carlo tree search (MCTS) to explore the search space"], "example": "For the CKY parsing program with initial degree 6, the search finds an equivalent program with degree 5 by introducing a temporary relation tmp(I,J,X,Z) to split the summation over variables Y and (Z,J,K)."}
{"url": "http://arxiv.org/pdf/2109.05205v2", "keywords": ["contrastive quantization", "code memory", "codeword diversity regularization", "debiased contrastive learning", "unsupervised image retrieval"], "overall_summary": "This paper proposes a novel unsupervised deep quantization method called Contrastive Quantization with Code Memory (MeCoQ) that combines contrastive learning and deep quantization in a mutually beneficial framework. It introduces codeword diversity regularization to prevent model degeneration during training.", "structure": "1. Introduction 2. Related Work 3. Modeling Framework 3.1. Problem Formulation and Model Overview 3.2. Debiased Contrastive Learning for Quantization 3.3. Regularization to Avoid Model Degeneration 3.4. Quantization Code Memory 4. Experiments 5. Conclusion", "methodology": ["Uses trainable quantization with codebook attention to enable end-to-end learning", "Employs debiased contrastive loss to handle false negative samples", "Introduces codeword diversity regularization to prevent codewords in the same codebook from getting too close", "Proposes a quantization code memory module to cache codes as additional negative keys, enhancing contrastive learning"], "example": "For an input image, MeCoQ extracts continuous embeddings from a CNN, quantizes them into binary codes using the trainable quantization module, and optimizes the debiased contrastive loss between the query codes and positive/negative codes from the code memory bank. The codeword diversity regularization prevents the codewords from collapsing."}
{"url": "http://arxiv.org/pdf/2108.11601v2", "keywords": ["retrieval-augmented generation", "dense retrieval", "code generation", "code summarization", "bimodal retrieval"], "overall_summary": "The paper proposes REDCODER, a framework that retrieves relevant code or summaries from a database and uses them to augment inputs to code generation or summarization models. It extends dense retrieval techniques for this retrieval and can handle unimodal or bimodal retrieval databases.", "structure": "1. Introduction - Motivates retrieval-augmented generation 2. Background - Describes dense passage retrieval and PLBART generator 3. Proposed Framework (REDCODER) - Details the retriever (SCODE-R) and generator (SCODE-G) modules 4. Experiments - Setup, datasets, evaluation metrics 5. Results - Shows gains over baselines on code gen and summarization 6. Analysis - Analyzes different aspects of the framework", "methodology": ["Uses dense retrieval with separate encoders for code and text to retrieve relevant candidates", "Retrieves top-k candidates and concatenates them with input to generator model", "Can handle unimodal (code or text) or bimodal (code-text pairs) retrieval databases", "Adopts PLBART as the generator model without modifying its architecture"], "example": "For code generation, given an input like 'Return the sum of two integers a and b', it retrieves relevant code snippets like 'def add(a, b): return a + b', concatenates them with the input, and provides this augmented input to the PLBART generator to produce the target code."}
{"url": "http://arxiv.org/pdf/2108.09646v2", "keywords": ["term weighting", "relevance feedback", "semantic relations", "co-occurrence analysis", "thesaurus lookup"], "overall_summary": "This systematic review analyzes 70 primary studies on automated query reformulation techniques for source code search. It identifies 8 major methodologies used, evaluates their approaches, highlights key limitations, and provides recommendations for future research in this area.", "structure": "1. Introduction 2. Background 3. Review Methodology 4. Review Results 4.1 Methodologies and Algorithms 4.2 Evaluation Methods 4.3 Limitations and Challenges 4.4 ... 5. Open Challenges and Opportunities 6. Threats to Validity 7. Related Work 8. Conclusion", "methodology": ["- Term weighting algorithms for constructing queries from change requests", "- Relevance feedback mechanisms for query reformulation", "- Semantic relation analysis between terms", "- Term co-occurrence analysis for related keywords", "- Thesaurus lookup for synonyms and related terms", "- Data mining and repository mining for relevant keywords", "- API recommendation for query expansion"], "example": "One approach used term co-occurrence analysis on software repositories to identify related keywords for query expansion. For example, for the query 'sort list', it recommended adding 'array' based on frequent co-occurrences, reformulating the query to 'sort list array' to improve code search results."}
{"url": "http://arxiv.org/pdf/2108.07114v1", "keywords": ["search-based refactoring", "Scratch programming", "code transformations", "readability metrics", "multi-objective optimization"], "overall_summary": "This paper proposes a search-based approach to automatically refactor Scratch programs to improve their readability, without changing functionality. It defines a set of code transformations tailored for Scratch and uses multi-objective optimization to find sequences of these transformations that reduce complexity, size, and entropy of the programs.", "structure": "1. Introduction 2. Background 2.1 Search-based refactoring 2.2 Code quality analysis for Scratch 2.3 Code readability 3. Approach 3.1 Code transformations for Scratch 3.2 Fitness functions 3.3 Search algorithm 3.4 Transformation application 3.5 Preserving semantics 4. Evaluation 5. Threats to Validity 6. Related Work 7. Conclusion", "methodology": ["- Define 26 atomic code transformations for Scratch programs, categorized into control flow and concurrency transformations", "- Use 3 fitness functions based on program size, complexity (Halstead metrics), and entropy to measure readability", "- Apply multi-objective optimization algorithm (NSGA-II) to search for sequences of transformations that improve the fitness functions", "- Ensure semantic preservation by checking control, data, and time dependencies when applying transformations"], "example": "For example, the complex Scratch script in Figure 1a with a nested loop condition can be refactored to the simpler version in Figure 1b using transformations like 'Extract Loop Condition' and 'Forever If to Forever Wait'. This reduces the number of blocks, complexity, and entropy, improving readability."}
{"url": "http://arxiv.org/pdf/2108.05890v2", "keywords": ["code search", "transfer learning", "BERT", "multimodal embeddings", "StackOverflow dataset"], "overall_summary": "This paper investigates the use of transfer learning with BERT models for improving code search performance. Pre-trained BERT encoders for natural language queries and source code are combined into a multimodal embedding model and fine-tuned on StackOverflow data, outperforming baselines.", "structure": "1. Introduction, 2. Background, 3. Approach (pre-training, data mining, fine-tuning), 4. Experimental Evaluation, 5. Results, 6. Related Work, 7. Conclusion", "methodology": ["Pre-train BERT encoders for queries (using existing model) and code (on GitHub data) separately", "Mine StackOverflow Q&A pairs as proxy for code search (title = query, answer = code)", "Assemble pre-trained encoders into multimodal embedding model (MEM)", "Fine-tune MEM on StackOverflow code search dataset"], "example": "For the query 'how to convert string to int in java', the MEM would return code like 'int i = Integer.parseInt(intString);' by encoding the query and code into vectors, finding the closest vectors in the shared embedding space."}
{"url": "http://arxiv.org/pdf/2108.04455v1", "keywords": ["multi-fault programs", "test case transplantation", "fault lifespan", "failure clustering", "fault localization"], "overall_summary": "The paper presents a systematic approach to construct a multi-fault Java dataset by extending the popular Defects4J benchmark. By transplanting fault-revealing test cases across different faulty versions, the authors found that over 95% of the studied Defects4J versions actually contain multiple co-existing faults, ranging from 2 to 24 faults.", "structure": "1. Introduction 2. Proposed Approach 2.1 Motivating Example 2.2 Searching for Multiple Fault Versions 2.3 Implementation Details 3. Results - Multiple Fault Subjects - Lifespan of Faults 4. Conclusion", "methodology": ["Iteratively transplant fault-revealing test cases from one faulty version to previous faulty versions", "Check if transplanted tests fail with the same error message, indicating the presence of the fault", "Stop searching older versions once a fault is not revealed", "Collect all revealed faults for each faulty version to identify multi-fault subjects"], "example": "For the Math-5 fault in Defects4J, the authors found that the faulty version Math-6b also contains Math-5 by transplanting the fault-revealing test case testReciprocalZero() from Math-5b to Math-6b and observing the same failing behavior."}
{"url": "http://arxiv.org/pdf/2108.02702v1", "keywords": ["crowd knowledge mining", "semantic gap", "word embeddings", "sentence embeddings", "convolutional neural networks", "social features", "antonym handling"], "overall_summary": "This paper proposes CRAR, an approach to improve the retrieval of relevant programming solutions from Stack Overflow by combining various features like word embeddings, sentence embeddings, social signals, and handling antonyms. CRAR outperforms the previous state-of-the-art CROKAGE in retrieving complete solutions with both code examples and explanations.", "structure": "1. Introduction 2. State-of-the-Art Limitations 3. CRAR Architecture 3.1. Constructing Dictionary of Antonyms 3.2. Constructing Models, Maps, and Indices 3.3. Searching for Relevant Answers 3.4. Features 4. Experimental Evaluation 5. Qualitative Discussion 6. Threats to Validity 7. Related Work 8. Conclusion", "methodology": ["Construct dictionary of antonyms from multiple sources", "Build dataset, process Q&A, and index threads and answers from Stack Overflow", "Employ various features like word/sentence embeddings, social signals, antonym handling", "First retrieve relevant threads, then select relevant answers within those threads", "Combine multiple features into a relevance score to rank answers"], "example": "For the query 'How to resize images in Java?', CROKAGE did not retrieve any of the 37 relevant answers in the top-10, while CRAR could leverage thread popularity and retrieve relevant high-scoring answers like those with IDs 244177, 5951429 and 4528136 from the same popular thread."}
{"url": "http://arxiv.org/pdf/2107.04773v2", "keywords": ["ensemble learning", "data augmentation", "code representation", "multi-perspective modeling", "semantic code search"], "overall_summary": "This paper proposes MuCoS, a multi-model ensemble learning architecture for semantic code search that combines several individual learners focused on different perspectives of code snippets. Data augmentation is used to generate datasets emphasizing structure, variables, and API usage, which are then used to train separate models that are ensembled.", "structure": "1. Introduction 2. Proposed Model: MuCoS 2.1 Data Augmentation/Separation 2.2 Individual Model Fine-Tuning and Model Combination 3. Experimental Setup 3.1 Dataset 3.2 Evaluation Metrics 4. Results 4.1 RQ1: Performance vs SOTA 4.2 RQ2: Performance on Small Dataset 4.3 RQ3: Contribution of Individual Models 4.4 Case Study 5. Related Work", "methodology": ["- Use data augmentation to generate datasets focused on code structure, variables, and API usage", "- Train separate CodeBERT models on each augmented dataset to get structure, variable, and API-focused models", "- Ensemble the individual models by concatenating their embeddings and adding an MLP classifier on top", "- Fine-tune the ensemble on the original dataset using cross-entropy loss"], "example": "For the query 'get the field label', the API-focused model ranks the correct code snippet first by capturing the overlap between the query and API calls like getString(). The structure and variable models rank it 3rd and 5th respectively, while the ensembled MuCoS ranks it 1st by combining all perspectives."}
{"url": "http://arxiv.org/pdf/2107.00992v3", "keywords": ["tree-serialized code representation", "simplified semantic tree", "multimodal code encoding", "tree traversal serialization", "code search evaluation metrics"], "overall_summary": "This paper proposes using tree-serialized representations of simplified abstract syntax trees, along with multimodal learning, to improve neural code search. Novel tree serialization methods and information completeness metrics are introduced and evaluated on the CodeSearchNet dataset, showing improvements over baselines.", "structure": "1. Introduction 2. Background 3. Multimodal Representation Approach 3.1 Overview 3.2 Simplified Semantic Tree 3.3 Tree Serialization 3.4 Multimodal Learning 4. Experimental Methodology 5. Results and Analysis 5.1 Quantitative Results 5.2 Qualitative Analysis 5.3 Ablation Study 5.4 Threats to Validity 6. Related Work 7. Conclusion", "methodology": ["- Introduce Simplified Semantic Tree (SST) - a simplified form of AST highlighting semantic information", "- Serialize SST into sequences using sampling (RootPath, LeafPath) and traversal methods (SBT, LCRS)", "- Use token sequences and tree sequences as input modalities to multimodal pseudo-siamese network", "- Evaluate on CodeSearchNet dataset using common IR metrics like MRR", "- Define link coverage and node coverage metrics to quantify syntactic/semantic information completeness"], "example": "For the Python code snippet:\n\ndef birthday_marketing(self):\n    \"\"\"send birthday messages to members\"\"\"\n    today = datetime.date.today()\n    for member in self.members:\n        birthday = member.birthday\n        if self.anniversary(today, birthday):\n            member.SMS()\n\nThe Simplified Semantic Tree (SST) removes type declarations like 'self', modifiers like 'def', and replaces loops/expressions with semantic tags like 'loop' and 'literal'. The SST is then serialized into a sequence like ['module', 'literal', 'loop', 'call', 'call'] using traversal, which is used as the tree modality input along with the token modality."}
{"url": "http://arxiv.org/pdf/2106.11053v3", "keywords": ["language-guided program synthesis", "joint language-program abstraction learning", "neural program search", "compositional generative models", "hierarchical Bayesian inference"], "overall_summary": "The paper introduces LAPS (Language for Abstraction and Program Search), a technique that leverages natural language annotations to jointly learn reusable program abstractions and neural search heuristics for program synthesis. By integrating language into a hierarchical Bayesian model, LAPS improves library learning, search efficiency, and generalization across domains like string editing and image composition.", "structure": "1. Introduction 2. Related Work 3. Inductive synthesis and library learning (problem formulation) 4. Base learning algorithm: DreamCoder 5. LAPS Approach 6. Experiments and Results", "methodology": ["Extends hierarchical Bayesian formulation to jointly generate programs and natural language descriptions", "Learns a joint generative model over programs and language", "Uses language to guide learning of program abstractions via compositional generativity", "Trains a neural search model conditioned on language and program executions", "Iteratively refines program library and search model on language-annotated tasks"], "example": "For the task 'draw a large hexagon', LAPS could learn an abstraction like 'xy.(for  (move_pen (* unit_line y) (/ 2 x)))' corresponding to the language fragment 'large six gon'. This reusable 'polygon' function generalizes better than lower-level abstractions learned without language."}
{"url": "http://arxiv.org/pdf/2106.09173v1", "keywords": ["cross-language code search", "non-dominated sorting", "static analysis", "dynamic analysis", "code similarity"], "overall_summary": "The paper proposes COSAL, a novel approach for cross-language code-to-code search that combines static and dynamic code analyses using non-dominated sorting. It outperforms existing techniques in precision and recall for finding similar code across programming languages.", "structure": "1. Introduction - Motivation and challenges of cross-language code search 2. Background and examples 3. COSAL approach a. Token-based search b. AST-based search c. Dynamic analysis d. Non-dominated sorting 4. Experimental setup 5. Results 6. Discussion 7. Threats to validity 8. Related work 9. Conclusion", "methodology": ["Uses token-based, AST-based, and dynamic input/output analysis for code similarity", "Employs non-dominated sorting to rank results across the different similarity measures without aggregation", "Builds indices for tokens, generic ASTs, and I/O behavior offline", "During search, computes similarity between query and corpus for each measure", "Returns ranked list of non-dominated results balancing the similarity measures"], "example": "For a Java function to get even numbers and a Python function to get odd numbers (Figure 1a, 1b), token similarity is 0 as names differ, AST similarity is 0 as structures are different, but I/O behavior is similar as both return filtered lists of numbers. Non-dominated sorting can identify this behavioral similarity without being dominated by token/AST differences."}
{"url": "http://arxiv.org/pdf/2106.03042v1", "keywords": ["code clone search", "annotation", "keyword extraction", "identifier analysis", "clone class features"], "overall_summary": "This paper proposes a novel approach called Clone-Seeker that utilizes clone class features and keyword annotations to improve code clone search, especially for retrieving semantically similar (Type-4) clones. It generates metadata for each clone by extracting identifiers and augmenting them with keywords from the clone class description.", "structure": "1. Introduction 2. Related Work 3. Methodology 3.1 Dataset Selection 3.2 Identifier Extraction 3.3 Annotating Code Clones 3.4 Building Natural Language Document 4. Evaluation 4.1 Code-to-Code Search 4.2 Natural Language Query Search 5. Results and Discussion 6. Threats to Validity 7. Conclusion", "methodology": ["- Select clone methods from BigCloneBench dataset", "- Extract identifiers from clone methods by tokenization, normalization, splitting, etc.", "- Annotate clone classes with keywords manually or automatically", "- Build natural language document for each clone by combining identifiers and annotations"], "example": "For the 'bubble_sort' clone class, the extracted identifiers could be ['swap', 'array', 'element', 'temp', 'iterate']. If manually annotated with keywords like 'sorting, comparison', the natural language document becomes: 'swap array element temp iterate sorting comparison bubble_sort'"}
{"url": "http://arxiv.org/pdf/2105.13239v1", "keywords": ["real user web queries", "query-code matching", "contrastive learning", "data augmentation", "code search"], "overall_summary": "This paper introduces CoSQA, a new dataset of 20,604 pairs of natural language web queries and code snippets annotated for whether the code answers the query. It also proposes a contrastive learning method called CoCLR to augment the dataset for better query-code matching.", "structure": "1. Introduction 2. Related Work 2.1 Datasets 2.2 Code Search Models 3. CoSQA Dataset 3.1 Data Collection 3.2 Data Annotation 4. CoCLR: Contrastive Learning for Query-Code Matching 5. Experiments 5.1 Code Question Answering 5.2 Code Search 6. Analysis 6.1 Effect of Data Size 6.2 Effect of Negative Instances 6.3 Effect of Query Quality 6.4 Leveraging Documentation 7. Conclusion", "methodology": ["Collected real user web queries from Bing search logs related to code search", "Retrieved Python code functions from GitHub repositories", "Used a CodeBERT model to get candidate query-code pairs", "Annotated pairs via crowdsourcing on whether code answers query", "Proposed CoCLR method using contrastive loss to generate synthetic training pairs"], "example": "For the query 'python check if path is absolute path or relative path', an annotated positive example code function is:\n\ndef is_relative_url(url):\n    \"\"\"simple method to determine if a url is relative or absolute\"\"\"\n    if url.startswith(\"#\"): return None\n    if url.find(\"://\") > 0 or url.startswith(\"//\"):\n        # either 'http(s)://...' or '//cdn...' and therefore absolute\n        return False\n    return True"}
{"url": "http://arxiv.org/pdf/2105.09630v1", "keywords": ["query semantics enrichment", "reinforcement learning for code search", "semantic gap between queries and descriptions", "generating enriched queries", "hybrid ranking with original and enriched queries"], "overall_summary": "This paper proposes QueCos, a novel approach to improve code search by enriching the semantics of user queries through reinforcement learning. It generates semantic-augmented queries that better capture the intent behind short user queries, which are then used along with the original queries for improved code retrieval.", "structure": "1. Introduction 2. Methodology 2.1 Code Search (CS) 2.2 Query Semantic Enrichment (QSE) 2.2.1 QSE Model 2.2.2 Training QSE via RL 2.2.3 Optimization 2.3 Hybrid Ranking 3. Experimental Setup 3.1 Dataset 3.1.1 CodeSearchNet 3.1.2 Collected Datasets 3.2 Evaluation Metrics 3.2.1 R@k 3.2.2 MRR 3.3 Baselines 4. Results and Analysis 5. Related Work 6. Conclusion", "methodology": ["- Trains a code search (CS) model on code-description pairs to learn joint embeddings", "- Trains a query semantic enrichment (QSE) model via reinforcement learning to generate enriched queries from input queries", "- Uses advantage actor-critic (A2C) algorithm for RL, with reward based on ranking of retrieved code and similarity to ground truth description", "- Employs a hybrid ranking that combines original query, enriched query, and retrieved code embeddings"], "example": "For the query 'How to remove noise from a picture in Python?', the QSE model generates an enriched query like: 'You can initialize a 5x5 kernel matrix. OpenCV provides a function cv2.filter2D() to convolve a kernel with an image and produce the output smoothed image.' This enriched query better captures the semantics to retrieve the relevant Python code for noise removal."}
{"url": "http://arxiv.org/pdf/2104.08017v1", "keywords": ["semantic code search", "embedding models", "code embeddings", "sentence embeddings", "neural network transformation"], "overall_summary": "This paper proposes BERT2Code, a neural network-based approach that leverages pretrained embedding models for natural language (BERT) and source code (Code2Vec, CodeBERT) to perform semantic code search by learning a transformation between the embedding spaces. The method achieves reasonable performance but is limited by the quality of available code embedding models.", "structure": "1. Introduction 2. Background (sentence embeddings, code embeddings, similarity search) 3. Methodology (dataset, generating embeddings, neural network architecture, training/evaluation) 4. Results and Analysis 5. Related Work 6. Conclusion", "methodology": ["- Use Sentence-BERT to generate sentence embeddings for natural language queries", "- Use Code2Vec and CodeBERT to generate code embeddings for source code snippets", "- Train a 2-layer feed-forward neural network to transform sentence embeddings to code embeddings using Euclidean distance loss", "- For new queries, generate sentence embedding, transform to code embedding using trained network, and retrieve nearest code embeddings using FAISS"], "example": "For the natural language query 'Check if string is a valid Byte', the method generates a sentence embedding, transforms it to a code embedding vector, and retrieves the Java code snippet:\n\nprivate static final boolean checkByte(String s) throws AttributeBadValueException {\n    try {\n        short val = Short.parseShort(s);\n        if (val > 0xFF || val < 0) return false;\n        else return true;\n    } catch (NumberFormatException e) {\n        throw new AttributeBadValueException('`' + s + '` is not a Byte value.');\n    }\n}"}
{"url": "http://arxiv.org/pdf/2103.13020v3", "keywords": ["variable-based flow graph", "intermediate representation", "code search", "graph neural networks", "LLVM IR"], "overall_summary": "This paper proposes DEGRAPH CS, a novel approach for neural code search that represents source code as variable-based flow graphs derived from LLVM intermediate representation (IR). It outperforms existing methods by more precisely capturing code semantics through data/control dependencies between variables.", "structure": "1. Introduction, 2. Motivating Examples, 3. Proposed Model (Overview, Neural Code Representation, Comment Representation, Model Learning), 4. Experiments, 5. Related Work, 6. Conclusion", "methodology": ["Construct variable-based flow graphs from LLVM IR, with nodes as tokens and edges as data/control dependencies", "Optimize graphs by removing redundant nodes without changing semantics", "Use attentional gated graph neural network to embed flow graphs into vectors", "Represent natural language queries with RNN encoder", "Train model to minimize ranking loss between code and query vectors"], "example": "For code snippets that sum an array (Fig 1d-f), their ASTs differ significantly despite having the same semantics. However, the proposed variable-based flow graphs (Fig 1g-h) are nearly identical, accurately capturing the shared underlying data/control flow."}
{"url": "http://arxiv.org/pdf/2103.12797v1", "keywords": ["cross-language code retrieval", "program representation", "hierarchical filtering", "path-type-bucket index", "unsupervised translation"], "overall_summary": "This paper proposes Rpt, a novel system for retrieving program translations from a large code database (Big Code) in an efficient and effective manner. The key contribution is a generalizable program representation and a hierarchical filtering mechanism with a customized index structure for cross-language code retrieval.", "structure": "1. Introduction 2. Our Approach 2.1 Program Representation 2.2 Translation Retrieval 3. Experiments 4. Conclusion and Future Work", "methodology": ["Represents programs using a combination of structural (simplified syntax tree paths) and textual features", "Generalizes the path types across languages by substituting node types with category labels", "Implements a path-type-bucket index (Pbi) based on path type frequencies", "Uses a hierarchical filtering mechanism with structural similarity filtering followed by textual similarity filtering", "Calculates weighted sum of structural and textual similarities to rank candidates"], "example": "For a JavaScript code snippet, Rpt first parses it to a concrete syntax tree, simplifies it by pruning, extracts path types like 'Program -> BlockStatement -> ExpressionStatement -> CallExpression', generalizes the path by substituting node types with categories, and stores the path type frequencies in the Pbi index."}
{"url": "http://arxiv.org/pdf/2102.12553v1", "keywords": ["meta-interpretive learning", "inductive logic programming", "polymorphic type inference", "refinement types", "search space pruning"], "overall_summary": "This paper introduces techniques for leveraging types to prune the search space and improve the efficiency of meta-interpretive learning (MIL) for synthesizing logic programs. It presents methods for polymorphic type checking and inferring types of synthesized clauses, as well as an approach for using refinement types with SMT solvers to further constrain the search space.", "structure": "1. Introduction 2. Review of Program Synthesis 3. Untyped Meta-Interpretive Learning 4. Typed Meta-Interpretive Learning 5. Synthesis with Polymorphic Types 6. Synthesis with Refinement Types 7. Conclusion", "methodology": ["- Introduces type checking to prune nonsensical programs from the search space in MIL", "- Presents an algorithm (Metagol PT) for polymorphic type checking through unification and type inference", "- Develops an approach to leverage refinement types with SMT solvers for further search space pruning", "- Provides theoretical analysis on soundness, completeness, and search space reduction", "- Conducts experiments to evaluate search space reduction and synthesis time improvements"], "example": "For the 'droplasts' example of dropping the last element from lists, the paper shows how polymorphic type checking can significantly reduce the search space by pruning inconsistent predicate combinations. It also demonstrates using refinement types to enforce properties like 'the output list is one element shorter than the input list'."}
{"url": "http://arxiv.org/pdf/2102.05299v1", "keywords": ["CUDA autotuning", "GPU performance counters", "tuning space exploration", "nonlinear regression models", "decision tree models"], "overall_summary": "The paper presents a dataset of computation times and hardware performance counter measurements across the full tuning spaces of several CUDA benchmarks running on multiple GPU architectures. It also provides scripts to generate nonlinear regression and decision tree models to predict performance counter values from tuning parameters.", "structure": "1. Introduction 2. Data Description 3. Experimental Setup 4. Obtaining Raw Data 5. Generating Prediction Models", "methodology": ["Used Kernel Tuning Toolkit to exhaustively explore tuning spaces of 5 CUDA benchmarks on 4 GPUs", "Measured computation time and hardware performance counters for each tuning configuration", "Provided scripts to generate nonlinear least-squares regression models predicting performance counters from tuning parameters", "Provided scripts to generate decision tree models for the same prediction task"], "example": "For the GEMM benchmark on a GTX 1070 GPU, the raw tuning data file '1070-gemm-reduced_output.csv' contains measurements across the full tuning space. The script 'create_least_squares_models.R' can be used to generate multiple nonlinear regression models predicting each performance counter based on the tuning parameters, with different models for different combinations of binary tuning parameters."}
{"url": "http://arxiv.org/pdf/2101.07910v1", "keywords": ["adversarial robustness", "code embedding", "search-based testing", "code refactoring", "mutation testing"], "overall_summary": "The paper proposes a search-based testing framework called Guided Mutation (GM) to generate adversarial examples for deep neural networks used in source code processing tasks like code embedding. It uses code refactoring techniques to create semantically equivalent variants and leverages mutation testing to guide the test generation process. Empirical evaluation shows GM can reduce the performance of state-of-the-art code embedding models while having a low negative impact on regular test data.", "structure": "1. Introduction 2. Background 2.1 DNN Testing 2.2 Code Embedding 2.3 Code Adversarial Models 3. Methodology 3.1 Refactoring as Test Generation Basis 3.2 Guided Mutation Framework 4. Experiment Setup 5. Results and Analysis 6. Threats to Validity 7. Related Work 8. Conclusion", "methodology": ["Uses 10 code refactoring operators like variable/method renaming, loop changes, etc. to generate semantically equivalent code variants", "Adopts an evolutionary search strategy guided by mutation testing metrics", "Generates adversarial examples that trigger robustness issues in code embedding DNNs", "Retrains DNNs with the adversarial examples to improve robustness"], "example": "For example, applying the 'Argument Adding' refactoring to the code snippet in Figure 3(a) generates the adversarial variant in Figure 3(b). The DNN embedding produces different vectors for the original and refactored code despite having the same semantics."}
