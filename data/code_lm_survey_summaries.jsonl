{"url": "http://arxiv.org/pdf/2408.16498v1", "keywords": ["code generation evaluation", "similarity metrics", "execution-based metrics", "compilation success rate", "unit test pass rate"], "overall_summary": "This paper provides a comprehensive review of methods and metrics for evaluating the performance of large language models in code generation tasks. It analyzes different approaches, including similarity-based, execution-based, and feedback-based evaluation, while also discussing benchmark datasets and future challenges.", "structure": "1. Introduction 2. Code Generation Evaluation Metrics 2.1 Evaluation Based on Similarity 2.1.1 Traditional Similarity Metrics 2.1.2 Code-Specific Similarity Metrics 2.2 Execution-Based Evaluation 2.2.1 Compilation/Interpretation Success Rate 2.2.2 Unit Test Pass Rate 2.2.3 Other Execution Metrics 3. Benchmark Datasets and Metrics 4. Future Challenges and Opportunities", "methodology": "- Similarity-based metrics (BLEU, ROUGE, METEOR, Exact Match, Edit Distance) - Code-specific similarity metrics (CodeBLEU, data flow analysis, semantic similarity) - Compilation/interpretation success rate - Unit test pass rate - Other execution metrics (time to solve, code complexity, resource usage)", "example": "The CodeBLEU metric combines n-gram matching from BLEU with syntactic AST matching and semantic data flow matching. It analyzes the AST structures of generated and reference code to ensure not only textual similarity but also structural and logical similarity, providing a more comprehensive evaluation of code generation quality."}
{"url": "http://arxiv.org/pdf/2406.00515v2", "keywords": ["code generation", "natural language to code", "large language model evaluation", "code LLM data curation", "instruction tuning for code LLMs"], "overall_summary": "This paper provides a comprehensive survey of the latest advancements in using large language models (LLMs) for code generation, which involves generating source code from natural language descriptions. It introduces a taxonomy to categorize recent developments, covering data curation, advanced techniques, evaluation methods, and practical applications of code generation with LLMs.", "structure": "1. Introduction 2. Background on LLMs and code LLMs 3. Methodology for literature review 4. Taxonomy of LLMs for code generation 5. Detailed analysis within the taxonomy 5.1 Data curation 5.2 Advanced topics (instruction tuning, prompting, retrieval augmentation, etc.) 5.3 Evaluation methods 5.4 Applications 6. Challenges and opportunities 7. Conclusion", "methodology": ["- Transformer architecture with multi-head self-attention and feed-forward layers", "- Encoder-decoder, decoder-only, and encoder-only model variants", "- Techniques like residual connections, layer normalization, positional encoding"], "example": "As an example, the paper discusses how the HumanEval benchmark has become a de facto standard for evaluating the coding proficiency of LLMs. It shows the remarkable improvement on this benchmark from early models like PaLM 8B achieving 3.6% Pass@1 to the latest LDB model reaching 95.1% Pass@1."}
{"url": "http://arxiv.org/pdf/2404.11160v2", "keywords": ["chain-of-thought prompting", "low-cost language models", "python code generation", "cpu-friendly models", "semi-manual evaluation"], "overall_summary": "This paper evaluates the performance of low-cost, CPU-friendly language models on the task of Python code generation. It introduces a new dataset of 60 programming problems and employs chain-of-thought prompting to improve model reasoning and code quality.", "structure": "1. Introduction 2. Related Work 2.1. Python code generation 2.1.1. Closed-source models 2.1.2. Open-source models 2.2. Small Language Models 3. Problem Formulation and Models 4. Datasets 5. Engineered Prompts 6. Evaluation Methodology 7. Results and Discussion 8. Semi-Manual Evaluation 9. Conclusion and Perspectives", "methodology": ["Propose a new dataset of 60 Python programming problems with varying difficulty levels", "Introduce a chain-of-thought (CoT) prompting strategy to guide model reasoning", "Evaluate multiple low-cost, CPU-friendly models on the proposed dataset, HumanEval, and EvalPlus", "Use a quality-assessment prompt with GPT-3.5-Turbo to semi-manually evaluate model outputs", "Manually correct mistakes made by GPT-3.5-Turbo in the evaluation"], "example": "For the problem 'Write a function that takes a list of integers and returns the sum of all even numbers in the list', with variables 'nums' and options 'None', the model Llama-3.1-8B-Instruct generated the following Python code: def soln(nums): even_sum = 0 for num in nums: if num % 2 == 0: even_sum += num return even_sum This code correctly iterates through the list 'nums', checks if each number is even using the modulus operator, and accumulates the sum of even numbers in the 'even_sum' variable, finally returning the result."}
{"url": "http://arxiv.org/pdf/2404.00227v1", "keywords": ["Infrastructure as Code (IaC) generation", "Large Language Model (LLM) for code generation", "Ansible YAML generation", "Terraform configuration generation", "ChatGPT for DevSecOps"], "overall_summary": "This paper surveys the use of large language models (LLMs) for automatically generating Infrastructure as Code (IaC) configurations like Ansible YAML files and Terraform scripts. It discusses the background of IaC, LLMs for code generation, and various approaches that leverage LLMs to assist in IaC creation and analysis for DevSecOps practices.", "structure": "1. Introduction 2. Background 2.1 Infrastructure as Code (IaC) 2.2 Large Language Models (LLMs) 2.3 LLMs for Code Generation 3. IaC using LLMs: Related Works 3.1 Ansible-YAML Generation by LLMs 3.2 LLMs used in DevSecOps 3.3 IaC Generation through ChatGPT Queries", "methodology": ["Pre-trained LLMs like CodeGen, Codex, CodeParrot fine-tuned on code datasets", "Novel prompting techniques to reformulate IaC generation as code completion task", "Leveraging ChatGPT for static code analysis and runtime log analysis for security", "Querying ChatGPT to generate IaC configurations like Terraform scripts"], "example": "In one approach, the WISDOM-ANSIBLE model is pre-trained on a curated dataset of 1.1M Ansible tasks and 2.2M YAML files. It is then fine-tuned on high-quality Ansible Galaxy data. Given a natural language prompt and partial Ansible context, it can generate the remaining Ansible YAML configuration by treating it as a code completion task."}
{"url": "http://arxiv.org/pdf/2401.00812v2", "keywords": ["program-of-thought", "code execution environment", "intelligent agents", "code pre-training", "structured reasoning"], "overall_summary": "This survey paper examines how integrating code into the training data of large language models (LLMs) enhances their capabilities as intelligent agents. Code enables LLMs to improve reasoning abilities, generate executable steps, and receive automated feedback through code execution environments.", "structure": "1) Introduction 2) Preliminaries 3) Code Pre-Training Boosts LLM Performance 4) Connecting LLMs to Functional Ends 5) Executable Environment for Feedback 6) Benefits for Intelligent Agents 7) Challenges and Future Directions", "methodology": ["Pre-training LLMs on code corpora (e.g. GitHub) in addition to natural language", "Fine-tuning LLMs on predefined formal languages like function sets or math formulas", "Leveraging code's properties like logical structure, abstraction, and executability during training"], "example": "The paper cites examples like AlphaCode generating high-quality code across multiple languages, and GPT-3 models like Codex showing dramatic improvements on math reasoning tasks when using 'chain-of-thought' prompting enabled by code pre-training."}
{"url": "http://arxiv.org/pdf/2311.10372v2", "keywords": ["code LLM benchmarking", "code LLM performance evaluation", "code LLM fine-tuning", "code generation models", "software vulnerability detection models"], "overall_summary": "This paper provides a comprehensive survey of large language models (LLMs) specifically designed for software engineering tasks, referred to as Code LLMs. It examines the evolution of Code LLMs, benchmarks their performance against general LLMs across various software engineering tasks, and identifies the most proficient models for different tasks.", "structure": "1. Introduction 2. Methodology 3. LLMs for Software Engineering 3.1 Company-led LLMs 3.2 Organization-led LLMs 3.3 Research Team & Open-Source Community LLMs 3.4 Individual & Anonymous Contributor LLMs 4. Performance Comparison of Code LLMs vs General LLMs 5. Benchmarking Code LLMs Across Software Engineering Tasks 5.1 Code Generation 5.2 Code Summarization 5.3 Code Translation 5.4 Vulnerability Detection 6. Related Work 7. Conclusion", "methodology": ["Collected relevant literature from GitHub, dblp, Google Scholar, and arXiv", "Used card sorting method to remove duplicates and irrelevant papers", "Expanded paper list using snowballing approach", "Categorized Code LLMs based on affiliations of main developers", "Analyzed experimental sections comparing Code LLMs and general LLMs", "Compiled performance of LLMs on software engineering benchmarks"], "example": "For the code generation task on the HumanEval benchmark, the CodeFuse-CodeLlama-34B model achieved a pass@1 score of 51.9%, outperforming the GPT-4 model which scored 43.8%. This demonstrates the superior performance of specialized Code LLMs over general LLMs for this specific task."}
{"url": "http://arxiv.org/pdf/2311.07989v7", "keywords": ["code language models", "program synthesis", "software engineering tasks", "transformer models", "code processing"], "overall_summary": "This paper provides a comprehensive survey of the application of large language models to various software engineering tasks, covering over 70 models, 40 tasks, 180 datasets, and 900 related works. It highlights the integration of natural language processing and software engineering perspectives.", "structure": "1. Introduction 2. Downstream SE Tasks 3. Language Modeling Preliminaries 4. Large Language Models for Code 5. Specialized Code Models 6. Code-Specific Features 7. LLMs for Full Software Development 8. Challenges and Conclusion", "methodology": ["Analyzes general large language models (GPT, LaMDA, PaLM, etc.) adapted for code tasks", "Reviews specialized code models with tailored pretraining objectives and architectures", "Discusses integration of code-specific features like abstract syntax trees and data flow", "Covers application of LLMs across the full software development lifecycle"], "example": "The paper discusses how models like Codex and InCoder achieve impressive performance on the HumanEval program synthesis benchmark by scaling up and pretraining on a large corpus of code from GitHub."}
{"url": "http://arxiv.org/pdf/2310.17903v1", "keywords": ["data noise", "label errors", "unbalanced data distribution", "vulnerability detection", "code summarization"], "overall_summary": "The paper systematically reviews pitfalls in language models for code intelligence (LM4Code) across different stages of the model lifecycle. It identifies major pitfalls related to data issues like noise, label errors, and unbalanced distributions that can significantly impact model performance.", "structure": "1. Introduction 2. Study Design 3. Data Collection and Labeling Pitfalls 4. System Design and Learning Pitfalls 5. Performance Evaluation Pitfalls 6. Deployment and Maintenance Pitfalls 7. Open Challenges and Future Directions", "methodology": ["Conducted a systematic literature review following Kitchenham's guidelines", "Performed searches on major academic databases and snowballing", "Identified 67 primary studies on pitfalls in LM4Code after applying inclusion/exclusion criteria", "Qualitatively analyzed pitfalls and categorized them into 4 aspects: data, model design/learning, evaluation, deployment"], "example": "For code summarization, Shi et al. found that 31-66% of data in popular benchmark datasets contained noise like empty methods or duplicated code. Filtering this noise improved BLEU-4 metric by 21-27% when training LMs like CodeBERT."}
{"url": "http://arxiv.org/pdf/2308.01191v3", "keywords": ["code clone detection", "chain-of-thought prompting", "code embeddings", "cross-language analysis", "large language models"], "overall_summary": "This paper conducts a comprehensive evaluation of the capability of large language models (LLMs) for code clone detection across different clone types, languages, and prompting strategies. It finds that LLMs excel at detecting complex semantic clones and that chain-of-thought prompting can significantly improve their performance.", "structure": "1. Introduction 2. Background 2.1 Code Clone Detection 2.2 Large Language Models 2.3 Chain-of-Thought Reasoning 3. Experimental Setup 3.1 Research Questions 4. Results 5. Future Work 6. Conclusion", "methodology": ["Evaluated LLMs like GPT-3, GPT-4, LLaMA, Alpaca on code clone detection using different prompting strategies", "Used simple prompts asking for direct yes/no answer", "Employed one-step chain-of-thought prompts asking for analysis from different perspectives (clone type, similarity, reasoning, etc.)", "Used multi-step chain-of-thought with separate explanations for each code snippet", "Compared performance using code embeddings from CodeBERT and text encoders", "Analyzed performance variation across programming languages like Python, Java, C++, etc."], "example": "For the one-step chain-of-thought 'Integrated' prompt, the LLM was asked: 'Please analyze the following two code snippets to assess their similarity and determine if they are code clones. Provide a similarity score between 0 and 10, where a higher score indicates more similarity. Additionally, identify the type of code clone they represent and present a detailed reasoning process for detecting code clones.' This allowed the LLM to consider multiple perspectives before giving its final clone detection output."}
{"url": "http://arxiv.org/pdf/2212.10079v1", "keywords": ["code summarization", "program synthesis", "defect detection", "data flow graphs", "abstract syntax trees"], "overall_summary": "This paper provides a comprehensive survey of neural code intelligence (NCI), covering pretrained language models for understanding and generating source code. It reviews techniques for preprocessing code, neural architectures like Transformers, and pretraining objectives tailored for programming languages.", "structure": "1. Introduction 2. Pretraining Language Models for Code 2.1 Preprocessing 2.1.1 Tokenization of Source Code 2.1.2 Extracting Structures from Source Code 2.2 Neural Modeling for Code Tokens 2.2.1 Transformers in NCI Models 2.2.2 Exploiting Program Structures 2.3 Training 2.3.1 Language Model Pretraining 2.3.2 Other Training Paradigms", "methodology": ["- Tokenization strategies: BPE, SentencePiece, PL-specific tokens, compiler tokenizers, naming conventions", "- Extracting code structures: Abstract Syntax Trees (ASTs), Control Flow Graphs (CFGs), Data Flow Graphs (DFGs)", "- Neural architectures: Transformers (encoder-only, encoder-decoder, decoder-only)", "- Incorporating structures: Syntax-based self-attention, semantic-based self-attention", "- Pretraining objectives: Masked LM, Replaced Token Detection, Contrastive Learning"], "example": "The paper describes how Guo et al. (2021) incorporate data flow graphs into the Transformer's self-attention. They input the data flow graph sequence along with the code, and mask the attention based on the graph edges and correspondence between identifiers and graph nodes."}
