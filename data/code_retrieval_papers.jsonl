{"id": "http://arxiv.org/abs/2412.04057v1", "title": "From Code to Play: Benchmarking Program Search for Games Using Large Language Models", "abstract": "Large language models (LLMs) have shown impressive capabilities in generating\nprogram code, opening exciting opportunities for applying program synthesis to\ngames. In this work, we explore the potential of LLMs to directly synthesize\nusable code for a wide range of gaming applications, focusing on two\nprogramming languages, Python and Java. We use an evolutionary hill-climbing\nalgorithm, where the mutations and seeds of the initial programs are controlled\nby LLMs. For Python, the framework covers various game-related tasks, including\nfive miniature versions of Atari games, ten levels of Baba is You, an\nenvironment inspired by Asteroids, and a maze generation task. For Java, the\nframework contains 12 games from the TAG tabletop games framework. Across 29\ntasks, we evaluated 12 language models for Python and 8 for Java. Our findings\nsuggest that the performance of LLMs depends more on the task than on model\nsize. While larger models generate more executable programs, these do not\nalways result in higher-quality solutions but are much more expensive. No model\nhas a clear advantage, although on any specific task, one model may be better.\nTrying many models on a problem and using the best results across them is more\nreliable than using just one.", "authors": ["Manuel Eberhardinger", "James Goodman", "Alexander Dockhorn", "Diego Perez-Liebana", "Raluca D. Gaina", "Duygu \u00c7akmak", "Setareh Maghsudi", "Simon Lucas"], "categories": ["cs.AI"], "published": "2024-12-05T10:50:58+00:00", "updated": "2024-12-05T10:50:58+00:00", "pdf_url": "http://arxiv.org/pdf/2412.04057v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2411.17538v2", "title": "Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search", "abstract": "Low isotropy in an embedding space impairs performance on tasks involving\nsemantic inference. Our study investigates the impact of isotropy on semantic\ncode search performance and explores post-processing techniques to mitigate\nthis issue. We analyze various code language models, examine isotropy in their\nembedding spaces, and its influence on search effectiveness. We propose a\nmodified ZCA whitening technique to control isotropy levels in embeddings. Our\nresults demonstrate that Soft-ZCA whitening improves the performance of\npre-trained code language models and can complement contrastive fine-tuning.", "authors": ["Andor Diera", "Lukas Galke", "Ansgar Scherp"], "categories": ["cs.CL"], "published": "2024-11-26T15:53:28+00:00", "updated": "2024-11-27T09:43:01+00:00", "pdf_url": "http://arxiv.org/pdf/2411.17538v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2411.17230v1", "title": "Fault Localization from the Semantic Code Search Perspective", "abstract": "The software development process is characterized by an iterative cycle of\ncontinuous functionality implementation and debugging, essential for the\nenhancement of software quality and adaptability to changing requirements. This\nprocess incorporates two isolatedly studied tasks: Code Search (CS), which\nretrieves reference code from a code corpus to aid in code implementation, and\nFault Localization (FL), which identifies code entities responsible for bugs\nwithin the software project to boost software debugging. These two tasks\nexhibit similarities since they both address search problems. Notably, CS\ntechniques have demonstrated greater effectiveness than FL ones, possibly\nbecause of the precise semantic details of the required code offered by natural\nlanguage queries, which are not readily accessible to FL methods. Drawing\ninspiration from this, we hypothesize that a fault localizer could achieve\ngreater proficiency if semantic information about the buggy methods were made\navailable. Based on this idea, we propose CosFL, an FL approach that decomposes\nthe FL task into two steps: query generation, which describes the functionality\nof the problematic code in natural language, and fault retrieval, which uses CS\nto find program elements semantically related to the query. Specifically, to\ndepict the buggy functionalities and generate high-quality queries, CosFL\nextensively harnesses the code analysis, semantic comprehension, and\ndecision-making capabilities of LLMs. Moreover, to enhance the accuracy of CS,\nCosFL captures varying levels of context information and employs a\nmulti-granularity code search strategy, which facilitates a more precise\nidentification of buggy methods from a holistic view. The evaluation on 835\nreal bugs from 23 Java projects shows that CosFL successfully localizes 324\nbugs within Top-1, which significantly outperforms the state-of-the-art\napproaches by 26.6%-57.3%.", "authors": ["Yihao Qin", "Shangwen Wang", "Yan Lei", "Zhuo Zhang", "Bo Lin", "Xin Peng", "Liqian Chen", "Xiaoguang Mao"], "categories": ["cs.SE"], "published": "2024-11-26T08:52:13+00:00", "updated": "2024-11-26T08:52:13+00:00", "pdf_url": "http://arxiv.org/pdf/2411.17230v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2411.12644v2", "title": "CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval", "abstract": "Despite the success of text retrieval in many NLP tasks, code retrieval\nremains a largely underexplored area. Most text retrieval systems are tailored\nfor natural language queries, often neglecting the specific challenges of\nretrieving code. This gap leaves existing models unable to effectively capture\nthe diversity of programming languages and tasks across different domains,\nhighlighting the need for more focused research in code retrieval. To address\nthis, we introduce CodeXEmbed, a family of large-scale code embedding models\nranging from 400M to 7B parameters. Our novel training pipeline unifies\nmultiple programming languages and transforms various code-related tasks into a\ncommon retrieval framework, enhancing model generalizability and retrieval\nperformance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,\noutperforming the previous leading model, Voyage-Code, by over 20% on CoIR\nbenchmark. In addition to excelling in code retrieval, our models demonstrate\ncompetitive performance on the widely adopted BeIR text retrieval benchmark,\noffering versatility across domains. Experimental results demonstrate that\nimproving retrieval performance significantly enhances end-to-end\nRetrieval-Augmented Generation (RAG) performance for code-related tasks.", "authors": ["Ye Liu", "Rui Meng", "Shafiq Joty", "Silvio Savarese", "Caiming Xiong", "Yingbo Zhou", "Semih Yavuz"], "categories": ["cs.SE", "cs.AI"], "published": "2024-11-19T16:54:45+00:00", "updated": "2024-11-24T18:52:38+00:00", "pdf_url": "http://arxiv.org/pdf/2411.12644v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2411.11053v4", "title": "SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation", "abstract": "Large language models demonstrate exceptional performance in simple code\ngeneration tasks but still face challenges in tackling complex problems. These\nchallenges may stem from insufficient reasoning and problem decomposition\ncapabilities. To address this issue, we propose a reasoning-augmented data\ngeneration process, SRA-MCTS, which guides the model to autonomously generate\nhigh-quality intermediate reasoning paths. This creates a positive feedback\nloop, enabling continuous improvement. Our method operates entirely through the\nmodel itself without requiring additional supervision. By synthesizing natural\nlanguage reasoning paths and translating them into executable code, the\napproach ensures analytical accuracy and enhances the success rate in solving\ncomplex tasks. Experimental results show that, even without additional\nsupervisory signals, our method achieves performance improvements across\ndifferent model scales, demonstrating the significant potential of\nself-improvement in small models. Furthermore, the method remains robust when\ntraditional Chain-of-Thought (CoT) approaches exhibit performance degradation,\nwith notable improvements observed in diversity metrics such as pass@10. We\nencourage further exploration of reasoning processes within training data to\nenhance the ability of language models to address complex problems. Our code\nand data are public at https://github.com/DIRECT-BIT/SRA-MCTS.", "authors": ["Bin Xu", "Yiguan Lin", "Yinghao Li", "Yang Gao"], "categories": ["cs.CL", "cs.AI"], "published": "2024-11-17T12:31:04+00:00", "updated": "2024-11-23T12:25:17+00:00", "pdf_url": "http://arxiv.org/pdf/2411.11053v4", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2411.08706v1", "title": "Searching Latent Program Spaces", "abstract": "Program synthesis methods aim to automatically generate programs restricted\nto a language that can explain a given specification of input-output pairs.\nWhile purely symbolic approaches suffer from a combinatorial search space,\nrecent methods leverage neural networks to learn distributions over program\nstructures to narrow this search space significantly, enabling more efficient\nsearch. However, for challenging problems, it remains difficult to train models\nto perform program synthesis in one shot, making test-time search essential.\nMost neural methods lack structured search mechanisms during inference, relying\ninstead on stochastic sampling or gradient updates, which can be inefficient.\nIn this work, we propose the Latent Program Network (LPN), a general algorithm\nfor program induction that learns a distribution over latent programs in a\ncontinuous space, enabling efficient search and test-time adaptation. We\nexplore how to train these networks to optimize for test-time computation and\ndemonstrate the use of gradient-based search both during training and at test\ntime. We evaluate LPN on ARC-AGI, a program synthesis benchmark that evaluates\nperformance by generalizing programs to new inputs rather than explaining the\nunderlying specification. We show that LPN can generalize beyond its training\ndistribution and adapt to unseen tasks by utilizing test-time computation,\noutperforming algorithms without test-time adaptation mechanisms.", "authors": ["Cl\u00e9ment Bonnet", "Matthew V Macfarlane"], "categories": ["cs.LG", "cs.AI"], "published": "2024-11-13T15:50:32+00:00", "updated": "2024-11-13T15:50:32+00:00", "pdf_url": "http://arxiv.org/pdf/2411.08706v1", "primary_category": "cs.LG"}
{"id": "http://arxiv.org/abs/2411.06796v1", "title": "Automatically Write Code Checker: An LLM-based Approach with Logic-guided API Retrieval and Case by Case Iteration", "abstract": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic as well as the complex API usage of\nlarge-scale frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we explore the feasibility of automated checker generation and\npropose AutoChecker, an innovative LLM-powered approach that can write code\ncheckers automatically based on only a rule description and a test suite.\nInstead of generating the checker at once, AutoChecker incrementally updates\nthe checker with the rule and one single test case each time, i.e., it\niteratively generates the checker case by case. During each iteration,\nAutoChecker first decomposes the whole logic into a series of sub-operations\nand then uses the logic-guided API-context retrieval strategy to search related\nAPI-contexts from all the framework APIs. To evaluate the effectiveness of\nAutoChecker, we apply AutoChecker and two LLM-based baseline approaches to\nautomatically generate checkers for 20 built-in PMD rules, including easy rules\nand hard rules. Experimental results demonstrate that AutoChecker significantly\noutperforms baseline approaches across all effectiveness metrics, where its\naverage test pass rate improved over 4.2 times. Moreover, the checkers\ngenerated by AutoChecker are successfully applied to real-world projects,\nmatching the performance of official checkers.", "authors": ["Yuanyuan Xie", "Jun Liu", "Jiwei Yan", "Jinhao Huang", "Jun Yan", "Jian Zhang"], "categories": ["cs.SE"], "published": "2024-11-11T08:50:24+00:00", "updated": "2024-11-11T08:50:24+00:00", "pdf_url": "http://arxiv.org/pdf/2411.06796v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2411.05547v2", "title": "Assessing the Answerability of Queries in Retrieval-Augmented Code Generation", "abstract": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance.", "authors": ["Geonmin Kim", "Jaeyeon Kim", "Hancheol Park", "Wooksu Shin", "Tae-Ho Kim"], "categories": ["cs.CL"], "published": "2024-11-08T13:09:14+00:00", "updated": "2024-11-25T07:18:33+00:00", "pdf_url": "http://arxiv.org/pdf/2411.05547v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2411.04752v1", "title": "RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval", "abstract": "Code-mixing, the integration of lexical and grammatical elements from\nmultiple languages within a single sentence, is a widespread linguistic\nphenomenon, particularly prevalent in multilingual societies. In India, social\nmedia users frequently engage in code-mixed conversations using the Roman\nscript, especially among migrant communities who form online groups to share\nrelevant local information. This paper focuses on the challenges of extracting\nrelevant information from code-mixed conversations, specifically within Roman\ntransliterated Bengali mixed with English. This study presents a novel approach\nto address these challenges by developing a mechanism to automatically identify\nthe most relevant answers from code-mixed conversations. We have experimented\nwith a dataset comprising of queries and documents from Facebook, and Query\nRelevance files (QRels) to aid in this task. Our results demonstrate the\neffectiveness of our approach in extracting pertinent information from complex,\ncode-mixed digital conversations, contributing to the broader field of natural\nlanguage processing in multilingual and informal text environments. We use\nGPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant\ndocuments to frame a mathematical model which helps to detect relevant\ndocuments corresponding to a query.", "authors": ["Aniket Deroy", "Subhankar Maity"], "categories": ["cs.CL"], "published": "2024-11-07T14:41:01+00:00", "updated": "2024-11-07T14:41:01+00:00", "pdf_url": "http://arxiv.org/pdf/2411.04752v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2411.04329v2", "title": "CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models", "abstract": "Pre-trained on massive amounts of code and text data, large language models\n(LLMs) have demonstrated remarkable achievements in performing code generation\ntasks. With additional execution-based feedback, these models can act as agents\nwith capabilities to self-refine and improve generated code autonomously.\nHowever, on challenging coding tasks with extremely large search space, current\nagentic approaches still struggle with multi-stage planning, generating, and\ndebugging. To address this problem, we propose CodeTree, a framework for LLM\nagents to efficiently explore the search space in different stages of the code\ngeneration process. Specifically, we adopted a unified tree structure to\nexplicitly explore different coding strategies, generate corresponding coding\nsolutions, and subsequently refine the solutions. In each stage, critical\ndecision-making (ranking, termination, expanding) of the exploration process is\nguided by both the environmental execution-based feedback and\nLLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code\ngeneration benchmarks and demonstrated the significant performance gains of\nCodeTree against strong baselines. Using GPT-4o as the base model, we\nconsistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0\non CodeContests. On the challenging SWEBench benchmark, our approach led to\nsignificant performance gains.", "authors": ["Jierui Li", "Hung Le", "Yingbo Zhou", "Caiming Xiong", "Silvio Savarese", "Doyen Sahoo"], "categories": ["cs.CL"], "published": "2024-11-07T00:09:54+00:00", "updated": "2024-11-12T19:37:20+00:00", "pdf_url": "http://arxiv.org/pdf/2411.04329v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2411.01102v3", "title": "BinEnhance: An Enhancement Framework Based on External Environment Semantics for Binary Code Search", "abstract": "Binary code search plays a crucial role in applications like software reuse\ndetection. Currently, existing models are typically based on either internal\ncode semantics or a combination of function call graphs (CG) and internal code\nsemantics. However, these models have limitations. Internal code semantic\nmodels only consider the semantics within the function, ignoring the\ninter-function semantics, making it difficult to handle situations such as\nfunction inlining. The combination of CG and internal code semantics is\ninsufficient for addressing complex real-world scenarios. To address these\nlimitations, we propose BinEnhance, a novel framework designed to leverage the\ninter-function semantics to enhance the expression of internal code semantics\nfor binary code search. Specifically, BinEnhance constructs an External\nEnvironment Semantic Graph (EESG), which establishes a stable and analogous\nexternal environment for homologous functions by using different inter-function\nsemantic relations (e.g., call, location, data-co-use). After the construction\nof EESG, we utilize the embeddings generated by existing internal code semantic\nmodels to initialize nodes of EESG. Finally, we design a Semantic Enhancement\nModel (SEM) that uses Relational Graph Convolutional Networks (RGCNs) and a\nresidual block to learn valuable external semantics on the EESG for generating\nthe enhanced semantics embedding. In addition, BinEnhance utilizes data feature\nsimilarity to refine the cosine similarity of semantic embeddings. We conduct\nexperiments under six different tasks (e.g., under function inlining scenario)\nand the results illustrate the performance and robustness of BinEnhance. The\napplication of BinEnhance to HermesSim, Asm2vec, TREX, Gemini, and Asteria on\ntwo public datasets results in an improvement of Mean Average Precision (MAP)\nfrom 53.6% to 69.7%. Moreover, the efficiency increases fourfold.", "authors": ["Yongpan Wang", "Hong Li", "Xiaojie Zhu", "Siyuan Li", "Chaopeng Dong", "Shouguo Yang", "Kangyuan Qin"], "categories": ["cs.SE", "cs.CR"], "published": "2024-11-02T01:54:52+00:00", "updated": "2024-11-26T03:30:12+00:00", "pdf_url": "http://arxiv.org/pdf/2411.01102v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2410.22240v1", "title": "Are Decoder-Only Large Language Models the Silver Bullet for Code Search?", "abstract": "Code search is crucial for code reuse, enabling developers to efficiently\nlocate relevant snippets. Current methods rely on encoder-based models, which\nsuffer from limitations such as poor generalization and restricted input\nlengths. Decoder-only large language models (LLMs), with their extensive\npre-training, larger size, and longer input capabilities, offer potential\nsolutions to these issues, yet their effectiveness in code search remains\nunderexplored. To fill this gap, our study presents the first systematic\nexploration of decoder-only LLMs for code search. We evaluate nine\nstate-of-the-art decoder-only models using two fine-tuning methods, two\ndatasets (CSN and CoSQA$^+$), and three model sizes. Our findings reveal that\nfine-tuned CodeGemma significantly outperforms encoder-only models like\nUniXcoder, achieving a 5.57% improvement in MRR on CSN and a 49.6% increase in\nMAP on CoSQA$^+$ compared to zero-shot UniXcoder. These results highlight the\nsuperior performance and adaptability of decoder-only models. Additionally, we\nprovide valuable insights into optimizing these models for code search,\ncovering aspects such as model selection, fine-tuning methods, training data,\nand model size, and discussing their strengths and limitations.", "authors": ["Yuxuan Chen", "Guangsheng Ou", "Mingwei Liu", "Yanlin Wang", "Zibin Zheng"], "categories": ["cs.SE"], "published": "2024-10-29T17:05:25+00:00", "updated": "2024-10-29T17:05:25+00:00", "pdf_url": "http://arxiv.org/pdf/2410.22240v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2410.16655v1", "title": "Semantic-guided Search for Efficient Program Repair with Large Language Models", "abstract": "In this paper, we first show that increases in beam size of even just\nsmall-sized LLM (1B-7B parameters) require an extensive GPU resource\nconsumption, leading to up to 80% of recurring crashes due to memory overloads\nin LLM-based APR. Seemingly simple solutions to reduce memory consumption are\n(1) to quantize LLM models, i.e., converting the weights of a LLM from\nhigh-precision values to lower-precision ones. and (2) to make beam search\nsequential, i.e., forwarding each beam through the model sequentially and then\nconcatenate them back into a single model output. However, we show that these\napproaches still do not work via both theoretical analysis and experiments. To\naddress this, we introduce FLAMES, a novel LLM-based APR technique that employs\nsemantic-guided patch generation to enhance repair effectiveness and memory\nefficiency. Unlike conventional methods that rely on beam search, FLAMES\nutilizes greedy decoding to enhance memory efficiency while steering the search\nto more potentially good repair candidates via a semantic-guided best-first\nsearch algorithm. At each decoding step, FLAMES uses semantic feedback from\ntest validation such as the number of passing and failing test cases to select\nthe most promising token to explore further. Our empirical evaluation on the\nDefects4J and HumanEval-Java datasets shows that FLAMES not only substantially\nreduces memory consumption by up to 83% compared to conventional LLM-based APR,\nbut also accelerates the repair process. Remarkably, FLAMES successfully\ngenerated 133 and 103 correct fixes for 333 and 163 bugs in the Defects4J and\nHumanEval-Java datasets, respectively. This suggests that FLAMES is not only\nmore efficient but also outperforms state-of-the-art techniques, fixing at\nleast 10 and 11 more bugs than SOTA baselines in the Defects4J and\nHumanEval-Java datasets, respectively.", "authors": ["Thanh Le-Cong", "Bach Le", "Toby Murray"], "categories": ["cs.SE", "cs.AI"], "published": "2024-10-22T02:59:47+00:00", "updated": "2024-10-22T02:59:47+00:00", "pdf_url": "http://arxiv.org/pdf/2410.16655v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2411.05010v1", "title": "Scattered Forest Search: Smarter Code Space Exploration with LLMs", "abstract": "We propose a novel approach to scaling LLM inference for code generation. We\nframe code generation as a black box optimization problem within the code\nspace, and employ optimization-inspired techniques to enhance exploration.\nSpecifically, we introduce Scattered Forest Search to enhance solution\ndiversity while searching for solutions. Our theoretical analysis illustrates\nhow these methods avoid local optima during optimization. Extensive experiments\non HumanEval, MBPP, APPS, CodeContests, and Leetcode reveal significant\nperformance improvements. For instance, our method achieves a pass@1 rate of\n67.1% on HumanEval+ and 87.2% on HumanEval with GPT-3.5, marking improvements\nof 8.6% and 4.3% over the state-of-the-art, while also halving the iterations\nneeded to find the correct solution. Furthermore, our method scales more\nefficiently than existing search techniques, including tree search, line\nsearch, and repeated sampling.", "authors": ["Jonathan Light", "Yue Wu", "Yiyou Sun", "Wenchao Yu", "Yanchi liu", "Xujiang Zhao", "Ziniu Hu", "Haifeng Chen", "Wei Cheng"], "categories": ["cs.SE", "cs.AI", "cs.LG"], "published": "2024-10-22T01:58:29+00:00", "updated": "2024-10-22T01:58:29+00:00", "pdf_url": "http://arxiv.org/pdf/2411.05010v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2410.16229v2", "title": "Building A Coding Assistant via the Retrieval-Augmented Language Model", "abstract": "Pretrained language models have shown strong effectiveness in code-related\ntasks, such as code retrieval, code generation, code summarization, and code\ncompletion tasks. In this paper, we propose COde assistaNt viA\nretrieval-augmeNted language model (CONAN), which aims to build a code\nassistant by mimicking the knowledge-seeking behaviors of humans during coding.\nSpecifically, it consists of a code structure aware retriever (CONAN-R) and a\ndual-view code representation-based retrieval-augmented generation model\n(CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and\nMasked Entity Prediction tasks to make language models code structure-aware and\nlearn effective representations for code snippets and documentation. Then\nCONAN-G designs a dual-view code representation mechanism for implementing a\nretrieval-augmented code generation model. CONAN-G regards the code\ndocumentation descriptions as prompts, which help language models better\nunderstand the code semantics. Our experiments show that CONAN achieves\nconvincing performance on different code generation tasks and significantly\noutperforms previous retrieval augmented code generation models. Our further\nanalyses show that CONAN learns tailored representations for both code snippets\nand documentation by aligning code-documentation data pairs and capturing\nstructural semantics by masking and predicting entities in the code data.\nAdditionally, the retrieved code snippets and documentation provide necessary\ninformation from both program language and natural language to assist the code\ngeneration process. CONAN can also be used as an assistant for Large Language\nModels (LLMs), providing LLMs with external knowledge in shorter code document\nlengths to improve their effectiveness on various code tasks. It shows the\nability of CONAN to extract necessary information and help filter out the noise\nfrom retrieved code documents.", "authors": ["Xinze Li", "Hanbin Wang", "Zhenghao Liu", "Shi Yu", "Shuo Wang", "Yukun Yan", "Yukai Fu", "Yu Gu", "Ge Yu"], "categories": ["cs.CL"], "published": "2024-10-21T17:34:39+00:00", "updated": "2024-11-02T09:06:26+00:00", "pdf_url": "http://arxiv.org/pdf/2410.16229v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2410.11300v1", "title": "Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks", "abstract": "Recent studies proposed to leverage large language models (LLMs) with\nIn-Context Learning (ICL) to handle code intelligence tasks without\nfine-tuning. ICL employs task instructions and a set of examples as\ndemonstrations to guide the model in generating accurate answers without\nupdating its parameters. While ICL has proven effective for code intelligence\ntasks, its performance heavily relies on the selected examples. Previous work\nhas achieved some success in using BM25 to retrieve examples for code\nintelligence tasks. However, existing approaches lack the ability to understand\nthe semantic and structural information of queries, resulting in less helpful\ndemonstrations. Moreover, they do not adapt well to the complex and dynamic\nnature of user queries in diverse domains. In this paper, we introduce a novel\napproach named Instructive Code Retriever (ICR), which is designed to retrieve\nexamples that enhance model inference across various code intelligence tasks\nand datasets. We enable ICR to learn the semantic and structural information of\nthe corpus by a tree-based loss function. To better understand the correlation\nbetween queries and examples, we incorporate the feedback from LLMs to guide\nthe training of the retriever. Experimental results demonstrate that our\nretriever significantly outperforms state-of-the-art approaches. We evaluate\nour model's effectiveness on various tasks, i.e., code summarization, program\nsynthesis, and bug fixing. Compared to previous state-of-the-art algorithms,\nour method achieved improvements of 50.0% and 90.0% in terms of BLEU-4 for two\ncode summarization datasets, 74.6% CodeBLEU on program synthesis dataset, and\nincreases of 3.6 and 3.2 BLEU-4 on two bug fixing datasets.", "authors": ["Jiawei Lu", "Haoye Wang", "Zhongxin Liu", "Keyu Liang", "Lingfeng Bao", "Xiaohu Yang"], "categories": ["cs.SE"], "published": "2024-10-15T05:44:00+00:00", "updated": "2024-10-15T05:44:00+00:00", "pdf_url": "http://arxiv.org/pdf/2410.11300v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2410.09662v1", "title": "Exploring Demonstration Retrievers in RAG for Coding Tasks: Yeas and Nays!", "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external knowledge bases, achieving state-of-the-art results in\nvarious coding tasks. The core of RAG is retrieving demonstration examples,\nwhich is essential to balance effectiveness (generation quality) and efficiency\n(retrieval time) for optimal performance. However, the high-dimensional nature\nof code representations and large knowledge bases often create efficiency\nbottlenecks, which are overlooked in previous research. This paper\nsystematically evaluates the efficiency-effectiveness trade-off of retrievers\nacross three coding tasks: Program Synthesis, Commit Message Generation, and\nAssertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)\nand four dense retrievers, including one exhaustive dense retriever (SBERT's\nSemantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).\nOur findings show that while BM25 excels in effectiveness, it suffers in\nefficiency as the knowledge base grows beyond 1000 entries. In large-scale\nretrieval, efficiency differences become more pronounced, with approximate\ndense retrievers offering the greatest gains. For instance, in Commit\nGeneration task, HNSW achieves a 44x speed up, while only with a 1.74% drop in\nRougeL compared with BM25. Our results also show that increasing the number of\ndemonstrations in the prompt doesn't always improve the effectiveness and can\nincrease latency and lead to incorrect outputs. Our findings provide valuable\ninsights for practitioners aiming to build efficient and effective RAG systems\nfor coding tasks.", "authors": ["Pengfei He", "Shaowei Wang", "Shaiful Chowdhury", "Tse-Hsun Chen"], "categories": ["cs.SE"], "published": "2024-10-12T22:31:01+00:00", "updated": "2024-10-12T22:31:01+00:00", "pdf_url": "http://arxiv.org/pdf/2410.09662v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2410.03431v2", "title": "Approaching Code Search for Python as a Translation Retrieval Problem with Dual Encoders", "abstract": "Code search is vital in the maintenance and extension of software systems.\nPast works have used separate language models for the natural language and\nprogramming language artifacts on models with multiple encoders and different\nloss functions. Similarly, this work approaches code search for Python as a\ntranslation retrieval problem while the natural language queries and the\nprogramming language are treated as two types of languages. By using dual\nencoders, these two types of language sequences are projected onto a shared\nembedding space, in which the distance reflects the similarity between a given\npair of query and code. However, in contrast to previous work, this approach\nuses a unified language model, and a dual encoder structure with a cosine\nsimilarity loss function. A unified language model helps the model take\nadvantage of the considerable overlap of words between the artifacts, making\nthe learning much easier. On the other hand, the dual encoders trained with\ncosine similarity loss helps the model learn the underlining patterns of which\nterms are important for predicting linked pairs of artifacts. Evaluation shows\nthe proposed model achieves performance better than state-of-the-art code\nsearch models. In addition, this model is much less expensive in terms of time\nand complexity, offering a cheaper, faster, and better alternative.", "authors": ["Monoshiz Mahbub Khan", "Zhe Yu"], "categories": ["cs.SE"], "published": "2024-10-04T13:42:54+00:00", "updated": "2024-10-24T13:11:05+00:00", "pdf_url": "http://arxiv.org/pdf/2410.03431v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2409.19668v1", "title": "Local Search for Integer Quadratic Programming", "abstract": "Integer Quadratic Programming (IQP) is an important problem in operations\nresearch. Local search is a powerful method for solving hard problems, but the\nresearch on local search algorithms for IQP solving is still on its early\nstage. This paper develops an efficient local search solver for solving general\nIQP, called LS-IQCQP. We propose four new local search operators for IQP that\ncan handle quadratic terms in the objective function, constraints or both.\nFurthermore, a two-mode local search algorithm is introduced, utilizing newly\ndesigned scoring functions to enhance the search process. Experiments are\nconducted on standard IQP benchmarks QPLIB and MINLPLIB, comparing LS-IQCQP\nwith several state-of-the-art IQP solvers. Experimental results demonstrate\nthat LS-IQCQP is competitive with the most powerful commercial solver Gurobi\nand outperforms other state-of-the-art solvers. Moreover, LS-IQCQP has\nestablished 6 new records for QPLIB and MINLPLIB open instances.", "authors": ["Xiang He", "Peng Lin", "Shaowei Cai"], "categories": ["cs.AI"], "published": "2024-09-29T11:45:44+00:00", "updated": "2024-09-29T11:45:44+00:00", "pdf_url": "http://arxiv.org/pdf/2409.19668v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2409.15895v1", "title": "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation", "abstract": "Retrieval-augmented code generation utilizes Large Language Models as the\ngenerator and significantly expands their code generation capabilities by\nproviding relevant code, documentation, and more via the retriever. The current\napproach suffers from two primary limitations: 1) information redundancy. The\nindiscriminate inclusion of redundant information can result in resource\nwastage and may misguide generators, affecting their effectiveness and\nefficiency. 2) preference gap. Due to different optimization objectives, the\nretriever strives to procure code with higher ground truth similarity, yet this\neffort does not substantially benefit the generator. The retriever and the\ngenerator may prefer different golden code, and this gap in preference results\nin a suboptimal design. Additionally, differences in parameterization knowledge\nacquired during pre-training result in varying preferences among different\ngenerators.\n  To address these limitations, in this paper, we propose RRG (Retrieve,\nRefactor, Generate), a novel framework for effective and efficient code\ngeneration. This framework introduces a code refactorer module between the\nretriever and the generator to bridge them. The refactoring process transforms\nthe raw retrieved code into a more concise, efficient, and model-friendly\nversion. It eliminates redundant information and noise, reducing the input\nlength. Consequently, the generator receives higher-quality context, enabling\nit to produce more accurate results with lower inference costs. We conducted\ncomprehensive experiments on multiple datasets. In the experiments, we\nconfirmed the existence of a preference gap between the retriever and the\ngenerator, and RRG effectively bridges this gap. Specifically, RRG achieved\nsignificant performance improvements, with increases of up to 28% on EM, 13% on\nBLEU, and 6.8% on CodeBLEU.", "authors": ["Xinyu Gao", "Yun Xiong", "Deze Wang", "Zhenhan Guan", "Zejian Shi", "Haofen Wang", "Shanshan Li"], "categories": ["cs.SE"], "published": "2024-09-24T09:15:37+00:00", "updated": "2024-09-24T09:15:37+00:00", "pdf_url": "http://arxiv.org/pdf/2409.15895v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2409.13122v2", "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal Reinforcement and Retrieval-Augmented Generation", "abstract": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.", "authors": ["Jicheng Wang", "Yifeng He", "Hao Chen"], "categories": ["cs.SE"], "published": "2024-09-19T23:38:59+00:00", "updated": "2024-09-23T19:53:37+00:00", "pdf_url": "http://arxiv.org/pdf/2409.13122v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2409.09584v1", "title": "RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation", "abstract": "LLM agents enhanced by tree search algorithms have yielded notable\nperformances in code generation. However, current search algorithms in this\ndomain suffer from low search quality due to several reasons: 1) Ineffective\ndesign of the search space for the high-reasoning demands of code generation\ntasks, 2) Inadequate integration of code feedback with the search algorithm,\nand 3) Poor handling of negative feedback during the search, leading to reduced\nsearch efficiency and quality. To address these challenges, we propose to\nsearch for the reasoning process of the code and use the detailed feedback of\ncode execution to refine erroneous thoughts during the search. In this paper,\nwe introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS)\nalgorithm to conduct thought-level searches before generating code, thereby\nexploring a wider range of strategies. More importantly, we construct verbal\nfeedback from fine-grained code execution feedback to refine erroneous thoughts\nduring the search. This ensures that the search progresses along the correct\nreasoning paths, thus improving the overall search quality of the tree by\nleveraging execution feedback. Through extensive experiments, we demonstrate\nthat RethinkMCTS outperforms previous search-based and feedback-based code\ngeneration baselines. On the HumanEval dataset, it improves the pass@1 of\nGPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It\neffectively conducts more thorough exploration through thought-level searches\nand enhances the search quality of the entire tree by incorporating rethink\noperation.", "authors": ["Qingyao Li", "Wei Xia", "Kounianhua Du", "Xinyi Dai", "Ruiming Tang", "Yasheng Wang", "Yong Yu", "Weinan Zhang"], "categories": ["cs.SE", "cs.CL"], "published": "2024-09-15T02:07:28+00:00", "updated": "2024-09-15T02:07:28+00:00", "pdf_url": "http://arxiv.org/pdf/2409.09584v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2409.03733v2", "title": "Planning In Natural Language Improves LLM Search For Code Generation", "abstract": "While scaling training compute has led to remarkable improvements in large\nlanguage models (LLMs), scaling inference compute has not yet yielded analogous\ngains. We hypothesize that a core missing component is a lack of diverse LLM\noutputs, leading to inefficient search due to models repeatedly sampling highly\nsimilar, yet incorrect generations. We empirically demonstrate that this lack\nof diversity can be mitigated by searching over candidate plans for solving a\nproblem in natural language. Based on this insight, we propose PlanSearch, a\nnovel search algorithm which shows strong results across HumanEval+, MBPP+, and\nLiveCodeBench (a contamination-free benchmark for competitive coding).\nPlanSearch generates a diverse set of observations about the problem and then\nuses these observations to construct plans for solving the problem. By\nsearching over plans in natural language rather than directly over code\nsolutions, PlanSearch explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PlanSearch on top of\nClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on\nLiveCodeBench, outperforming both the best score achieved without search\n(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks\nanalyzed, we can accurately predict performance gains due to search as a direct\nfunction of the diversity over generated ideas. Code can be found at\nhttps://github.com/scaleapi/plansearch.", "authors": ["Evan Wang", "Federico Cassano", "Catherine Wu", "Yunfeng Bai", "Will Song", "Vaskar Nath", "Ziwen Han", "Sean Hendryx", "Summer Yue", "Hugh Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2024-09-05T17:44:49+00:00", "updated": "2024-10-18T23:53:07+00:00", "pdf_url": "http://arxiv.org/pdf/2409.03733v2", "primary_category": "cs.LG"}
{"id": "http://arxiv.org/abs/2409.03267v1", "title": "No Man is an Island: Towards Fully Automatic Programming by Code Search, Code Generation and Program Repair", "abstract": "Automatic programming attempts to minimize human intervention in the\ngeneration of executable code, and has been a long-standing challenge in the\nsoftware engineering community. To advance automatic programming, researchers\nare focusing on three primary directions: (1) code search that reuses existing\ncode snippets from external databases; (2) code generation that produces new\ncode snippets from natural language; and (3) program repair that refines\nexisting code snippets by fixing detected bugs. Despite significant\nadvancements, the effectiveness of state-of-the-art techniques is still\nlimited, such as the usability of searched code and the correctness of\ngenerated code.\n  Motivated by the real-world programming process, where developers usually use\nvarious external tools to aid their coding processes, such as code search\nengines and code testing tools, in this work, we propose \\toolname{}, an\nautomatic programming framework that leverages recent large language models\n(LLMs) to integrate the three research areas to address their inherent\nlimitations. In particular, our framework first leverages different code search\nstrategies to retrieve similar code snippets, which are then used to further\nguide the code generation process of LLMs. Our framework further validates the\nquality of generated code by compilers and test cases, and constructs repair\nprompts to query LLMs for generating correct patches. We conduct preliminary\nexperiments to demonstrate the potential of our framework, \\eg helping\nCodeLlama solve 267 programming problems with an improvement of 62.53\\%. As a\ngeneric framework, \\toolname{} can integrate various code search, generation,\nand repair tools, combining these three research areas together for the first\ntime. More importantly, it demonstrates the potential of using traditional SE\ntools to enhance the usability of LLMs in automatic programming.", "authors": ["Quanjun Zhang", "Chunrong Fang", "Ye Shang", "Tongke Zhang", "Shengcheng Yu", "Zhenyu Chen"], "categories": ["cs.SE"], "published": "2024-09-05T06:24:29+00:00", "updated": "2024-09-05T06:24:29+00:00", "pdf_url": "http://arxiv.org/pdf/2409.03267v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2408.16198v1", "title": "Chain-of-Experts (CoE): Reverse Engineering Software Bills of Materials for JavaScript Application Bundles through Code Clone Search", "abstract": "A Software Bill of Materials (SBoM) is a detailed inventory of all\ncomponents, libraries, and modules in a software artifact, providing\ntraceability throughout the software supply chain. With the increasing\npopularity of JavaScript in software engineering due to its dynamic syntax and\nseamless supply chain integration, the exposure to vulnerabilities and attacks\nhas risen significantly. A JavaScript application bundle, which is a\nconsolidated, symbol-stripped, and optimized assembly of code for deployment\npurpose. Generating a SBoM from a JavaScript application bundle through a\nreverse-engineering process ensures the integrity, security, and compliance of\nthe supplier's software release, even without access to the original dependency\ngraphs.\n  This paper presents the first study on SBoM generation for JavaScript\napplication bundles. We identify three key challenges for this task, i.e.,\nnested code scopes, extremely long sequences, and large retrieval spaces. To\naddress these challenges, we introduce Chain-of-Experts (CoE), a multi-task\ndeep learning model designed to generate SBoMs through three tasks: code\nsegmentation, code classification, and code clone retrieval. We evaluate CoE\nagainst individual task-specific solutions on 500 web application bundles with\nover 66,000 dependencies. Our experimental results demonstrate that CoE offers\ncompetitive outcomes with less training and inference time when compared with\ncombined individual task-specific solutions. Consequently, CoE provides the\nfirst scalable, efficient, and end-to-end solution for the SBoM generation of\nreal-world JavaScript application bundles.", "authors": ["Leo Song", "Steven H. H. Ding", "Yuan Tian", "Li Tao Li", "Philippe Charland", "Andrew Walenstein"], "categories": ["cs.SE"], "published": "2024-08-29T01:32:49+00:00", "updated": "2024-08-29T01:32:49+00:00", "pdf_url": "http://arxiv.org/pdf/2408.16198v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2408.12159v1", "title": "Search-Based LLMs for Code Optimization", "abstract": "The code written by developers usually suffers from efficiency problems and\ncontain various performance bugs. These inefficiencies necessitate the research\nof automated refactoring methods for code optimization. Early research in code\noptimization employs rule-based methods and focuses on specific inefficiency\nissues, which are labor-intensive and suffer from the low coverage issue.\nRecent work regards the task as a sequence generation problem, and resorts to\ndeep learning (DL) techniques such as large language models (LLMs). These\nmethods typically prompt LLMs to directly generate optimized code. Although\nthese methods show state-of-the-art performance, such one-step generation\nparadigm is hard to achieve an optimal solution. First, complex optimization\nmethods such as combinatorial ones are hard to be captured by LLMs. Second, the\none-step generation paradigm poses challenge in precisely infusing the\nknowledge required for effective code optimization within LLMs, resulting in\nunder-optimized code.To address these problems, we propose to model this task\nfrom the search perspective, and propose a search-based LLMs framework named\nSBLLM that enables iterative refinement and discovery of improved optimization\nmethods. SBLLM synergistically integrate LLMs with evolutionary search and\nconsists of three key components: 1) an execution-based representative sample\nselection part that evaluates the fitness of each existing optimized code and\nprioritizes promising ones to pilot the generation of improved code; 2) an\nadaptive optimization pattern retrieval part that infuses targeted optimization\npatterns into the model for guiding LLMs towards rectifying and progressively\nenhancing their optimization methods; and 3) a genetic operator-inspired\nchain-of-thought prompting part that aids LLMs in combining different\noptimization methods and generating improved optimization methods.", "authors": ["Shuzheng Gao", "Cuiyun Gao", "Wenchao Gu", "Michael Lyu"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2024-08-22T06:59:46+00:00", "updated": "2024-08-22T06:59:46+00:00", "pdf_url": "http://arxiv.org/pdf/2408.12159v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2408.11198v1", "title": "EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation", "abstract": "Large Language Models (LLMs) have seen increasing use in various software\ndevelopment tasks, especially in code generation. The most advanced recent\nmethods attempt to incorporate feedback from code execution into prompts to\nhelp guide LLMs in generating correct code, in an iterative process. While\neffective, these methods could be costly and time-consuming due to numerous\ninteractions with the LLM and the extensive token usage. To address this issue,\nwe propose an alternative approach named Evolutionary Prompt Engineering for\nCode (EPiC), which leverages a lightweight evolutionary algorithm to evolve the\noriginal prompts toward better ones that produce high-quality code, with\nminimal interactions with LLM. Our evaluation against state-of-the-art (SOTA)\nLLM-based code generation models shows that EPiC outperforms all the baselines\nin terms of cost-effectiveness.", "authors": ["Hamed Taherkhani", "Melika Sepindband", "Hung Viet Pham", "Song Wang", "Hadi Hemmati"], "categories": ["cs.SE", "cs.AI", "cs.NE"], "published": "2024-08-20T21:15:36+00:00", "updated": "2024-08-20T21:15:36+00:00", "pdf_url": "http://arxiv.org/pdf/2408.11198v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2408.09345v1", "title": "Deep Code Search with Naming-Agnostic Contrastive Multi-View Learning", "abstract": "Software development is a repetitive task, as developers usually reuse or get\ninspiration from existing implementations. Code search, which refers to the\nretrieval of relevant code snippets from a codebase according to the\ndeveloper's intent that has been expressed as a query, has become increasingly\nimportant in the software development process. Due to the success of deep\nlearning in various applications, a great number of deep learning based code\nsearch approaches have sprung up and achieved promising results. However,\ndevelopers may not follow the same naming conventions and the same variable may\nhave different variable names in different implementations, bringing a\nchallenge to deep learning based code search methods that rely on explicit\nvariable correspondences to understand source code. To overcome this challenge,\nwe propose a naming-agnostic code search method (NACS) based on contrastive\nmulti-view code representation learning. NACS strips information bound to\nvariable names from Abstract Syntax Tree (AST), the representation of the\nabstract syntactic structure of source code, and focuses on capturing intrinsic\nproperties solely from AST structures. We use semantic-level and syntax-level\naugmentation techniques to prepare realistically rational data and adopt\ncontrastive learning to design a graph-view modeling component in NACS to\nenhance the understanding of code snippets. We further model ASTs in a path\nview to strengthen the graph-view modeling component through multi-view\nlearning. Extensive experiments show that NACS provides superior code search\nperformance compared to baselines and NACS can be adapted to help existing code\nsearch methods overcome the impact of different naming conventions.", "authors": ["Jiadong Feng", "Wei Li", "Zhao Wei", "Yong Xu", "Juhong Wang", "Hui Li"], "categories": ["cs.IR", "cs.SE"], "published": "2024-08-18T03:47:34+00:00", "updated": "2024-08-18T03:47:34+00:00", "pdf_url": "http://arxiv.org/pdf/2408.09345v1", "primary_category": "cs.IR"}
{"id": "http://arxiv.org/abs/2408.06385v1", "title": "ViC: Virtual Compiler Is All You Need For Assembly Code Search", "abstract": "Assembly code search is vital for reducing the burden on reverse engineers,\nallowing them to quickly identify specific functions using natural language\nwithin vast binary programs. Despite its significance, this critical task is\nimpeded by the complexities involved in building high-quality datasets. This\npaper explores training a Large Language Model (LLM) to emulate a general\ncompiler. By leveraging Ubuntu packages to compile a dataset of 20 billion\ntokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC),\ncapable of compiling any source code of any language to assembly code. This\napproach allows for virtual compilation across a wide range of programming\nlanguages without the need for a real compiler, preserving semantic equivalency\nand expanding the possibilities for assembly code dataset construction.\nFurthermore, we use ViC to construct a sufficiently large dataset for assembly\ncode search. Employing this extensive dataset, we achieve a substantial\nimprovement in assembly code search performance, with our model surpassing the\nleading baseline by 26%.", "authors": ["Zeyu Gao", "Hao Wang", "Yuanda Wang", "Chao Zhang"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2024-08-10T17:23:02+00:00", "updated": "2024-08-10T17:23:02+00:00", "pdf_url": "http://arxiv.org/pdf/2408.06385v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2408.05542v2", "title": "You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search", "abstract": "Code search plays a crucial role in software development, enabling developers\nto retrieve and reuse code using natural language queries. While the\nperformance of code search models improves with an increase in high-quality\ndata, obtaining such data can be challenging and expensive. Recently, large\nlanguage models (LLMs) such as ChatGPT have made remarkable progress in both\nnatural and programming language understanding and generation, offering\nuser-friendly interaction via simple prompts. Inspired by these advancements,\nwe propose a novel approach ChatDANCE, which utilizes high-quality and diverse\naugmented data generated by a large language model and leverages a filtering\nmechanism to eliminate low-quality augmentations. Specifically, we first\npropose a set of ChatGPT prompting rules that are specifically designed for\nsource code and queries. Then, we leverage ChatGPT to rewrite code and queries\nbased on the according prompts and then propose a filtering mechanism which\ntrains a cross-encoder from the backbone model UniXcoder to filter out code and\nquery pairs with low matching scores. Finally, we re-train the backbone model\nusing the obtained high-quality augmented data. Experimental results show that\nChatDANCE achieves state-of-the-art performance, improving the best baseline by\n13.2% (R@1) and 7% (MRR). Surprisingly, we find that this\naugment-filter-retrain strategy enables the backbone model (UniXcoder) to\nself-grow. Moreover, extensive experiments show the effectiveness of each\ncomponent and ChatDANCE has stable performance under different hyperparameter\nsettings. In addition, we conduct qualitative and quantitative analyses to\ninvestigate why ChatDANCE works well and find that it learns a more uniform\ndistribution of representations and effectively aligns the code and query\nspaces.", "authors": ["Yanlin Wang", "Lianghong Guo", "Ensheng Shi", "Wenqing Chen", "Jiachi Chen", "Wanjun Zhong", "Menghan Wang", "Hui Li", "Hongyu Zhang", "Ziyu Lyu", "Zibin Zheng"], "categories": ["cs.SE"], "published": "2024-08-10T12:51:21+00:00", "updated": "2024-08-17T11:18:16+00:00", "pdf_url": "http://arxiv.org/pdf/2408.05542v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2408.05344v1", "title": "AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations", "abstract": "In this work, we discuss a recently popular type of recommender system: an\nLLM-based coding assistant. Connecting the task of providing code\nrecommendations in multiple formats to traditional RecSys challenges, we\noutline several similarities and differences due to domain specifics. We\nemphasize the importance of providing relevant context to an LLM for this use\ncase and discuss lessons learned from context enhancements & offline and online\nevaluation of such AI-assisted coding systems.", "authors": ["Jan Hartman", "Rishabh Mehrotra", "Hitesh Sagtani", "Dominic Cooney", "Rafal Gajdulewicz", "Beyang Liu", "Julie Tibshirani", "Quinn Slack"], "categories": ["cs.IR", "cs.LG", "cs.SE"], "published": "2024-08-09T21:21:15+00:00", "updated": "2024-08-09T21:21:15+00:00", "pdf_url": "http://arxiv.org/pdf/2408.05344v1", "primary_category": "cs.IR"}
{"id": "http://arxiv.org/abs/2408.05026v1", "title": "Retrieval-augmented code completion for local projects using large language models", "abstract": "The use of large language models (LLMs) is becoming increasingly widespread\namong software developers. However, privacy and computational requirements are\nproblematic with commercial solutions and the use of LLMs. In this work, we\nfocus on using LLMs with around 160 million parameters that are suitable for\nlocal execution and augmentation with retrieval from local projects. We train\ntwo models based on the transformer architecture, the generative model GPT-2\nand the retrieval-adapted RETRO model, on open-source Python files, and\nempirically evaluate and compare them, confirming the benefits of vector\nembedding based retrieval. Further, we improve our models' performance with\nIn-context retrieval-augmented generation, which retrieves code snippets based\non the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented\ngeneration on larger models and conclude that, despite its simplicity, the\napproach is more suitable than using the RETRO architecture. We highlight the\nkey role of proper tokenization in achieving the full potential of LLMs in code\ncompletion.", "authors": ["Marko Hostnik", "Marko Robnik-\u0160ikonja"], "categories": ["cs.SE", "cs.LG", "68T07, 68T50"], "published": "2024-08-09T12:26:57+00:00", "updated": "2024-08-09T12:26:57+00:00", "pdf_url": "http://arxiv.org/pdf/2408.05026v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2408.03623v1", "title": "Improving Retrieval-Augmented Code Comment Generation by Retrieving for Generation", "abstract": "Code comment generation aims to generate high-quality comments from source\ncode automatically and has been studied for years. Recent studies proposed to\nintegrate information retrieval techniques with neural generation models to\ntackle this problem, i.e., Retrieval-Augmented Comment Generation (RACG)\napproaches, and achieved state-of-the-art results. However, the retrievers in\nprevious work are built independently of their generators. This results in that\nthe retrieved exemplars are not necessarily the most useful ones for generating\ncomments, limiting the performance of existing approaches. To address this\nlimitation, we propose a novel training strategy to enable the retriever to\nlearn from the feedback of the generator and retrieve exemplars for generation.\nSpecifically, during training, we use the retriever to retrieve the top-k\nexemplars and calculate their retrieval scores, and use the generator to\ncalculate a generation loss for the sample based on each exemplar. By aligning\nhigh-score exemplars retrieved by the retriever with low-loss exemplars\nobserved by the generator, the retriever can learn to retrieve exemplars that\ncan best improve the quality of the generated comments. Based on this strategy,\nwe propose a novel RACG approach named JOINTCOM and evaluate it on two\nreal-world datasets, JCSD and PCSD. The experimental results demonstrate that\nour approach surpasses the state-of-the-art baselines by 7.3% to 30.0% in terms\nof five metrics on the two datasets. We also conduct a human evaluation to\ncompare JOINTCOM with the best-performing baselines. The results indicate that\nJOINTCOM outperforms the baselines, producing comments that are more natural,\ninformative, and useful.", "authors": ["Hanzhen Lu", "Zhongxin Liu"], "categories": ["cs.SE"], "published": "2024-08-07T08:32:55+00:00", "updated": "2024-08-07T08:32:55+00:00", "pdf_url": "http://arxiv.org/pdf/2408.03623v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2408.11058v1", "title": "LLM Agents Improve Semantic Code Search", "abstract": "Code Search is a key task that many programmers often have to perform while\ndeveloping solutions to problems. Current methodologies suffer from an\ninability to perform accurately on prompts that contain some ambiguity or ones\nthat require additional context relative to a code-base. We introduce the\napproach of using Retrieval Augmented Generation (RAG) powered agents to inject\ninformation into user prompts allowing for better inputs into embedding models.\nBy utilizing RAG, agents enhance user queries with relevant details from GitHub\nrepositories, making them more informative and contextually aligned.\nAdditionally, we introduce a multi-stream ensemble approach which when paired\nwith agentic workflow can obtain improved retrieval accuracy, which we deploy\non application called repo-rift.com. Experimental results on the CodeSearchNet\ndataset demonstrate that RepoRift significantly outperforms existing methods,\nachieving an 78.2% success rate at Success@10 and a 34.6% success rate at\nSuccess@1. This research presents a substantial advancement in semantic code\nsearch, highlighting the potential of agentic LLMs and RAG to enhance code\nretrieval systems.", "authors": ["Sarthak Jain", "Aditya Dora", "Ka Seng Sam", "Prabhat Singh"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "published": "2024-08-05T00:43:56+00:00", "updated": "2024-08-05T00:43:56+00:00", "pdf_url": "http://arxiv.org/pdf/2408.11058v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2407.19619v1", "title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation", "abstract": "The advent of large language models (LLMs) has significantly advanced the\nfield of code translation, enabling automated translation between programming\nlanguages. However, these models often struggle with complex translation tasks\ndue to inadequate contextual understanding. This paper introduces a novel\napproach that enhances code translation through Few-Shot Learning, augmented\nwith retrieval-based techniques. By leveraging a repository of existing code\ntranslations, we dynamically retrieve the most relevant examples to guide the\nmodel in translating new code segments. Our method, based on\nRetrieval-Augmented Generation (RAG), substantially improves translation\nquality by providing contextual examples from which the model can learn in\nreal-time. We selected RAG over traditional fine-tuning methods due to its\nability to utilize existing codebases or a locally stored corpus of code, which\nallows for dynamic adaptation to diverse translation tasks without extensive\nretraining. Extensive experiments on diverse datasets with open LLM models such\nas Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code\nInstruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5\nTurbo and GPT-4o, demonstrate our approach's superiority over traditional\nzero-shot methods, especially in translating between Fortran and CPP. We also\nexplored varying numbers of shots i.e. examples provided during inference,\nspecifically 1, 2, and 3 shots and different embedding models for RAG,\nincluding Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and\neffectiveness of our approach.", "authors": ["Manish Bhattarai", "Javier E. Santos", "Shawn Jones", "Ayan Biswas", "Boian Alexandrov", "Daniel O'Malley"], "categories": ["cs.AI", "cs.SE"], "published": "2024-07-29T00:41:48+00:00", "updated": "2024-07-29T00:41:48+00:00", "pdf_url": "http://arxiv.org/pdf/2407.19619v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2407.21049v1", "title": "Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval", "abstract": "As language models support larger and larger context sizes, evaluating their\nability to make effective use of that context becomes increasingly important.\nWe analyze the ability of several code generation models to handle long range\ndependencies using a suite of multi-step key retrieval tasks in context windows\nup to 8k tokens in length. The tasks progressively increase in difficulty and\nallow more nuanced evaluation of model capabilities than tests like the popular\nneedle-in-the-haystack test. We find that performance degrades significantly\n(up to 2x) when a function references another function that is defined later in\nthe prompt. We also observe that models that use sliding window attention\nmechanisms have difficulty handling references further than the size of a\nsingle window. We perform simple prompt modifications using call graph\ninformation to improve multi-step retrieval performance up to 3x. Our analysis\nhighlights different facets of long-context performance and is suggestive of\nprompt construction strategies for code completion tools", "authors": ["Yannick Assogba", "Donghao Ren"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2024-07-23T02:45:22+00:00", "updated": "2024-07-23T02:45:22+00:00", "pdf_url": "http://arxiv.org/pdf/2407.21049v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2407.02883v1", "title": "CoIR: A Comprehensive Benchmark for Code Information Retrieval Models", "abstract": "Despite the substantial success of Information Retrieval (IR) in various NLP\ntasks, most IR systems predominantly handle queries and corpora in natural\nlanguage, neglecting the domain of code retrieval. Code retrieval is critically\nimportant yet remains under-explored, with existing methods and benchmarks\ninadequately representing the diversity of code in various domains and tasks.\nAddressing this gap, we present \\textbf{\\name} (\\textbf{Co}de\n\\textbf{I}nformation \\textbf{R}etrieval Benchmark), a robust and comprehensive\nbenchmark specifically designed to assess code retrieval capabilities. \\name\ncomprises \\textbf{ten} meticulously curated code datasets, spanning\n\\textbf{eight} distinctive retrieval tasks across \\textbf{seven} diverse\ndomains. We first discuss the construction of \\name and its diverse dataset\ncomposition. Further, we evaluate nine widely used retrieval models using\n\\name, uncovering significant difficulties in performing code retrieval tasks\neven with state-of-the-art systems. To facilitate easy adoption and integration\nwithin existing research workflows, \\name has been developed as a user-friendly\nPython framework, readily installable via pip. It shares same data schema as\nother popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark\nevaluations. Through \\name, we aim to invigorate research in the code retrieval\ndomain, providing a versatile benchmarking tool that encourages further\ndevelopment and exploration of code retrieval systems\\footnote{\\url{\nhttps://github.com/CoIR-team/coir}}.", "authors": ["Xiangyang Li", "Kuicai Dong", "Yi Quan Lee", "Wei Xia", "Yichun Yin", "Hao Zhang", "Yong Liu", "Yasheng Wang", "Ruiming Tang"], "categories": ["cs.IR", "cs.CL"], "published": "2024-07-03T07:58:20+00:00", "updated": "2024-07-03T07:58:20+00:00", "pdf_url": "http://arxiv.org/pdf/2407.02883v1", "primary_category": "cs.IR"}
{"id": "http://arxiv.org/abs/2407.02742v1", "title": "A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation", "abstract": "Natural Language to Code Generation has made significant progress in recent\nyears with the advent of Large Language Models(LLMs). While generation for\ngeneral-purpose languages like C, C++, and Python has improved significantly,\nLLMs struggle with custom function names in Domain Specific Languages or DSLs.\nThis leads to higher hallucination rates and syntax errors, specially for DSLs\nhaving a high number of custom function names. Additionally, constant updates\nto function names add to the challenge as LLMs need to stay up-to-date. In this\npaper, we present optimizations for using Retrieval Augmented Generation (or\nRAG) with LLMs for DSL generation along with an ablation study comparing these\nstrategies. We generated a train as well as test dataset with a DSL to\nrepresent automation tasks across roughly 700 APIs in public domain. We used\nthe training dataset to fine-tune a Codex model for this DSL. Our results\nshowed that the fine-tuned model scored the best on code similarity metric.\nWith our RAG optimizations, we achieved parity for similarity metric. The\ncompilation rate, however, showed that both the models still got the syntax\nwrong many times, with RAG-based method being 2 pts better. Conversely,\nhallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for\nAPI parameter keys. We conclude that an optimized RAG model can match the\nquality of fine-tuned models and offer advantages for new, unseen APIs.", "authors": ["Nastaran Bassamzadeh", "Chhaya Methani"], "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.2; I.2.7"], "published": "2024-07-03T01:28:51+00:00", "updated": "2024-07-03T01:28:51+00:00", "pdf_url": "http://arxiv.org/pdf/2407.02742v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2406.17553v1", "title": "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft", "abstract": "In the Minecraft Collaborative Building Task, two players collaborate: an\nArchitect (A) provides instructions to a Builder (B) to assemble a specified\nstructure using 3D blocks. In this work, we investigate the use of large\nlanguage models (LLMs) to predict the sequence of actions taken by the Builder.\nLeveraging LLMs' in-context learning abilities, we use few-shot prompting\ntechniques, that significantly improve performance over baseline methods.\nAdditionally, we present a detailed analysis of the gaps in performance for\nfuture work", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "categories": ["cs.CL"], "published": "2024-06-25T13:43:24+00:00", "updated": "2024-06-25T13:43:24+00:00", "pdf_url": "http://arxiv.org/pdf/2406.17553v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2406.14497v1", "title": "CodeRAG-Bench: Can Retrieval Augment Code Generation?", "abstract": "While language models (LMs) have proven remarkably adept at generating code,\nmany programs are challenging for LMs to generate using their parametric\nknowledge alone. Providing external contexts such as library documentation can\nfacilitate generating accurate and functional code. Despite the success of\nretrieval-augmented generation (RAG) in various text-oriented tasks, its\npotential for improving code generation remains under-explored. In this work,\nwe conduct a systematic, large-scale analysis by asking: in what scenarios can\nretrieval benefit code generation models? and what challenges remain? We first\ncurate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three\ncategories of code generation tasks, including basic programming, open-domain,\nand repository-level problems. We aggregate documents from five sources for\nmodels to retrieve contexts: competition solutions, online tutorials, library\ndocumentation, StackOverflow posts, and GitHub repositories. We examine\ntop-performing models on CodeRAG-Bench by providing contexts retrieved from one\nor multiple sources. While notable gains are made in final code generation by\nretrieving high-quality contexts across various settings, our analysis reveals\nroom for improvement -- current retrievers still struggle to fetch useful\ncontexts especially with limited lexical overlap, and generators fail to\nimprove with limited context lengths or abilities to integrate additional\ncontexts. We hope CodeRAG-Bench serves as an effective testbed to encourage\nfurther development of advanced code-oriented RAG methods.", "authors": ["Zora Zhiruo Wang", "Akari Asai", "Xinyan Velocity Yu", "Frank F. Xu", "Yiqing Xie", "Graham Neubig", "Daniel Fried"], "categories": ["cs.SE", "cs.CL"], "published": "2024-06-20T16:59:52+00:00", "updated": "2024-06-20T16:59:52+00:00", "pdf_url": "http://arxiv.org/pdf/2406.14497v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2406.11589v2", "title": "CoSQA+: Enhancing Code Search Dataset with Matching Code", "abstract": "Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets are problematic: either using unrealistic\nqueries, or with mismatched codes, and typically using one-to-one query-code\npairing, which fails to reflect the reality that a query might have multiple\nvalid code matches. This paper introduces CoSQA+, pairing high-quality queries\n(reused from CoSQA) with multiple suitable codes. We collect code candidates\nfrom diverse sources and form candidate pairs by pairing queries with these\ncodes. Utilizing the power of large language models (LLMs), we automate pair\nannotation, filtering, and code generation for queries without suitable\nmatches. Through extensive experiments, CoSQA+ has demonstrated superior\nquality over CoSQA. Models trained on CoSQA+ exhibit improved performance.\nFurthermore, we propose a new metric Mean Multi-choice Reciprocal Rank (MMRR),\nto assess one-to-N code search performance. We provide the code and data at\nhttps://github.com/DeepSoftwareAnalytics/CoSQA_Plus.", "authors": ["Jing Gong", "Yanghui Wu", "Linxi Liang", "Zibin Zheng", "Yanlin Wang"], "categories": ["cs.SE", "cs.AI", "cs.IR", "I.2.7; D.2.3"], "published": "2024-06-17T14:34:14+00:00", "updated": "2024-08-23T19:55:52+00:00", "pdf_url": "http://arxiv.org/pdf/2406.11589v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2406.07003v2", "title": "GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model", "abstract": "The performance of repository-level code completion depends upon the\neffective leverage of both general and repository-specific knowledge. Despite\nthe impressive capability of code LLMs in general code completion tasks, they\noften exhibit less satisfactory performance on repository-level completion due\nto the lack of repository-specific knowledge in these LLMs. To address this\nproblem, we propose GraphCoder, a retrieval-augmented code completion framework\nthat leverages LLMs' general code knowledge and the repository-specific\nknowledge via a graph-based retrieval-generation process. In particular,\nGraphCoder captures the context of completion target more accurately through\ncode context graph (CCG) that consists of control-flow, data- and\ncontrol-dependence between code statements, a more structured way to capture\nthe completion target context than the sequence-based context used in existing\nretrieval-augmented approaches; based on CCG, GraphCoder further employs a\ncoarse-to-fine retrieval process to locate context-similar code snippets with\nthe completion target from the current repository. Experimental results\ndemonstrate both the effectiveness and efficiency of GraphCoder: Compared to\nbaseline retrieval-augmented methods, GraphCoder achieves higher exact match\n(EM) on average, with increases of +6.06 in code match and +6.23 in identifier\nmatch, while using less time and space.", "authors": ["Wei Liu", "Ailun Yu", "Daoguang Zan", "Bo Shen", "Wei Zhang", "Haiyan Zhao", "Zhi Jin", "Qianxiang Wang"], "categories": ["cs.SE"], "published": "2024-06-11T06:55:32+00:00", "updated": "2024-09-13T07:19:16+00:00", "pdf_url": "http://arxiv.org/pdf/2406.07003v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2406.10263v1", "title": "A Lightweight Framework for Adaptive Retrieval In Code Completion With Critique Model", "abstract": "Recent advancements in Retrieval-Augmented Generation have significantly\nenhanced code completion at the repository level. Various RAG-based code\ncompletion systems are proposed based on different design choices. For\ninstance, gaining more effectiveness at the cost of repeating the\nretrieval-generation process multiple times. However, the indiscriminate use of\nretrieval in current methods reveals issues in both efficiency and\neffectiveness, as a considerable portion of retrievals are unnecessary and may\nintroduce unhelpful or even harmful suggestions to code language models. To\naddress these challenges, we introduce CARD, a lightweight critique method\ndesigned to provide insights into the necessity of retrievals and select the\noptimal answer from multiple predictions. CARD can seamlessly integrate into\nany RAG-based code completion system. Our evaluation shows that CARD saves 21%\nto 46% times of retrieval for Line completion, 14% to 40% times of retrieval\nfor API completion, and 6% to 46.5% times of retrieval for function completion\nrespectively, while improving the accuracy. CARD reduces latency ranging from\n16% to 83%. CARD is generalizable to different LMs, retrievers, and programming\nlanguages. It is lightweight with training in few seconds and inference in few\nmilliseconds.", "authors": ["Wenrui Zhang", "Tiehang Fu", "Ting Yuan", "Ge Zhang", "Dong Chen", "Jie Wang"], "categories": ["cs.SE"], "published": "2024-06-11T02:37:06+00:00", "updated": "2024-06-11T02:37:06+00:00", "pdf_url": "http://arxiv.org/pdf/2406.10263v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2406.04464v1", "title": "On The Importance of Reasoning for Context Retrieval in Repository-Level Code Editing", "abstract": "Recent advancements in code-fluent Large Language Models (LLMs) enabled the\nresearch on repository-level code editing. In such tasks, the model navigates\nand modifies the entire codebase of a project according to request. Hence, such\ntasks require efficient context retrieval, i.e., navigating vast codebases to\ngather relevant context. Despite the recognized importance of context\nretrieval, existing studies tend to approach repository-level coding tasks in\nan end-to-end manner, rendering the impact of individual components within\nthese complicated systems unclear. In this work, we decouple the task of\ncontext retrieval from the other components of the repository-level code\nediting pipelines. We lay the groundwork to define the strengths and weaknesses\nof this component and the role that reasoning plays in it by conducting\nexperiments that focus solely on context retrieval. We conclude that while the\nreasoning helps to improve the precision of the gathered context, it still\nlacks the ability to identify its sufficiency. We also outline the ultimate\nrole of the specialized tools in the process of context gathering. The code\nsupplementing this paper is available at\nhttps://github.com/JetBrains-Research/ai-agents-code-editing.", "authors": ["Alexander Kovrigin", "Aleksandra Eliseeva", "Yaroslav Zharov", "Timofey Bryksin"], "categories": ["cs.SE", "cs.AI", "cs.LG"], "published": "2024-06-06T19:44:17+00:00", "updated": "2024-06-06T19:44:17+00:00", "pdf_url": "http://arxiv.org/pdf/2406.04464v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2405.19782v1", "title": "Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion", "abstract": "Recent years have witnessed the deployment of code language models (LMs) in\nvarious code intelligence tasks such as code completion. Yet, it is challenging\nfor pre-trained LMs to generate correct completions in private repositories.\nPrevious studies retrieve cross-file context based on import relations or text\nsimilarity, which is insufficiently relevant to completion targets. In this\npaper, we propose a dataflow-guided retrieval augmentation approach, called\nDraCo, for repository-level code completion. DraCo parses a private repository\ninto code entities and establishes their relations through an extended dataflow\nanalysis, forming a repo-specific context graph. Whenever triggering code\ncompletion, DraCo precisely retrieves relevant background knowledge from the\nrepo-specific context graph and generates well-formed prompts to query code\nLMs. Furthermore, we construct a large Python dataset, ReccEval, with more\ndiverse completion targets. Our experiments demonstrate the superior accuracy\nand applicable efficiency of DraCo, improving code exact match by 3.43% and\nidentifier F1-score by 3.27% on average compared to the state-of-the-art\napproach.", "authors": ["Wei Cheng", "Yuhan Wu", "Wei Hu"], "categories": ["cs.SE", "cs.CL"], "published": "2024-05-30T07:48:00+00:00", "updated": "2024-05-30T07:48:00+00:00", "pdf_url": "http://arxiv.org/pdf/2405.19782v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2405.19093v1", "title": "Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding Recommendation", "abstract": "The International Classification of Diseases (ICD) serves as a definitive\nmedical classification system encompassing a wide range of diseases and\nconditions. The primary objective of ICD indexing is to allocate a subset of\nICD codes to a medical record, which facilitates standardized documentation and\nmanagement of various health conditions. Most existing approaches have suffered\nfrom selecting the proper label subsets from an extremely large ICD collection\nwith a heavy long-tailed label distribution. In this paper, we leverage a\nmulti-stage ``retrieve and re-rank'' framework as a novel solution to ICD\nindexing, via a hybrid discrete retrieval method, and re-rank retrieved\ncandidates with contrastive learning that allows the model to make more\naccurate predictions from a simplified label space. The retrieval model is a\nhybrid of auxiliary knowledge of the electronic health records (EHR) and a\ndiscrete retrieval method (BM25), which efficiently collects high-quality\ncandidates. In the last stage, we propose a label co-occurrence guided\ncontrastive re-ranking model, which re-ranks the candidate labels by pulling\ntogether the clinical notes with positive ICD codes. Experimental results show\nthe proposed method achieves state-of-the-art performance on a number of\nmeasures on the MIMIC-III benchmark.", "authors": ["Xindi Wang", "Robert E. Mercer", "Frank Rudzicz"], "categories": ["cs.CL", "cs.IR"], "published": "2024-05-29T13:54:30+00:00", "updated": "2024-05-29T13:54:30+00:00", "pdf_url": "http://arxiv.org/pdf/2405.19093v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2405.16337v3", "title": "Learning to Reason via Program Generation, Emulation, and Search", "abstract": "Program synthesis with language models (LMs) has unlocked a large set of\nreasoning abilities; code-tuned LMs have proven adept at generating programs\nthat solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word\nconcatenation). However, not all reasoning tasks are easily expressible as\ncode, e.g. tasks involving commonsense reasoning, moral decision-making, and\nsarcasm understanding. Our goal is to extend an LM's program synthesis skills\nto such tasks and evaluate the results via pseudo-programs, namely Python\nprograms where some leaf function calls are left undefined. To that end, we\npropose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)\ntraining LMs to generate pseudo-programs, (2) teaching them to emulate their\ngenerated program's execution, including those leaf functions, allowing the\nLM's knowledge to fill in the execution gaps; and (3) using them to search over\nmany programs to find an optimal one. To adapt the CoGEX model to a new task,\nwe introduce a method for performing program search to find a single program\nwhose pseudo-execution yields optimal performance when applied to all the\ninstances of a given dataset. We show that our approach yields large\nimprovements compared to standard in-context learning approaches on a battery\nof tasks, both algorithmic and soft reasoning. This result thus demonstrates\nthat code synthesis can be applied to a much broader class of problems than\npreviously considered. Our released dataset, fine-tuned models, and\nimplementation can be found at \\url{https://github.com/nweir127/CoGEX}.", "authors": ["Nathaniel Weir", "Muhammad Khalifa", "Linlu Qiu", "Orion Weller", "Peter Clark"], "categories": ["cs.CL", "cs.AI"], "published": "2024-05-25T19:40:50+00:00", "updated": "2024-11-03T22:44:20+00:00", "pdf_url": "http://arxiv.org/pdf/2405.16337v3", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2405.15383v2", "title": "Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search", "abstract": "In this work we consider Code World Models, world models generated by a Large\nLanguage Model (LLM) in the form of Python code for model-based Reinforcement\nLearning (RL). Calling code instead of LLMs for planning has potential to be\nmore precise, reliable, interpretable, and extremely efficient. However,\nwriting appropriate Code World Models requires the ability to understand\ncomplex instructions, to generate exact code with non-trivial logic and to\nself-debug a long program with feedback from unit tests and environment\ntrajectories. To address these challenges, we propose Generate, Improve and Fix\nwith Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for\nLLMs. To test our approach in an offline RL setting, we introduce the Code\nWorld Models Benchmark (CWMB), a suite of program synthesis and planning tasks\ncomprised of 18 diverse RL environments paired with corresponding textual\ndescriptions and curated trajectories. GIF-MCTS surpasses all baselines on the\nCWMB and two other benchmarks, and we show that the Code World Models\nsynthesized with it can be successfully used for planning, resulting in\nmodel-based RL agents with greatly improved sample efficiency and inference\nspeed.", "authors": ["Nicola Dainese", "Matteo Merler", "Minttu Alakuijala", "Pekka Marttinen"], "categories": ["cs.AI"], "published": "2024-05-24T09:31:26+00:00", "updated": "2024-10-30T14:19:57+00:00", "pdf_url": "http://arxiv.org/pdf/2405.15383v2", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2405.11305v1", "title": "Large Neighborhood Prioritized Search for Combinatorial Optimization with Answer Set Programming", "abstract": "We propose Large Neighborhood Prioritized Search (LNPS) for solving\ncombinatorial optimization problems in Answer Set Programming (ASP). LNPS is a\nmetaheuristic that starts with an initial solution and then iteratively tries\nto find better solutions by alternately destroying and prioritized searching\nfor a current solution. Due to the variability of neighborhoods, LNPS allows\nfor flexible search without strongly depending on the destroy operators. We\npresent an implementation of LNPS based on ASP. The resulting heulingo solver\ndemonstrates that LNPS can significantly enhance the solving performance of ASP\nfor optimization. Furthermore, we establish the competitiveness of our LNPS\napproach by empirically contrasting it to (adaptive) large neighborhood search.", "authors": ["Irumi Sugimori", "Katsumi Inoue", "Hidetomo Nabeshima", "Torsten Schaub", "Takehide Soh", "Naoyuki Tamura", "Mutsunori Banbara"], "categories": ["cs.AI"], "published": "2024-05-18T14:37:43+00:00", "updated": "2024-05-18T14:37:43+00:00", "pdf_url": "http://arxiv.org/pdf/2405.11305v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2405.07530v1", "title": "Prompt-based Code Completion via Multi-Retrieval Augmented Generation", "abstract": "Automated code completion, aiming at generating subsequent tokens from\nunfinished code, has been significantly benefited from recent progress in\npre-trained Large Language Models (LLMs). However, these models often suffer\nfrom coherence issues and hallucinations when dealing with complex code logic\nor extrapolating beyond their training data. Existing Retrieval Augmented\nGeneration (RAG) techniques partially address these issues by retrieving\nrelevant code with a separate encoding model where the retrieved snippet serves\nas contextual reference for code completion. However, their retrieval scope is\nsubject to a singular perspective defined by the encoding model, which largely\noverlooks the complexity and diversity inherent in code semantics. To address\nthis limitation, we propose ProCC, a code completion framework leveraging\nprompt engineering and the contextual multi-armed bandits algorithm to flexibly\nincorporate and adapt to multiple perspectives of code. ProCC first employs a\nprompt-based multi-retriever system which crafts prompt templates to elicit LLM\nknowledge to understand code semantics with multiple retrieval perspectives.\nThen, it adopts the adaptive retrieval selection algorithm to incorporate code\nsimilarity into the decision-making process to determine the most suitable\nretrieval perspective for the LLM to complete the code. Experimental results\ndemonstrate that ProCC outperforms state-of-the-art code completion technique\nby 8.6% on our collected open-source benchmark suite and 10.1% on the\nprivate-domain benchmark suite collected from a billion-user e-commerce company\nin terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in\na plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned\nmodel.", "authors": ["Hanzhuo Tan", "Qi Luo", "Ling Jiang", "Zizheng Zhan", "Jing Li", "Haotian Zhang", "Yuqun Zhang"], "categories": ["cs.SE"], "published": "2024-05-13T07:56:15+00:00", "updated": "2024-05-13T07:56:15+00:00", "pdf_url": "http://arxiv.org/pdf/2405.07530v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2405.04126v1", "title": "Refining Joint Text and Source Code Embeddings for Retrieval Task with Parameter-Efficient Fine-Tuning", "abstract": "The latest developments in Natural Language Processing (NLP) have\ndemonstrated remarkable progress in a code-text retrieval problem. As the\nTransformer-based models used in this task continue to increase in size, the\ncomputational costs and time required for end-to-end fine-tuning become\nsubstantial. This poses a significant challenge for adapting and utilizing\nthese models when computational resources are limited. Motivated by these\nconcerns, we propose a fine-tuning framework that leverages Parameter-Efficient\nFine-Tuning (PEFT) techniques. Moreover, we adopt contrastive learning\nobjectives to improve the quality of bimodal representations learned by\ntransformer models. Additionally, for PEFT methods we provide extensive\nbenchmarking, the lack of which has been highlighted as a crucial problem in\nthe literature. Based on the thorough experimentation with the CodeT5+ model\nconducted on two datasets, we demonstrate that the proposed fine-tuning\nframework has the potential to improve code-text retrieval performance by\ntuning only 0.4% parameters at most.", "authors": ["Karim Galliamov", "Leila Khaertdinova", "Karina Denisova"], "categories": ["cs.LG", "cs.SE"], "published": "2024-05-07T08:50:25+00:00", "updated": "2024-05-07T08:50:25+00:00", "pdf_url": "http://arxiv.org/pdf/2405.04126v1", "primary_category": "cs.LG"}
{"id": "http://arxiv.org/abs/2405.02355v3", "title": "CodeGRAG: Bridging the Gap between Natural Language and Programming Language via Graphical Retrieval Augmented Generation", "abstract": "Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. Code is available at\nhttps://anonymous.4open.science/r/Code-5970/.", "authors": ["Kounianhua Du", "Jizheng Chen", "Renting Rui", "Huacan Chai", "Lingyue Fu", "Wei Xia", "Yasheng Wang", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "categories": ["cs.SE", "cs.AI"], "published": "2024-05-03T02:48:55+00:00", "updated": "2024-11-08T14:17:05+00:00", "pdf_url": "http://arxiv.org/pdf/2405.02355v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2404.16565v1", "title": "PyRadar: Towards Automatically Retrieving and Validating Source Code Repository Information for PyPI Packages", "abstract": "A package's source code repository records the development history of the\npackage, providing indispensable information for the use and risk monitoring of\nthe package. However, a package release often misses its source code repository\ndue to the separation of the package's development platform from its\ndistribution platform. Existing tools retrieve the release's repository\ninformation from its metadata, which suffers from two limitations: the metadata\nmay not contain or contain wrong information. Our analysis shows that existing\ntools can only retrieve repository information for up to 70.5% of PyPI\nreleases. To address the limitations, this paper proposes PyRadar, a novel\nframework that utilizes the metadata and source distribution to retrieve and\nvalidate the repository information for PyPI releases. We start with an\nempirical study to compare four existing tools on 4,227,425 PyPI releases and\nanalyze phantom files (files appearing in the release's distribution but not in\nthe release's repository) in 14,375 correct package-repository links and 2,064\nincorrect links. Based on the findings, we design PyRadar with three\ncomponents, i.e., Metadata-based Retriever, Source Code Repository Validator,\nand Source Code-based Retriever. In particular, the Metadata-based Retriever\ncombines best practices of existing tools and successfully retrieves repository\ninformation from the metadata for 72.1% of PyPI releases. The Source Code\nRepository Validator applies common machine learning algorithms on six crafted\nfeatures and achieves an AUC of up to 0.995. The Source Code-based Retriever\nqueries World of Code with the SHA-1 hashes of all Python files in the\nrelease's source distribution and retrieves repository information for 90.2% of\npackages in our dataset with an accuracy of 0.970. Both practitioners and\nresearchers can employ the PyRadar to better use PyPI packages.", "authors": ["Kai Gao", "Weiwei Xu", "Wenhao Yang", "Minghui Zhou"], "categories": ["cs.SE"], "published": "2024-04-25T12:27:59+00:00", "updated": "2024-04-25T12:27:59+00:00", "pdf_url": "http://arxiv.org/pdf/2404.16565v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2404.04688v1", "title": "Search-based Automated Program Repair of CPS Controllers Modeled in Simulink-Stateflow", "abstract": "Stateflow models are widely used in the industry to model the high-level\ncontrol logic of Cyber-Physical Systems (CPSs) in Simulink--the defacto CPS\nsimulator. Many approaches exist to test Simulink models, but once a fault is\ndetected, the process to repair it remains manual. Such a manual process\nincreases the software development cost, making it paramount to develop novel\ntechniques that reduce this cost. Automated Program Repair (APR) techniques can\nsignificantly reduce the time for fixing bugs by automatically generating\npatches. However, current approaches face scalability issues to be applicable\nin the CPS context. To deal with this problem, we propose an automated\nsearch-based approach called FlowRepair, explicitly designed to repair\nStateflow models. The novelty of FlowRepair includes, (1) a new algorithm that\ncombines global and local search for patch generation; (2) a definition of\nnovel repair objectives (e.g., the time a fault remained active) specifically\ndesigned for repairing CPSs; and (3) a set of mutation operators to repair\nStateflow models automatically. We evaluated FlowRepair with three different\ncase study systems and a total of nine faulty stateflow models. Our experiments\nsuggest that (1) Flo wRepaircan fix bugs in stateflow models, including models\nwith multiple faults; (2) FlowRepair surpasses or performs similarly to a\nbaseline APR technique inspired by a well-known CPS program repair approach.\nBesides, we provide both a replication package and a live repository, paving\nthe way towards the APR of CPSs modeled in Simulink.", "authors": ["Aitor Arrieta", "Pablo Valle", "Shaukat Ali"], "categories": ["cs.SE"], "published": "2024-04-06T17:31:13+00:00", "updated": "2024-04-06T17:31:13+00:00", "pdf_url": "http://arxiv.org/pdf/2404.04688v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2404.02319v2", "title": "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization", "abstract": "In many modern LLM applications, such as retrieval augmented generation,\nprompts have become programs themselves. In these settings, prompt programs are\nrepeatedly called with different user queries or data instances. A big\npractical challenge is optimizing such prompt programs. Recent work has mostly\nfocused on either simple prompt programs or assumed that the general structure\nof a prompt program is fixed.\n  We introduce SAMMO, a framework to perform symbolic prompt program search for\ncompile-time optimizations of prompt programs. SAMMO represents prompt programs\non a symbolic level which allows for a rich set of transformations that can be\nsearched over during optimization. We show that SAMMO generalizes previous\nmethods and improves the performance of complex prompts on (1) instruction\ntuning, (2) RAG pipeline tuning, and (3) prompt compression, across several\ndifferent LLMs. We make all code available open-source at\nhttps://github.com/microsoft/sammo .", "authors": ["Tobias Schnabel", "Jennifer Neville"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "published": "2024-04-02T21:35:54+00:00", "updated": "2024-06-27T23:22:14+00:00", "pdf_url": "http://arxiv.org/pdf/2404.02319v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2404.01554v1", "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion", "abstract": "The rise of code pre-trained models has significantly enhanced various coding\ntasks, such as code completion, and tools like GitHub Copilot. However, the\nsubstantial size of these models, especially large models, poses a significant\nchallenge when it comes to fine-tuning them for specific downstream tasks. As\nan alternative approach, retrieval-based methods have emerged as a promising\nsolution, augmenting model predictions without the need for fine-tuning.\nDespite their potential, a significant challenge is that the designs of these\nmethods often rely on heuristics, leaving critical questions about what\ninformation should be stored or retrieved and how to interpolate such\ninformation for augmenting predictions.\n  To tackle this challenge, we first perform a theoretical analysis of the\nfine-tuning process, highlighting the importance of delta logits as a catalyst\nfor improving model predictions. Building on this insight, we develop a novel\nretrieval-based method, FT2Ra, which aims to mimic genuine fine-tuning. While\nFT2Ra adopts a retrieval-based mechanism, it uniquely adopts a paradigm with a\nlearning rate and multi-epoch retrievals, which is similar to fine-tuning.In\ntoken-level completion, which represents a relatively easier task, FT2Ra\nachieves a 4.29% improvement in accuracy compared to the best baseline method\non UniXcoder. In the more challenging line-level completion task, we observe a\nsubstantial more than twice increase in Exact Match (EM) performance,\nindicating the significant advantages of our theoretical analysis. Notably,\neven when operating without actual fine-tuning, FT2Ra exhibits competitive\nperformance compared to the models with real fine-tuning.", "authors": ["Qi Guo", "Xiaohong Li", "Xiaofei Xie", "Shangqing Liu", "Ze Tang", "Ruitao Feng", "Junjie Wang", "Jidong Ge", "Lei Bu"], "categories": ["cs.SE"], "published": "2024-04-02T01:42:15+00:00", "updated": "2024-04-02T01:42:15+00:00", "pdf_url": "http://arxiv.org/pdf/2404.01554v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2403.16702v1", "title": "ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search", "abstract": "Retrieval-based code question answering seeks to match user queries in\nnatural language to relevant code snippets. Previous approaches typically rely\non pretraining models using crafted bi-modal and uni-modal datasets to align\ntext and code representations. In this paper, we introduce ProCQA, a\nlarge-scale programming question answering dataset extracted from the\nStackOverflow community, offering naturally structured mixed-modal QA pairs. To\nvalidate its effectiveness, we propose a modality-agnostic contrastive\npre-training approach to improve the alignment of text and code representations\nof current code language models. Compared to previous models that primarily\nemploy bimodal and unimodal pairs extracted from CodeSearchNet for\npre-training, our model exhibits significant performance improvements across a\nwide range of code retrieval benchmarks.", "authors": ["Zehan Li", "Jianfei Zhang", "Chuantao Yin", "Yuanxin Ouyang", "Wenge Rong"], "categories": ["cs.CL", "cs.IR", "cs.SE"], "published": "2024-03-25T12:34:33+00:00", "updated": "2024-03-25T12:34:33+00:00", "pdf_url": "http://arxiv.org/pdf/2403.16702v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2403.13583v3", "title": "CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing", "abstract": "Large Language Models have revolutionized code generation ability by\nconverting natural language descriptions into executable code. However,\ngenerating complex code within real-world scenarios remains challenging due to\nintricate structures, subtle bugs, understanding of advanced data types, and\nlack of supplementary contents. To address these challenges, we introduce the\nCoCoST framework, which enhances complex code generation by online searching\nfor more information with planned queries and correctness testing for code\nrefinement. Moreover, CoCoST serializes the complex inputs and outputs to\nimprove comprehension and generates test cases to ensure the adaptability for\nreal-world applications. CoCoST is validated through rigorous experiments on\nthe DS-1000 and ClassEval datasets. Experimental results show that CoCoST\nsubstantially improves the quality of complex code generation, highlighting its\npotential to enhance the practicality of LLMs in generating complex code.", "authors": ["Xinyi He", "Jiaru Zou", "Yun Lin", "Mengyu Zhou", "Shi Han", "Zejian Yuan", "Dongmei Zhang"], "categories": ["cs.SE", "cs.CL", "cs.LG"], "published": "2024-03-20T13:33:55+00:00", "updated": "2024-10-12T09:43:42+00:00", "pdf_url": "http://arxiv.org/pdf/2403.13583v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2403.10720v1", "title": "Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies", "abstract": "In this study, we explore the efficiency of the Monte Carlo Tree Search\n(MCTS), a prominent decision-making algorithm renowned for its effectiveness in\ncomplex decision environments, contingent upon the volume of simulations\nconducted. Notwithstanding its broad applicability, the algorithm's performance\ncan be adversely impacted in certain scenarios, particularly within the domain\nof game strategy development. This research posits that the inherent branch\ndivergence within the Da Vinci Code board game significantly impedes\nparallelism when executed on Graphics Processing Units (GPUs). To investigate\nthis hypothesis, we implemented and meticulously evaluated two variants of the\nMCTS algorithm, specifically designed to assess the impact of branch divergence\non computational performance. Our comparative analysis reveals a linear\nimprovement in performance with the CPU-based implementation, in stark contrast\nto the GPU implementation, which exhibits a non-linear enhancement pattern and\ndiscernible performance troughs. These findings contribute to a deeper\nunderstanding of the MCTS algorithm's behavior in divergent branch scenarios,\nhighlighting critical considerations for optimizing game strategy algorithms on\nparallel computing architectures.", "authors": ["Ye Zhang", "Mengran Zhu", "Kailin Gui", "Jiayue Yu", "Yong Hao", "Haozhan Sun"], "categories": ["cs.AI"], "published": "2024-03-15T22:43:37+00:00", "updated": "2024-03-15T22:43:37+00:00", "pdf_url": "http://arxiv.org/pdf/2403.10720v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2403.10059v2", "title": "Repoformer: Selective Retrieval for Repository-Level Code Completion", "abstract": "Recent advances in retrieval-augmented generation (RAG) have initiated a new\nera in repository-level code completion. However, the invariable use of\nretrieval in existing methods exposes issues in both efficiency and robustness,\nwith a large proportion of the retrieved contexts proving unhelpful or harmful\nto code language models (code LMs). In this paper, we propose a selective RAG\nframework to avoid retrieval when unnecessary. To power this framework, we\ndesign a self-supervised learning approach to enable a code LM to accurately\nself-evaluate whether retrieval can improve its output quality and robustly\nleverage the potentially noisy retrieved contexts. Using this LM as both the\nselective RAG policy and the generation model, our framework achieves\nstate-of-the-art repository-level code completion performance on diverse\nbenchmarks including RepoEval, CrossCodeEval, and CrossCodeLongEval, a new\nlong-form code completion benchmark. Meanwhile, our analyses show that\nselectively retrieving brings as much as 70% inference speedup in the online\nserving setting without harming the performance. We further demonstrate that\nour framework is able to accommodate different generation models, retrievers,\nand programming languages. These advancements position our framework as an\nimportant step towards more accurate and efficient repository-level code\ncompletion.", "authors": ["Di Wu", "Wasi Uddin Ahmad", "Dejiao Zhang", "Murali Krishna Ramanathan", "Xiaofei Ma"], "categories": ["cs.SE", "cs.CL"], "published": "2024-03-15T06:59:43+00:00", "updated": "2024-06-04T10:04:33+00:00", "pdf_url": "http://arxiv.org/pdf/2403.10059v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2403.06095v4", "title": "RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completion", "abstract": "Code Large Language Models (CodeLLMs) have demonstrated impressive\nproficiency in code completion tasks. However, they often fall short of fully\nunderstanding the extensive context of a project repository, such as the\nintricacies of relevant files and class hierarchies, which can result in less\nprecise completions. To overcome these limitations, we present \\tool, a\nmultifaceted framework designed to address the complex challenges associated\nwith repository-level code completion. Central to RepoHYPER is the {\\em\nRepo-level Semantic Graph} (RSG), a novel semantic graph structure that\nencapsulates the vast context of code repositories. Furthermore, RepoHyper\nleverages Expand and Refine retrieval method, including a graph expansion and a\nlink prediction algorithm applied to the RSG, enabling the effective retrieval\nand prioritization of relevant code snippets. Our evaluations show that \\tool\nmarkedly outperforms existing techniques in repository-level code completion,\nshowcasing enhanced accuracy across various datasets when compared to several\nstrong baselines. Our implementation of RepoHYPER can be found at\nhttps://github.com/FSoft-AI4Code/RepoHyper.", "authors": ["Huy N. Phan", "Hoang N. Phan", "Tien N. Nguyen", "Nghi D. Q. Bui"], "categories": ["cs.SE", "cs.AI"], "published": "2024-03-10T05:10:34+00:00", "updated": "2024-08-14T16:15:31+00:00", "pdf_url": "http://arxiv.org/pdf/2403.06095v4", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2403.01364v1", "title": "Improving Cross-lingual Representation for Semantic Retrieval with Code-switching", "abstract": "Semantic Retrieval (SR) has become an indispensable part of the FAQ system in\nthe task-oriented question-answering (QA) dialogue scenario. The demands for a\ncross-lingual smart-customer-service system for an e-commerce platform or some\nparticular business conditions have been increasing recently. Most previous\nstudies exploit cross-lingual pre-trained models (PTMs) for multi-lingual\nknowledge retrieval directly, while some others also leverage the continual\npre-training before fine-tuning PTMs on the downstream tasks. However, no\nmatter which schema is used, the previous work ignores to inform PTMs of some\nfeatures of the downstream task, i.e. train their PTMs without providing any\nsignals related to SR. To this end, in this work, we propose an Alternative\nCross-lingual PTM for SR via code-switching. We are the first to utilize the\ncode-switching approach for cross-lingual SR. Besides, we introduce the novel\ncode-switched continual pre-training instead of directly using the PTMs on the\nSR tasks. The experimental results show that our proposed approach consistently\noutperforms the previous SOTA methods on SR and semantic textual similarity\n(STS) tasks with three business corpora and four open datasets in 20+\nlanguages.", "authors": ["Mieradilijiang Maimaiti", "Yuanhang Zheng", "Ji Zhang", "Fei Huang", "Yue Zhang", "Wenpei Luo", "Kaiyu Huang"], "categories": ["cs.CL"], "published": "2024-03-03T01:47:52+00:00", "updated": "2024-03-03T01:47:52+00:00", "pdf_url": "http://arxiv.org/pdf/2403.01364v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2403.00865v1", "title": "Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning", "abstract": "In this paper, we develop upon the topic of loss function learning, an\nemergent meta-learning paradigm that aims to learn loss functions that\nsignificantly improve the performance of the models trained under them.\nSpecifically, we propose a new meta-learning framework for task and\nmodel-agnostic loss function learning via a hybrid search approach. The\nframework first uses genetic programming to find a set of symbolic loss\nfunctions. Second, the set of learned loss functions is subsequently\nparameterized and optimized via unrolled differentiation. The versatility and\nperformance of the proposed framework are empirically validated on a diverse\nset of supervised learning tasks. Results show that the learned loss functions\nbring improved convergence, sample efficiency, and inference performance on\ntabulated, computer vision, and natural language processing problems, using a\nvariety of task-specific neural network architectures.", "authors": ["Christian Raymond", "Qi Chen", "Bing Xue", "Mengjie Zhang"], "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "published": "2024-03-01T02:20:04+00:00", "updated": "2024-03-01T02:20:04+00:00", "pdf_url": "http://arxiv.org/pdf/2403.00865v1", "primary_category": "cs.NE"}
{"id": "http://arxiv.org/abs/2402.12317v2", "title": "EVOR: Evolving Retrieval for Code Generation", "abstract": "Recently the retrieval-augmented generation (RAG) has been successfully\napplied in code generation. However, existing pipelines for retrieval-augmented\ncode generation (RACG) employ static knowledge bases with a single source,\nlimiting the adaptation capabilities of Large Language Models (LLMs) to domains\nthey have insufficient knowledge of. In this work, we develop a novel pipeline,\nEVOR, that employs the synchronous evolution of both queries and diverse\nknowledge bases. On two realistic settings where the external knowledge is\nrequired to solve code generation tasks, we compile four new datasets\nassociated with frequently updated libraries and long-tail programming\nlanguages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR\nachieves two to four times of execution accuracy compared to other methods such\nas Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We\ndemonstrate that EVOR is flexible and can be easily combined with them to\nachieve further improvement. Further analysis reveals that EVOR benefits from\nthe synchronous evolution of queries and documents and the diverse information\nsources in the knowledge base. We hope that our studies will inspire more\ninsights into the design of advanced RACG pipelines in future research. Our\nmodel, code, and data are available at https://arks-codegen.github.io.", "authors": ["Hongjin Su", "Shuyang Jiang", "Yuhang Lai", "Haoyuan Wu", "Boao Shi", "Che Liu", "Qian Liu", "Tao Yu"], "categories": ["cs.CL", "cs.AI"], "published": "2024-02-19T17:37:28+00:00", "updated": "2024-12-03T15:56:26+00:00", "pdf_url": "http://arxiv.org/pdf/2402.12317v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2402.08147v2", "title": "VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search", "abstract": "Large Language Models (LLMs) can generate useful code, but often the code\nthey generate cannot be trusted to be sound. In this paper, we present VerMCTS,\nan approach to begin to resolve this issue by generating verified programs in\nDafny and Coq. VerMCTS uses a logical verifier in concert with an LLM to guide\na modified Monte Carlo Tree Search (MCTS). This approach leverages the verifier\nto gain intermediate feedback inside the search algorithm by checking partial\nprograms at each step to estimate an upper bound on the value function. To\nmeasure the performance of VerMCTS, we develop a new suite of multi-step\nverified programming problems in Dafny and Coq. In terms of pass@T, a new\nmetric which computes the pass rate given a budget of T tokens sampled from the\nLLM, VerMCTS leads to more than a 30% absolute increase in average pass@5000\nacross the suite over repeated sampling from the base language model. Our code\nand benchmarks are available at\nhttps://github.com/namin/llm-verified-with-monte-carlo-tree-search .", "authors": ["David Brandfonbrener", "Simon Henniger", "Sibi Raja", "Tarun Prasad", "Chloe Loughridge", "Federico Cassano", "Sabrina Ruixin Hu", "Jianang Yang", "William E. Byrd", "Robert Zinkov", "Nada Amin"], "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.LO", "cs.PL"], "published": "2024-02-13T00:55:14+00:00", "updated": "2024-05-24T14:51:14+00:00", "pdf_url": "http://arxiv.org/pdf/2402.08147v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2401.04514v2", "title": "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search", "abstract": "In code search, the Generation-Augmented Retrieval (GAR) framework, which\ngenerates exemplar code snippets to augment queries, has emerged as a promising\nstrategy to address the principal challenge of modality misalignment between\ncode snippets and natural language queries, particularly with the demonstrated\ncode generation capabilities of Large Language Models (LLMs). Nevertheless, our\npreliminary investigations indicate that the improvements conferred by such an\nLLM-augmented framework are somewhat constrained. This limitation could\npotentially be ascribed to the fact that the generated codes, albeit\nfunctionally accurate, frequently display a pronounced stylistic deviation from\nthe ground truth code in the codebase. In this paper, we extend the\nfoundational GAR framework and propose a simple yet effective method that\nadditionally Rewrites the Code (ReCo) within the codebase for style\nnormalization. Experimental results demonstrate that ReCo significantly boosts\nretrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%),\nand fine-tuned dense (up to 23.6%) retrieval settings in diverse search\nscenarios. To further elucidate the advantages of ReCo and stimulate research\nin code style normalization, we introduce Code Style Similarity, the first\nmetric tailored to quantify stylistic similarities in code. Notably, our\nempirical findings reveal the inadequacy of existing metrics in capturing\nstylistic nuances. The source code and data are available at\n\\url{https://github.com/Alex-HaochenLi/ReCo}.", "authors": ["Haochen Li", "Xin Zhou", "Zhiqi Shen"], "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.LG"], "published": "2024-01-09T12:12:50+00:00", "updated": "2024-06-03T06:50:26+00:00", "pdf_url": "http://arxiv.org/pdf/2401.04514v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2312.14798v1", "title": "Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs. SQL for No-Code Access to Relational Databases", "abstract": "Large Language Models (LLMs) have spurred progress in text-to-SQL, the task\nof generating SQL queries from natural language questions based on a given\ndatabase schema. Despite the declarative nature of SQL, it continues to be a\ncomplex programming language. In this paper, we investigate the potential of an\nalternative query language with simpler syntax and modular specification of\ncomplex queries. The purpose is to create a query language that can be learned\nmore easily by modern neural semantic parsing architectures while also enabling\nnon-programmers to better assess the validity of the query plans produced by an\ninteractive query plan assistant.\n  The proposed alternative query language is called Query Plan Language (QPL).\nIt is designed to be modular and can be translated into a restricted form of\nSQL Common Table Expressions (CTEs). The aim of QPL is to make complex data\nretrieval accessible to non-programmers by allowing users to express their\nquestions in natural language while also providing an easier-to-verify target\nlanguage. The paper demonstrates how neural LLMs can benefit from QPL's\nmodularity to generate complex query plans in a compositional manner. This\ninvolves a question decomposition strategy and a planning stage.\n  We conduct experiments on a version of the Spider text-to-SQL dataset that\nhas been converted to QPL. The hierarchical structure of QPL programs enables\nus to measure query complexity naturally. Based on this assessment, we identify\nthe low accuracy of existing text-to-SQL systems on complex compositional\nqueries. We present ways to address the challenge of complex queries in an\niterative, user-controlled manner, using fine-tuned LLMs and a variety of\nprompting strategies in a compositional manner.", "authors": ["Ben Eyal", "Amir Bachar", "Ophir Haroche", "Michael Elhadad"], "categories": ["cs.CL"], "published": "2023-12-22T16:16:15+00:00", "updated": "2023-12-22T16:16:15+00:00", "pdf_url": "http://arxiv.org/pdf/2312.14798v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2312.04731v1", "title": "STraceBERT: Source Code Retrieval using Semantic Application Traces", "abstract": "Software reverse engineering is an essential task in software engineering and\nsecurity, but it can be a challenging process, especially for adversarial\nartifacts. To address this challenge, we present STraceBERT, a novel approach\nthat utilizes a Java dynamic analysis tool to record calls to core Java\nlibraries, and pretrain a BERT-style model on the recorded application traces\nfor effective method source code retrieval from a candidate set. Our\nexperiments demonstrate the effectiveness of STraceBERT in retrieving the\nsource code compared to existing approaches. Our proposed approach offers a\npromising solution to the problem of code retrieval in software reverse\nengineering and opens up new avenues for further research in this area.", "authors": ["Claudio Spiess"], "categories": ["cs.SE", "cs.AI", "cs.IR"], "published": "2023-12-07T22:19:50+00:00", "updated": "2023-12-07T22:19:50+00:00", "pdf_url": "http://arxiv.org/pdf/2312.04731v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2311.14901v2", "title": "Code Search Debiasing:Improve Search Results beyond Overall Ranking Performance", "abstract": "Code search engine is an essential tool in software development. Many code\nsearch methods have sprung up, focusing on the overall ranking performance of\ncode search. In this paper, we study code search from another perspective by\nanalyzing the bias of code search models. Biased code search engines provide\npoor user experience, even though they show promising overall performance. Due\nto different development conventions (e.g., prefer long queries or\nabbreviations), some programmers will find the engine useful, while others may\nfind it hard to get desirable search results. To mitigate biases, we develop a\ngeneral debiasing framework that employs reranking to calibrate search results.\nIt can be easily plugged into existing engines and handle new code search\nbiases discovered in the future. Experiments show that our framework can\neffectively reduce biases. Meanwhile, the overall ranking performance of code\nsearch gets improved after debiasing.", "authors": ["Sheng Zhang", "Hui Li", "Yanlin Wang", "Zhao Wei", "Yong Xiu", "Juhong Wang", "Rongong Ji"], "categories": ["cs.CL"], "published": "2023-11-25T02:31:22+00:00", "updated": "2024-02-17T01:22:24+00:00", "pdf_url": "http://arxiv.org/pdf/2311.14901v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2311.07107v1", "title": "A Survey of Source Code Search: A 3-Dimensional Perspective", "abstract": "(Source) code search is widely concerned by software engineering researchers\nbecause it can improve the productivity and quality of software development.\nGiven a functionality requirement usually described in a natural language\nsentence, a code search system can retrieve code snippets that satisfy the\nrequirement from a large-scale code corpus, e.g., GitHub. To realize effective\nand efficient code search, many techniques have been proposed successively.\nThese techniques improve code search performance mainly by optimizing three\ncore components, including query understanding component, code understanding\ncomponent, and query-code matching component. In this paper, we provide a\n3-dimensional perspective survey for code search. Specifically, we categorize\nexisting code search studies into query-end optimization techniques, code-end\noptimization techniques, and match-end optimization techniques according to the\nspecific components they optimize. Considering that each end can be optimized\nindependently and contributes to the code search performance, we treat each end\nas a dimension. Therefore, this survey is 3-dimensional in nature, and it\nprovides a comprehensive summary of each dimension in detail. To understand the\nresearch trends of the three dimensions in existing code search studies, we\nsystematically review 68 relevant literatures. Different from existing code\nsearch surveys that only focus on the query end or code end or introduce\nvarious aspects shallowly (including codebase, evaluation metrics, modeling\ntechnique, etc.), our survey provides a more nuanced analysis and review of the\nevolution and development of the underlying techniques used in the three ends.\nBased on a systematic review and summary of existing work, we outline several\nopen challenges and opportunities at the three ends that remain to be addressed\nin future work.", "authors": ["Weisong Sun", "Chunrong Fang", "Yifei Ge", "Yuling Hu", "Yuchen Chen", "Quanjun Zhang", "Xiuting Ge", "Yang Liu", "Zhenyu Chen"], "categories": ["cs.SE", "68-04", "D.2.3"], "published": "2023-11-13T06:42:08+00:00", "updated": "2023-11-13T06:42:08+00:00", "pdf_url": "http://arxiv.org/pdf/2311.07107v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2311.02962v1", "title": "Retrieval-Augmented Code Generation for Universal Information Extraction", "abstract": "Information Extraction (IE) aims to extract structural knowledge (e.g.,\nentities, relations, events) from natural language texts, which brings\nchallenges to existing methods due to task-specific schemas and complex text\nexpressions. Code, as a typical kind of formalized language, is capable of\ndescribing structural knowledge under various schemas in a universal way. On\nthe other hand, Large Language Models (LLMs) trained on both codes and texts\nhave demonstrated powerful capabilities of transforming texts into codes, which\nprovides a feasible solution to IE tasks. Therefore, in this paper, we propose\na universal retrieval-augmented code generation framework based on LLMs, called\nCode4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define\ntask-specific schemas of various structural knowledge in a universal way. By so\ndoing, extracting knowledge under these schemas can be transformed into\ngenerating codes that instantiate the predefined Python classes with the\ninformation in texts. To generate these codes more precisely, Code4UIE adopts\nthe in-context learning mechanism to instruct LLMs with examples. In order to\nobtain appropriate examples for different tasks, Code4UIE explores several\nexample retrieval strategies, which can retrieve examples semantically similar\nto the given texts. Extensive experiments on five representative IE tasks\nacross nine datasets demonstrate the effectiveness of the Code4UIE framework.", "authors": ["Yucan Guo", "Zixuan Li", "Xiaolong Jin", "Yantao Liu", "Yutao Zeng", "Wenxuan Liu", "Xiang Li", "Pan Yang", "Long Bai", "Jiafeng Guo", "Xueqi Cheng"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "published": "2023-11-06T09:03:21+00:00", "updated": "2023-11-06T09:03:21+00:00", "pdf_url": "http://arxiv.org/pdf/2311.02962v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2310.11546v1", "title": "Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models", "abstract": "Data generation and analysis is a fundamental aspect of many industries and\ndisciplines, from strategic decision making in business to research in the\nphysical and social sciences. However, data generated using software and\nalgorithms can be subject to biases and errors. These can be due to problems\nwith the original software, default settings that do not align with the\nspecific needs of the situation, or even deeper problems with the underlying\ntheories and models. This paper proposes an advanced search and optimization\nframework aimed at generating and choosing optimal source code capable of\ncorrecting errors and biases from previous versions to address typical problems\nin software systems specializing in data analysis and generation, especially\nthose in the corporate and data science world. Applying this framework multiple\ntimes on the same software system would incrementally improve the quality of\nthe output results. It uses Solomonoff Induction as a sound theoretical basis,\nextending it with Kolmogorov Conditional Complexity, a novel adaptation, to\nevaluate a set of candidate programs. We propose the use of generative models\nfor the creation of this set of programs, with special emphasis on the\ncapabilities of Large Language Models (LLMs) to generate high quality code.", "authors": ["Ernesto Giralt Hern\u00e1ndez"], "categories": ["cs.SE", "cs.IT", "cs.LG", "math.IT", "math.OC"], "published": "2023-10-17T19:31:05+00:00", "updated": "2023-10-17T19:31:05+00:00", "pdf_url": "http://arxiv.org/pdf/2310.11546v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2310.08069v1", "title": "Rethinking Negative Pairs in Code Search", "abstract": "Recently, contrastive learning has become a key component in fine-tuning code\nsearch models for software development efficiency and effectiveness. It pulls\ntogether positive code snippets while pushing negative samples away given\nsearch queries. Among contrastive learning, InfoNCE is the most widely used\nloss function due to its better performance. However, the following problems in\nnegative samples of InfoNCE may deteriorate its representation learning: 1) The\nexistence of false negative samples in large code corpora due to duplications.\n2). The failure to explicitly differentiate between the potential relevance of\nnegative samples. As an example, a bubble sorting algorithm example is less\n``negative'' than a file saving function for the quick sorting algorithm query.\nIn this paper, we tackle the above problems by proposing a simple yet effective\nSoft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss\nfunction, we apply three methods to estimate the weights of negative pairs and\nshow that the vanilla InfoNCE loss is a special case of Soft-InfoNCE.\nTheoretically, we analyze the effects of Soft-InfoNCE on controlling the\ndistribution of learnt code representations and on deducing a more precise\nmutual information estimation. We furthermore discuss the superiority of\nproposed loss functions with other design alternatives. Extensive experiments\ndemonstrate the effectiveness of Soft-InfoNCE and weights estimation methods\nunder state-of-the-art code search models on a large-scale public dataset\nconsisting of six programming languages. Source code is available at\n\\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.", "authors": ["Haochen Li", "Xin Zhou", "Luu Anh Tuan", "Chunyan Miao"], "categories": ["cs.SE", "cs.CL", "cs.IR", "cs.LG"], "published": "2023-10-12T06:32:42+00:00", "updated": "2023-10-12T06:32:42+00:00", "pdf_url": "http://arxiv.org/pdf/2310.08069v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2310.06342v1", "title": "Contrastive Prompt Learning-based Code Search based on Interaction Matrix", "abstract": "Code search aims to retrieve the code snippet that highly matches the given\nquery described in natural language. Recently, many code pre-training\napproaches have demonstrated impressive performance on code search. However,\nexisting code search methods still suffer from two performance constraints:\ninadequate semantic representation and the semantic gap between natural\nlanguage (NL) and programming language (PL). In this paper, we propose CPLCS, a\ncontrastive prompt learning-based code search method based on the cross-modal\ninteraction mechanism. CPLCS comprises:(1) PL-NL contrastive learning, which\nlearns the semantic matching relationship between PL and NL representations;\n(2) a prompt learning design for a dual-encoder structure that can alleviate\nthe problem of inadequate semantic representation; (3) a cross-modal\ninteraction mechanism to enhance the fine-grained mapping between NL and PL. We\nconduct extensive experiments to evaluate the effectiveness of our approach on\na real-world dataset across six programming languages. The experiment results\ndemonstrate the efficacy of our approach in improving semantic representation\nquality and mapping ability between PL and NL.", "authors": ["Yubo Zhang", "Yanfang Liu", "Xinxin Fan", "Yunfeng Lu"], "categories": ["cs.SE", "cs.AI"], "published": "2023-10-10T06:24:52+00:00", "updated": "2023-10-10T06:24:52+00:00", "pdf_url": "http://arxiv.org/pdf/2310.06342v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2310.05286v2", "title": "Generalizable Error Modeling for Human Data Annotation: Evidence From an Industry-Scale Search Data Annotation Program", "abstract": "Machine learning (ML) and artificial intelligence (AI) systems rely heavily\non human-annotated data for training and evaluation. A major challenge in this\ncontext is the occurrence of annotation errors, as their effects can degrade\nmodel performance. This paper presents a predictive error model trained to\ndetect potential errors in search relevance annotation tasks for three\nindustry-scale ML applications (music streaming, video streaming, and mobile\napps). Drawing on real-world data from an extensive search relevance annotation\nprogram, we demonstrate that errors can be predicted with moderate model\nperformance (AUC=0.65-0.75) and that model performance generalizes well across\napplications (i.e., a global, task-agnostic model performs on par with\ntask-specific models). In contrast to past research, which has often focused on\npredicting annotation labels from task-specific features, our model is trained\nto predict errors directly from a combination of task features and behavioral\nfeatures derived from the annotation process, in order to achieve a high degree\nof generalizability. We demonstrate the usefulness of the model in the context\nof auditing, where prioritizing tasks with high predicted error probabilities\nconsiderably increases the amount of corrected annotation errors (e.g., 40%\nefficiency gains for the music streaming application). These results highlight\nthat behavioral error detection models can yield considerable improvements in\nthe efficiency and quality of data annotation processes. Our findings reveal\ncritical insights into effective error management in the data annotation\nprocess, thereby contributing to the broader field of human-in-the-loop ML.", "authors": ["Heinrich Peters", "Alireza Hashemi", "James Rae"], "categories": ["cs.LG", "cs.AI", "cs.HC"], "published": "2023-10-08T21:21:19+00:00", "updated": "2024-09-25T22:41:14+00:00", "pdf_url": "http://arxiv.org/pdf/2310.05286v2", "primary_category": "cs.LG"}
{"id": "http://arxiv.org/abs/2310.03605v3", "title": "FASER: Binary Code Similarity Search through the use of Intermediate Representations", "abstract": "Being able to identify functions of interest in cross-architecture software\nis useful whether you are analysing for malware, securing the software supply\nchain or conducting vulnerability research. Cross-Architecture Binary Code\nSimilarity Search has been explored in numerous studies and has used a wide\nrange of different data sources to achieve its goals. The data sources\ntypically used draw on common structures derived from binaries such as function\ncontrol flow graphs or binary level call graphs, the output of the disassembly\nprocess or the outputs of a dynamic analysis approach. One data source which\nhas received less attention is binary intermediate representations. Binary\nIntermediate representations possess two interesting properties: they are cross\narchitecture by their very nature and encode the semantics of a function\nexplicitly to support downstream usage. Within this paper we propose Function\nas a String Encoded Representation (FASER) which combines long document\ntransformers with the use of intermediate representations to create a model\ncapable of cross architecture function search without the need for manual\nfeature engineering, pre-training or a dynamic analysis step. We compare our\napproach against a series of baseline approaches for two tasks; A general\nfunction search task and a targeted vulnerability search task. Our approach\ndemonstrates strong performance across both tasks, performing better than all\nbaseline approaches.", "authors": ["Josh Collyer", "Tim Watson", "Iain Phillips"], "categories": ["cs.CR", "cs.AI", "cs.IR", "cs.LG"], "published": "2023-10-05T15:36:35+00:00", "updated": "2023-11-29T14:30:29+00:00", "pdf_url": "http://arxiv.org/pdf/2310.03605v3", "primary_category": "cs.CR"}
{"id": "http://arxiv.org/abs/2309.06057v1", "title": "RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair", "abstract": "Automatic program repair (APR) is crucial to reduce manual debugging efforts\nfor developers and improve software reliability. While conventional\nsearch-based techniques typically rely on heuristic rules or a redundancy\nassumption to mine fix patterns, recent years have witnessed the surge of deep\nlearning (DL) based approaches to automate the program repair process in a\ndata-driven manner. However, their performance is often limited by a fixed set\nof parameters to model the highly complex search space of APR. To ease such\nburden on the parametric models, in this work, we propose a novel\nRetrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly\nleveraging relevant fix patterns retrieved from a codebase of previous bug-fix\npairs. Specifically, we build a hybrid patch retriever to account for both\nlexical and semantic matching based on the raw source code in a\nlanguage-agnostic manner, which does not rely on any code-specific features. In\naddition, we adapt a code-aware language model CodeT5 as our foundation model\nto facilitate both patch retrieval and generation tasks in a unified manner. We\nadopt a stage-wise approach where the patch retriever first retrieves a\nrelevant external bug-fix pair to augment the buggy input for the CodeT5 patch\ngenerator, which synthesizes a ranked list of repair patch candidates. Notably,\nRAP-Gen is a generic APR framework that can flexibly integrate different patch\nretrievers and generators to repair various types of bugs. We thoroughly\nevaluate RAP-Gen on three benchmarks in two programming languages, including\nthe TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks\nin Java, where the bug localization information may or may not be provided.\nExperimental results show that RAP-Gen significantly outperforms previous\nstate-of-the-art approaches on all benchmarks, e.g., repairing 15 more bugs on\n818 Defects4J bugs.", "authors": ["Weishi Wang", "Yue Wang", "Shafiq Joty", "Steven C. H. Hoi"], "categories": ["cs.SE", "cs.CL"], "published": "2023-09-12T08:52:56+00:00", "updated": "2023-09-12T08:52:56+00:00", "pdf_url": "http://arxiv.org/pdf/2309.06057v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2308.15234v1", "title": "Hyperbolic Code Retrieval: A Novel Approach for Efficient Code Search Using Hyperbolic Space Embeddings", "abstract": "Within the realm of advanced code retrieval, existing methods have primarily\nrelied on intricate matching and attention-based mechanisms. However, these\nmethods often lead to computational and memory inefficiencies, posing a\nsignificant challenge to their real-world applicability. To tackle this\nchallenge, we propose a novel approach, the Hyperbolic Code QA Matching\n(HyCoQA). This approach leverages the unique properties of Hyperbolic space to\nexpress connections between code fragments and their corresponding queries,\nthereby obviating the necessity for intricate interaction layers. The process\ncommences with a reimagining of the code retrieval challenge, framed within a\nquestion-answering (QA) matching framework, constructing a dataset with triple\nmatches characterized as \\texttt{<}negative code, description, positive\ncode\\texttt{>}. These matches are subsequently processed via a static BERT\nembedding layer, yielding initial embeddings. Thereafter, a hyperbolic embedder\ntransforms these representations into hyperbolic space, calculating distances\nbetween the codes and descriptions. The process concludes by implementing a\nscoring layer on these distances and leveraging hinge loss for model training.\nEspecially, the design of HyCoQA inherently facilitates self-organization,\nallowing for the automatic detection of embedded hierarchical patterns during\nthe learning phase. Experimentally, HyCoQA showcases remarkable effectiveness\nin our evaluations: an average performance improvement of 3.5\\% to 4\\% compared\nto state-of-the-art code retrieval techniques.", "authors": ["Xunzhu Tang", "zhenghan Chen", "Saad Ezzini", "Haoye Tian", "Yewei Song", "Jacques Klein", "Tegawende F. Bissyande"], "categories": ["cs.SE"], "published": "2023-08-29T11:45:20+00:00", "updated": "2023-08-29T11:45:20+00:00", "pdf_url": "http://arxiv.org/pdf/2308.15234v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2308.13775v2", "title": "EditSum: A Retrieve-and-Edit Framework for Source Code Summarization", "abstract": "Existing studies show that code summaries help developers understand and\nmaintain source code. Unfortunately, these summaries are often missing or\noutdated in software projects. Code summarization aims to generate natural\nlanguage descriptions automatically for source code. Code summaries are highly\nstructured and have repetitive patterns. Besides the patternized words, a code\nsummary also contains important keywords, which are the key to reflecting the\nfunctionality of the code. However, the state-of-the-art approaches perform\npoorly on predicting the keywords, which leads to the generated summaries\nsuffering a loss in informativeness. To alleviate this problem, this paper\nproposes a novel retrieve-and-edit approach named EditSum for code\nsummarization. Specifically, EditSum first retrieves a similar code snippet\nfrom a pre-defined corpus and treats its summary as a prototype summary to\nlearn the pattern. Then, EditSum edits the prototype automatically to combine\nthe pattern in the prototype with the semantic information of input code. Our\nmotivation is that the retrieved prototype provides a good start-point for\npost-generation because the summaries of similar code snippets often have the\nsame pattern. The post-editing process further reuses the patternized words in\nthe prototype and generates keywords based on the semantic information of input\ncode. We conduct experiments on a large-scale Java corpus and experimental\nresults demonstrate that EditSum outperforms the state-of-the-art approaches by\na substantial margin. The human evaluation also proves the summaries generated\nby EditSum are more informative and useful. We also verify that EditSum\nperforms well on predicting the patternized words and keywords.", "authors": ["Jia Li", "Yongmin Li", "Ge Li", "Xing Hu", "Xin Xia", "Zhi Jin"], "categories": ["cs.SE", "cs.CL"], "published": "2023-08-26T05:48:57+00:00", "updated": "2023-09-07T11:19:30+00:00", "pdf_url": "http://arxiv.org/pdf/2308.13775v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2308.04693v1", "title": "Evaluating and Optimizing the Effectiveness of Neural Machine Translation in Supporting Code Retrieval Models: A Study on the CAT Benchmark", "abstract": "Neural Machine Translation (NMT) is widely applied in software engineering\ntasks. The effectiveness of NMT for code retrieval relies on the ability to\nlearn from the sequence of tokens in the source language to the sequence of\ntokens in the target language. While NMT performs well in pseudocode-to-code\ntranslation, it might have challenges in learning to translate from natural\nlanguage query to source code in newly curated real-world code documentation/\nimplementation datasets. In this work, we analyze the performance of NMT in\nnatural language-to-code translation in the newly curated CAT benchmark that\nincludes the optimized versions of three Java datasets TLCodeSum,\nCodeSearchNet, Funcom, and a Python dataset PCSD. Our evaluation shows that NMT\nhas low accuracy, measured by CrystalBLEU and Meteor metrics in this task. To\nalleviate the duty of NMT in learning complex representation of source code, we\npropose ASTTrans Representation, a tailored representation of an Abstract\nSyntax Tree (AST) using a subset of non-terminal nodes. We show that the\nclassical approach NMT performs significantly better in learning ASTTrans\nRepresentation over code tokens with up to 36% improvement on Meteor score.\nMoreover, we leverage ASTTrans Representation to conduct combined code search\nprocesses from the state-of-the-art code search processes using GraphCodeBERT\nand UniXcoder. Our NMT models of learning ASTTrans Representation can boost the\nMean Reciprocal Rank of these state-of-the-art code search processes by up to\n3.08% and improve 23.08% of queries' results over the CAT benchmark.", "authors": ["Hung Phan", "Ali Jannesari"], "categories": ["cs.SE", "cs.IR"], "published": "2023-08-09T04:06:24+00:00", "updated": "2023-08-09T04:06:24+00:00", "pdf_url": "http://arxiv.org/pdf/2308.04693v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2307.05603v1", "title": "Can You Improve My Code? Optimizing Programs with Local Search", "abstract": "This paper introduces a local search method for improving an existing program\nwith respect to a measurable objective. Program Optimization with Locally\nImproving Search (POLIS) exploits the structure of a program, defined by its\nlines. POLIS improves a single line of the program while keeping the remaining\nlines fixed, using existing brute-force synthesis algorithms, and continues\niterating until it is unable to improve the program's performance. POLIS was\nevaluated with a 27-person user study, where participants wrote programs\nattempting to maximize the score of two single-agent games: Lunar Lander and\nHighway. POLIS was able to substantially improve the participants' programs\nwith respect to the game scores. A proof-of-concept demonstration on existing\nStack Overflow code measures applicability in real-world problems. These\nresults suggest that POLIS could be used as a helpful programming assistant for\nprogramming problems with measurable objectives.", "authors": ["Fatemeh Abdollahi", "Saqib Ameen", "Matthew E. Taylor", "Levi H. S. Lelis"], "categories": ["cs.SE", "cs.LG", "cs.PL"], "published": "2023-07-10T20:39:41+00:00", "updated": "2023-07-10T20:39:41+00:00", "pdf_url": "http://arxiv.org/pdf/2307.05603v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2307.00267v1", "title": "Self-Supervised Query Reformulation for Code Search", "abstract": "Automatic query reformulation is a widely utilized technology for enriching\nuser requirements and enhancing the outcomes of code search. It can be\nconceptualized as a machine translation task, wherein the objective is to\nrephrase a given query into a more comprehensive alternative. While showing\npromising results, training such a model typically requires a large parallel\ncorpus of query pairs (i.e., the original query and a reformulated query) that\nare confidential and unpublished by online code search engines. This restricts\nits practicality in software development processes. In this paper, we propose\nSSQR, a self-supervised query reformulation method that does not rely on any\nparallel query corpus. Inspired by pre-trained models, SSQR treats query\nreformulation as a masked language modeling task conducted on an extensive\nunannotated corpus of queries. SSQR extends T5 (a sequence-to-sequence model\nbased on Transformer) with a new pre-training objective named corrupted query\ncompletion (CQC), which randomly masks words within a complete query and trains\nT5 to predict the masked content. Subsequently, for a given query to be\nreformulated, SSQR identifies potential locations for expansion and leverages\nthe pre-trained T5 model to generate appropriate content to fill these gaps.\nThe selection of expansions is then based on the information gain associated\nwith each candidate. Evaluation results demonstrate that SSQR outperforms\nunsupervised baselines significantly and achieves competitive performance\ncompared to supervised methods.", "authors": ["Yuetian Mao", "Chengcheng Wan", "Yuze Jiang", "Xiaodong Gu"], "categories": ["cs.SE"], "published": "2023-07-01T08:17:23+00:00", "updated": "2023-07-01T08:17:23+00:00", "pdf_url": "http://arxiv.org/pdf/2307.00267v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2306.15604v1", "title": "Constructing Multilingual Code Search Dataset Using Neural Machine Translation", "abstract": "Code search is a task to find programming codes that semantically match the\ngiven natural language queries. Even though some of the existing datasets for\nthis task are multilingual on the programming language side, their query data\nare only in English. In this research, we create a multilingual code search\ndataset in four natural and four programming languages using a neural machine\ntranslation model. Using our dataset, we pre-train and fine-tune the\nTransformer-based models and then evaluate them on multiple code search test\nsets. Our results show that the model pre-trained with all natural and\nprogramming language data has performed best in most cases. By applying\nback-translation data filtering to our dataset, we demonstrate that the\ntranslation quality affects the model's performance to a certain extent, but\nthe data size matters more.", "authors": ["Ryo Sekizawa", "Nan Duan", "Shuai Lu", "Hitomi Yanaka"], "categories": ["cs.CL", "cs.SE"], "published": "2023-06-27T16:42:36+00:00", "updated": "2023-06-27T16:42:36+00:00", "pdf_url": "http://arxiv.org/pdf/2306.15604v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2306.06490v2", "title": "Automated Code Editing with Search-Generate-Modify", "abstract": "Code editing is essential in evolving software development. Many automated\ncode editing tools have been proposed that leverage both Information\nRetrieval-based techniques and Machine Learning-based code generation and code\nediting models. Each technique comes with its own promises and perils, and they\nare often used together to complement their strengths and compensate for their\nweaknesses. This paper proposes a hybrid approach to better synthesize code\nedits by leveraging the power of code search, generation, and modification. Our\nkey observation is that a patch obtained by search and retrieval, even if\nimperfect, can provide helpful guidance to a code generation model. However, a\nretrieval-guided patch produced by a code generation model can still be a few\ntokens off from the intended patch. Such generated patches can be slightly\nmodified to create the intended patches. SARGAM is a novel tool designed to\nmimic a real developer's code editing behavior. Given an original code version,\nthe developer may search for related patches, generate or write the code, and\nthen modify the generated code to adapt it to the right context. Our evaluation\nof SARGAM on edit generation shows superior performance with respect to current\nstate-of-the-art techniques. SARGAM also shows great effectiveness on automated\nprogram repair tasks.", "authors": ["Changshu Liu", "Pelin Cetin", "Yogesh Patodia", "Saikat Chakraborty", "Yangruibo Ding", "Baishakhi Ray"], "categories": ["cs.SE", "cs.PL"], "published": "2023-06-10T17:11:21+00:00", "updated": "2024-02-26T16:03:24+00:00", "pdf_url": "http://arxiv.org/pdf/2306.06490v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2305.17506v2", "title": "Backdooring Neural Code Search", "abstract": "Reusing off-the-shelf code snippets from online repositories is a common\npractice, which significantly enhances the productivity of software developers.\nTo find desired code snippets, developers resort to code search engines through\nnatural language queries. Neural code search models are hence behind many such\nengines. These models are based on deep learning and gain substantial attention\ndue to their impressive performance. However, the security aspect of these\nmodels is rarely studied. Particularly, an adversary can inject a backdoor in\nneural code search models, which return buggy or even vulnerable code with\nsecurity/privacy issues. This may impact the downstream software (e.g., stock\ntrading systems and autonomous driving) and cause financial loss and/or\nlife-threatening incidents. In this paper, we demonstrate such attacks are\nfeasible and can be quite stealthy. By simply modifying one variable/function\nname, the attacker can make buggy/vulnerable code rank in the top 11%. Our\nattack BADCODE features a special trigger generation and injection procedure,\nmaking the attack more effective and stealthy. The evaluation is conducted on\ntwo neural code search models and the results show our attack outperforms\nbaselines by 60%. Our user study demonstrates that our attack is more stealthy\nthan the baseline by two times based on the F1 score.", "authors": ["Weisong Sun", "Yuchen Chen", "Guanhong Tao", "Chunrong Fang", "Xiangyu Zhang", "Quanjun Zhang", "Bin Luo"], "categories": ["cs.SE", "cs.AI", "cs.CL", "68T01", "I.2.2; D.2.13"], "published": "2023-05-27T16:00:50+00:00", "updated": "2023-06-12T08:05:03+00:00", "pdf_url": "http://arxiv.org/pdf/2305.17506v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2305.13347v1", "title": "Further Decimating the Inductive Programming Search Space with Instruction Digrams", "abstract": "Overlapping instruction subsets derived from human originated code have\npreviously been shown to dramatically shrink the inductive programming search\nspace, often by many orders of magnitude. Here we extend the instruction subset\napproach to consider direct instruction-instruction applications (or\ninstruction digrams) as an additional search heuristic for inductive\nprogramming. In this study we analyse the frequency distribution of instruction\ndigrams in a large sample of open source code. This indicates that the\ninstruction digram distribution is highly skewed with over 93% of possible\ninstruction digrams not represnted in the code sample. We demonstrate that\ninstruction digrams can be used to constrain instruction selection during\nsearch, further reducing size of the the search space, in some cases by several\norders of magnitude. This significantly increases the size of programs that can\nbe generated using search based inductive programming techniques. We discuss\nthe results and provide some suggestions for further work.", "authors": ["Edward McDaid", "Sarah McDaid"], "categories": ["cs.PL", "cs.AI", "I.2.2; I.2.5; I.2.8; I.2.11; F.3.1; D.3.1"], "published": "2023-05-22T15:58:18+00:00", "updated": "2023-05-22T15:58:18+00:00", "pdf_url": "http://arxiv.org/pdf/2305.13347v1", "primary_category": "cs.PL"}
{"id": "http://arxiv.org/abs/2305.11626v1", "title": "CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search", "abstract": "We consider the clone detection and information retrieval problems for source\ncode, well-known tasks important for any programming language. Although it is\nalso an important and interesting problem to find code snippets that operate\nidentically but are written in different programming languages, to the best of\nour knowledge multilingual clone detection has not been studied in literature.\nIn this work, we formulate the multilingual clone detection problem and present\nXCD, a new benchmark dataset produced from the CodeForces submissions dataset.\nMoreover, we present a novel training procedure, called cross-consistency\ntraining (CCT), that we apply to train language models on source code in\ndifferent programming languages. The resulting CCT-LM model, initialized with\nGraphCodeBERT and fine-tuned with CCT, achieves new state of the art,\noutperforming existing approaches on the POJ-104 clone detection benchmark with\n95.67\\% MAP and AdvTest code search benchmark with 47.18\\% MRR; it also shows\nthe best results on the newly created multilingual clone detection benchmark\nXCD across all programming languages.", "authors": ["Nikita Sorokin", "Dmitry Abulkhanov", "Sergey Nikolenko", "Valentin Malykh"], "categories": ["cs.CL", "cs.SE"], "published": "2023-05-19T12:09:49+00:00", "updated": "2023-05-19T12:09:49+00:00", "pdf_url": "http://arxiv.org/pdf/2305.11626v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2305.11625v2", "title": "Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets", "abstract": "Code search is an important and well-studied task, but it usually means\nsearching for code by a text query. We argue that using a code snippet (and\npossibly an error traceback) as a query while looking for bugfixing\ninstructions and code samples is a natural use case not covered by prior art.\nMoreover, existing datasets use code comments rather than full-text\ndescriptions as text, making them unsuitable for this use case. We present a\nnew SearchBySnippet dataset implementing the search-by-code use case based on\nStackOverflow data; we show that on SearchBySnippet, existing architectures\nfall short of a simple BM25 baseline even after fine-tuning. We present a new\nsingle encoder model SnippeR that outperforms several strong baselines on\nSearchBySnippet with a result of 0.451 Recall@10; we propose the\nSearchBySnippet dataset and SnippeR as a new important benchmark for code\nsearch evaluation.", "authors": ["Ivan Sedykh", "Dmitry Abulkhanov", "Nikita Sorokin", "Sergey Nikolenko", "Valentin Malykh"], "categories": ["cs.CL", "cs.SE"], "published": "2023-05-19T12:09:30+00:00", "updated": "2024-05-27T05:44:48+00:00", "pdf_url": "http://arxiv.org/pdf/2305.11625v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2305.11074v3", "title": "Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization", "abstract": "Automatically generating human-readable text describing the functionality of\na program is the intent of source code summarization. Although neural language\nmodels achieve significant performance in this field, they are limited by their\ninability to access external knowledge. To address this limitation, an emerging\ntrend is combining neural models with external knowledge through retrieval\nmethods. Previous methods have relied on the sentence-level retrieval paradigm\non the encoder side. However, this paradigm is coarse-grained, noise-filled and\ncannot directly take advantage of the high-quality retrieved summary tokens on\nthe decoder side. In this paper, we propose a fine-grained Token-level\nretrieval-augmented mechanism (Tram) on the decoder side rather than the\nencoder side to enhance the performance of neural models and produce more\nlow-frequency tokens in generating summaries. Furthermore, to overcome the\nchallenge of token-level retrieval in capturing contextual code semantics, we\nalso propose integrating code semantics into individual summary tokens. The\nresults of extensive experiments and human evaluation show that our token-level\nretrieval-augmented approach significantly improves performance and is more\ninterpretable.", "authors": ["Tong Ye", "Lingfei Wu", "Tengfei Ma", "Xuhong Zhang", "Yangkai Du", "Peiyu Liu", "Shouling Ji", "Wenhai Wang"], "categories": ["cs.AI"], "published": "2023-05-18T16:02:04+00:00", "updated": "2024-03-30T10:45:22+00:00", "pdf_url": "http://arxiv.org/pdf/2305.11074v3", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2305.07594v1", "title": "Opti Code Pro: A Heuristic Search-based Approach to Code Refactoring", "abstract": "This paper presents an approach that evaluates best-first search methods to\ncode refactoring. The motivation for code refactoring could be to improve the\ndesign, structure, or implementation of an existing program without changing\nits functionality. To solve a very specific problem of coupling and cohesion,\nwe propose using heuristic search-based techniques on an approximation of the\nfull code refactoring problem, to guide the refactoring process toward\nsolutions that have high cohesion and low coupling. We evaluated our approach\nby providing demonstrative examples of the effectiveness of this approach on\nrandom state problems and created a tool to implement the algorithm on Java\nprojects.", "authors": ["Sourena Khanzadeh", "Samad Alias Nyein Chan", "Richard Valenzano", "Manar Alalfi"], "categories": ["cs.SE", "cs.AI"], "published": "2023-05-12T16:39:38+00:00", "updated": "2023-05-12T16:39:38+00:00", "pdf_url": "http://arxiv.org/pdf/2305.07594v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2305.05959v2", "title": "Survey of Code Search Based on Deep Learning", "abstract": "Code writing is repetitive and predictable, inspiring us to develop various\ncode intelligence techniques. This survey focuses on code search, that is, to\nretrieve code that matches a given query by effectively capturing the semantic\nsimilarity between the query and code. Deep learning, being able to extract\ncomplex semantics information, has achieved great success in this field.\nRecently, various deep learning methods, such as graph neural networks and\npretraining models, have been applied to code search with significant progress.\nDeep learning is now the leading paradigm for code search. In this survey, we\nprovide a comprehensive overview of deep learning-based code search. We review\nthe existing deep learning-based code search framework which maps query/code to\nvectors and measures their similarity. Furthermore, we propose a new taxonomy\nto illustrate the state-of-the-art deep learning-based code search in a\nthree-steps process: query semantics modeling, code semantics modeling, and\nmatching modeling which involves the deep learning model training. Finally, we\nsuggest potential avenues for future research in this promising field.", "authors": ["Yutao Xie", "Jiayi Lin", "Hande Dong", "Lei Zhang", "Zhonghai Wu"], "categories": ["cs.SE", "cs.PL"], "published": "2023-05-10T08:07:04+00:00", "updated": "2023-12-13T03:50:42+00:00", "pdf_url": "http://arxiv.org/pdf/2305.05959v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2305.05896v3", "title": "A Black-Box Attack on Code Models via Representation Nearest Neighbor Search", "abstract": "Existing methods for generating adversarial code examples face several\nchallenges: limted availability of substitute variables, high verification\ncosts for these substitutes, and the creation of adversarial samples with\nnoticeable perturbations. To address these concerns, our proposed approach,\nRNNS, uses a search seed based on historical attacks to find potential\nadversarial substitutes. Rather than directly using the discrete substitutes,\nthey are mapped to a continuous vector space using a pre-trained variable name\nencoder. Based on the vector representation, RNNS predicts and selects better\nsubstitutes for attacks. We evaluated the performance of RNNS across six coding\ntasks encompassing three programming languages: Java, Python, and C. We\nemployed three pre-trained code models (CodeBERT, GraphCodeBERT, and CodeT5)\nthat resulted in a cumulative of 18 victim models. The results demonstrate that\nRNNS outperforms baselines in terms of ASR and QT. Furthermore, the\nperturbation of adversarial examples introduced by RNNS is smaller compared to\nthe baselines in terms of the number of replaced variables and the change in\nvariable length. Lastly, our experiments indicate that RNNS is efficient in\nattacking defended models and can be employed for adversarial training.", "authors": ["Jie Zhang", "Wei Ma", "Qiang Hu", "Shangqing Liu", "Xiaofei Xie", "Yves Le Traon", "Yang Liu"], "categories": ["cs.CR", "cs.AI", "cs.SE"], "published": "2023-05-10T04:58:39+00:00", "updated": "2023-10-18T18:01:27+00:00", "pdf_url": "http://arxiv.org/pdf/2305.05896v3", "primary_category": "cs.CR"}
{"id": "http://arxiv.org/abs/2305.05503v1", "title": "BadCS: A Backdoor Attack Framework for Code search", "abstract": "With the development of deep learning (DL), DL-based code search models have\nachieved state-of-the-art performance and have been widely used by developers\nduring software development. However, the security issue, e.g., recommending\nvulnerable code, has not received sufficient attention, which will bring\npotential harm to software development. Poisoning-based backdoor attack has\nproven effective in attacking DL-based models by injecting poisoned samples\ninto training datasets. However, previous work shows that the attack technique\ndoes not perform successfully on all DL-based code search models and tends to\nfail for Transformer-based models, especially pretrained models. Besides, the\ninfected models generally perform worse than benign models, which makes the\nattack not stealthy enough and thereby hinders the adoption by developers. To\ntackle the two issues, we propose a novel Backdoor attack framework for Code\nSearch models, named BadCS. BadCS mainly contains two components, including\npoisoned sample generation and re-weighted knowledge distillation. The poisoned\nsample generation component aims at providing selected poisoned samples. The\nre-weighted knowledge distillation component preserves the model effectiveness\nby knowledge distillation and further improves the attack by assigning more\nweights to poisoned samples. Experiments on four popular DL-based models and\ntwo benchmark datasets demonstrate that the existing code search systems are\neasily attacked by BadCS. For example, BadCS improves the state-of-the-art\npoisoning-based method by 83.03%-99.98% and 75.98%-99.90% on Python and Java\ndatasets, respectively. Meanwhile, BadCS also achieves a relatively better\nperformance than benign models, increasing the baseline models by 0.49% and\n0.46% on average, respectively.", "authors": ["Shiyi Qi", "Yuanhang Yang", "Shuzhzeng Gao", "Cuiyun Gao", "Zenglin Xu"], "categories": ["cs.SE"], "published": "2023-05-09T14:52:38+00:00", "updated": "2023-05-09T14:52:38+00:00", "pdf_url": "http://arxiv.org/pdf/2305.05503v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2305.05295v2", "title": "Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data", "abstract": "Transferring information retrieval (IR) models from a high-resource language\n(typically English) to other languages in a zero-shot fashion has become a\nwidely adopted approach. In this work, we show that the effectiveness of\nzero-shot rankers diminishes when queries and documents are present in\ndifferent languages. Motivated by this, we propose to train ranking models on\nartificially code-switched data instead, which we generate by utilizing\nbilingual lexicons. To this end, we experiment with lexicons induced from (1)\ncross-lingual word embeddings and (2) parallel Wikipedia page titles. We use\nthe mMARCO dataset to extensively evaluate reranking models on 36 language\npairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual\nIR (MLIR). Our results show that code-switching can yield consistent and\nsubstantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while\nmaintaining stable performance in MoIR. Encouragingly, the gains are especially\npronounced for distant languages (up to 2x absolute gain). We further show that\nour approach is robust towards the ratio of code-switched tokens and also\nextends to unseen languages. Our results demonstrate that training on\ncode-switched data is a cheap and effective way of generalizing zero-shot\nrankers for cross-lingual and multilingual retrieval.", "authors": ["Robert Litschko", "Ekaterina Artemova", "Barbara Plank"], "categories": ["cs.CL", "cs.IR"], "published": "2023-05-09T09:32:19+00:00", "updated": "2023-05-26T13:16:42+00:00", "pdf_url": "http://arxiv.org/pdf/2305.05295v2", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2305.04508v2", "title": "Improving Code Search with Hard Negative Sampling Based on Fine-tuning", "abstract": "Pre-trained code models have emerged as the state-of-the-art paradigm for\ncode search tasks. The paradigm involves pre-training the model on\nsearch-irrelevant tasks such as masked language modeling, followed by the\nfine-tuning stage, which focuses on the search-relevant task. The typical\nfine-tuning method is to employ a dual-encoder architecture to encode semantic\nembeddings of query and code separately, and then calculate their similarity\nbased on the embeddings. However, the typical dual-encoder architecture falls\nshort in modeling token-level interactions between query and code, which limits\nthe capabilities of model. To address this limitation, we introduce a\ncross-encoder architecture for code search that jointly encodes the\nconcatenation of query and code. We further introduce a Retriever-Ranker (RR)\nframework that cascades the dual-encoder and cross-encoder to promote the\nefficiency of evaluation and online serving. Moreover, we present a\nranking-based hard negative sampling (PS) method to improve the ability of\ncross-encoder to distinguish hard negative codes, which further enhances the\ncascaded RR framework. Experiments on four datasets using three code models\ndemonstrate the superiority of our proposed method. We have made the code\navailable at https://github.com/DongHande/R2PS.", "authors": ["Hande Dong", "Jiayi Lin", "Yanlin Wang", "Yichong Leng", "Jiawei Chen", "Yutao Xie"], "categories": ["cs.SE"], "published": "2023-05-08T07:04:28+00:00", "updated": "2024-11-22T09:34:34+00:00", "pdf_url": "http://arxiv.org/pdf/2305.04508v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2305.04316v2", "title": "Synthesizing Conjunctive Queries for Code Search", "abstract": "This paper presents Squid, a new conjunctive query synthesis algorithm for\nsearching code with target patterns. Given positive and negative examples along\nwith a natural language description, Squid analyzes the relations derived from\nthe examples by a Datalog-based program analyzer and synthesizes a conjunctive\nquery expressing the search intent. The synthesized query can be further used\nto search for desired grammatical constructs in the editor. To achieve high\nefficiency, we prune the huge search space by removing unnecessary relations\nand enumerating query candidates via refinement. We also introduce two\nquantitative metrics for query prioritization to select the queries from\nmultiple candidates, yielding desired queries for code search. We have\nevaluated Squid on over thirty code search tasks. It is shown that Squid\nsuccessfully synthesizes the conjunctive queries for all the tasks, taking only\n2.56 seconds on average.", "authors": ["Chengpeng Wang", "Peisen Yao", "Wensheng Tang", "Gang Fan", "Charles Zhang"], "categories": ["cs.PL", "cs.SE"], "published": "2023-05-07T15:54:10+00:00", "updated": "2023-05-11T15:02:19+00:00", "pdf_url": "http://arxiv.org/pdf/2305.04316v2", "primary_category": "cs.PL"}
{"id": "http://arxiv.org/abs/2305.04032v5", "title": "ToolCoder: Teach Code Generation Models to use API search tools", "abstract": "Automatically generating source code from natural language descriptions has\nbeen a growing field of research in recent years. However, current large-scale\ncode generation models often encounter difficulties when selecting appropriate\nAPIs for specific contexts. These models may generate APIs that do not meet\nrequirements or refer to non-existent APIs in third-party libraries, especially\nfor lesser-known or private libraries. Inspired by the process of human\ndevelopers using tools to search APIs, we propose ToolCoder, a novel approach\nthat integrates API search tools with existing models to assist in code\ngeneration and API selection. To teach our model to use tools, we introduce an\nautomated data annotation method using ChatGPT to add tool usage information\ninto the source code data and fine-tune code generation models. During\ninference, we integrate API search tools into the generation process so that\nour model can automatically use the search tool to get suggestions when\nselecting an API. Our experimental results demonstrate that ToolCoder exhibits\nexcellent performance and generalization across five public and private library\ncode generation benchmarks, with at least 6.21\\% improvement on average pass@1\nmetrics and 9.64\\% improvement on average pass@10 metrics compared to\nstate-of-the-art methods. Furthermore, we show that our relatively small\nToolCoder model is comparable to one of the current best models, GPT-3.5,\nhighlighting the potential of incorporating programming tools into the code\ngeneration process.", "authors": ["Kechi Zhang", "Huangzhao Zhang", "Ge Li", "Jia Li", "Zhuo Li", "Zhi Jin"], "categories": ["cs.SE"], "published": "2023-05-06T12:45:28+00:00", "updated": "2023-09-11T06:33:46+00:00", "pdf_url": "http://arxiv.org/pdf/2305.04032v5", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2305.03843v2", "title": "REINFOREST: Reinforcing Semantic Code Similarity for Cross-Lingual Code Search Models", "abstract": "This paper introduces a novel code-to-code search technique that enhances the\nperformance of Large Language Models (LLMs) by including both static and\ndynamic features as well as utilizing both similar and dissimilar examples\nduring training. We present the first-ever code search method that encodes\ndynamic runtime information during training without the need to execute either\nthe corpus under search or the search query at inference time and the first\ncode search technique that trains on both positive and negative reference\nsamples. To validate the efficacy of our approach, we perform a set of studies\ndemonstrating the capability of enhanced LLMs to perform cross-language\ncode-to-code search. Our evaluation demonstrates that the effectiveness of our\napproach is consistent across various model architectures and programming\nlanguages. We outperform the state-of-the-art cross-language search tool by up\nto 44.7\\%. Moreover, our ablation studies reveal that even a single positive\nand negative reference sample in the training process results in substantial\nperformance improvements demonstrating both similar and dissimilar references\nare important parts of code search. Importantly, we show that enhanced\nwell-crafted, fine-tuned models consistently outperform enhanced larger modern\nLLMs without fine tuning, even when enhancing the largest available LLMs\nhighlighting the importance for open-sourced models. To ensure the\nreproducibility and extensibility of our research, we present an open-sourced\nimplementation of our tool and training procedures called REINFOREST.", "authors": ["Anthony Saieva", "Saikat Chakraborty", "Gail Kaiser"], "categories": ["cs.SE", "cs.AI", "cs.PL"], "published": "2023-05-05T20:46:56+00:00", "updated": "2024-04-15T18:24:40+00:00", "pdf_url": "http://arxiv.org/pdf/2305.03843v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2305.00188v4", "title": "New Characterizations and Efficient Local Search for General Integer Linear Programming", "abstract": "Integer linear programming (ILP) models a wide range of practical\ncombinatorial optimization problems and significantly impacts industry and\nmanagement sectors. This work proposes new characterizations of ILP with the\nconcept of boundary solutions. Motivated by the new characterizations, we\ndevelop a new local search algorithm Local-ILP, which is efficient for solving\ngeneral ILP validated on a large heterogeneous problem dataset. We propose a\nnew local search framework that switches between three modes, namely Search,\nImprove, and Restore modes. Two new operators are proposed, namely the tight\nmove and the lift move operators, which are associated with appropriate scoring\nfunctions. Different modes apply different operators to realize different\nsearch strategies and the algorithm switches between three modes according to\nthe current search state. Putting these together, we develop a local search ILP\nsolver called Local-ILP. Experiments conducted on the MIPLIB dataset show the\neffectiveness of our algorithm in solving large-scale hard ILP problems. In the\naspect of finding a good feasible solution quickly, Local-ILP is competitive\nand complementary to the state-of-the-art commercial solver Gurobi and\nsignificantly outperforms the state-of-the-art non-commercial solver SCIP.\nMoreover, our algorithm establishes new records for 6 MIPLIB open instances.\nThe theoretical analysis of our algorithm is also presented, which shows our\nalgorithm could avoid visiting unnecessary regions.", "authors": ["Peng Lin", "Shaowei Cai", "Mengchuan Zou", "Jinkun Lin"], "categories": ["math.OC", "cs.AI", "90C10 (Primary), 90C06 (Secondary)", "I.2.8; G.2.0"], "published": "2023-04-29T07:22:07+00:00", "updated": "2024-03-01T05:56:59+00:00", "pdf_url": "http://arxiv.org/pdf/2305.00188v4", "primary_category": "math.OC"}
{"id": "http://arxiv.org/abs/2304.11473v2", "title": "(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis", "abstract": "As ecommerce continues growing, huge investments in ML and NLP for\nInformation Retrieval are following. While the vector space model dominated\nretrieval modelling in product search - even as vectorization itself greatly\nchanged with the advent of deep learning -, our position paper argues in a\ncontrarian fashion that program synthesis provides significant advantages for\nmany queries and a significant number of players in the market. We detail the\nindustry significance of the proposed approach, sketch implementation details,\nand address common objections drawing from our experience building a similar\nsystem at Tooso.", "authors": ["Jacopo Tagliabue", "Ciro Greco"], "categories": ["cs.IR", "cs.CL", "cs.LG"], "published": "2023-04-22T20:00:06+00:00", "updated": "2023-06-11T17:35:02+00:00", "pdf_url": "http://arxiv.org/pdf/2304.11473v2", "primary_category": "cs.IR"}
{"id": "http://arxiv.org/abs/2303.15822v1", "title": "One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization", "abstract": "As pre-trained models automate many code intelligence tasks, a widely used\nparadigm is to fine-tune a model on the task dataset for each programming\nlanguage. A recent study reported that multilingual fine-tuning benefits a\nrange of tasks and models. However, we find that multilingual fine-tuning leads\nto performance degradation on recent models UniXcoder and CodeT5.\n  To alleviate the potentially catastrophic forgetting issue in multilingual\nmodels, we fix all pre-trained model parameters, insert the parameter-efficient\nstructure adapter, and fine-tune it. Updating only 0.6\\% of the overall\nparameters compared to full-model fine-tuning for each programming language,\nadapter tuning yields consistent improvements on code search and summarization\ntasks, achieving state-of-the-art results. In addition, we experimentally show\nits effectiveness in cross-lingual and low-resource scenarios. Multilingual\nfine-tuning with 200 samples per programming language approaches the results\nfine-tuned with the entire dataset on code summarization. Our experiments on\nthree probing tasks show that adapter tuning significantly outperforms\nfull-model fine-tuning and effectively overcomes catastrophic forgetting.", "authors": ["Deze Wang", "Boxing Chen", "Shanshan Li", "Wei Luo", "Shaoliang Peng", "Wei Dong", "Xiangke Liao"], "categories": ["cs.SE", "cs.AI"], "published": "2023-03-28T08:49:54+00:00", "updated": "2023-03-28T08:49:54+00:00", "pdf_url": "http://arxiv.org/pdf/2303.15822v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2303.12570v3", "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation", "abstract": "The task of repository-level code completion is to continue writing the\nunfinished code based on a broader context of the repository. While for\nautomated code completion tools, it is difficult to utilize the useful\ninformation scattered in different files. We propose RepoCoder, a simple,\ngeneric, and effective framework to address the challenge. It streamlines the\nrepository-level code completion process by incorporating a similarity-based\nretriever and a pre-trained code language model in an iterative\nretrieval-generation pipeline. RepoCoder makes effective utilization of\nrepository-level information for code completion and has the ability to\ngenerate code at various levels of granularity. Moreover, we propose a new\nbenchmark RepoEval, which consists of the latest and high-quality real-world\nrepositories covering line, API invocation, and function body completion\nscenarios. Experimental results indicate that RepoCoder significantly improves\nthe In-File completion baseline by over 10% in all settings and consistently\noutperforms the vanilla retrieval-augmented code completion approach.\nFurthermore, we validate the effectiveness of RepoCoder through comprehensive\nanalysis, providing valuable insights for future research. Our source code and\nbenchmark are publicly available:\nhttps://github.com/microsoft/CodeT/tree/main/RepoCoder", "authors": ["Fengji Zhang", "Bei Chen", "Yue Zhang", "Jacky Keung", "Jin Liu", "Daoguang Zan", "Yi Mao", "Jian-Guang Lou", "Weizhu Chen"], "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "published": "2023-03-22T13:54:46+00:00", "updated": "2023-10-20T15:21:51+00:00", "pdf_url": "http://arxiv.org/pdf/2303.12570v3", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2303.07166v1", "title": "Improved Tree Search for Automatic Program Synthesis", "abstract": "In the task of automatic program synthesis, one obtains pairs of matching\ninputs and outputs and generates a computer program, in a particular\ndomain-specific language (DSL), which given each sample input returns the\nmatching output. A key element is being able to perform an efficient search in\nthe space of valid programs. Here, we suggest a variant of MCTS that leads to\nstate of the art results on two vastly different DSLs. The exploration method\nwe propose includes multiple contributions: a modified visit count, a\npreprocessing procedure for the training dataset, and encoding the part of the\nprogram that was already executed.", "authors": ["Aran Carmon", "Lior Wolf"], "categories": ["cs.LG", "cs.PL", "cs.SE"], "published": "2023-03-13T15:09:52+00:00", "updated": "2023-03-13T15:09:52+00:00", "pdf_url": "http://arxiv.org/pdf/2303.07166v1", "primary_category": "cs.LG"}
{"id": "http://arxiv.org/abs/2303.03004v4", "title": "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval", "abstract": "Recently, pre-trained large language models (LLMs) have shown impressive\nabilities in generating codes from natural language descriptions, repairing\nbuggy codes, translating codes between languages, and retrieving relevant code\nsegments. However, the evaluation of these models has often been performed in a\nscattered way on only one or two specific tasks, in a few languages, at a\npartial granularity (e.g., function) level, and in many cases without proper\ntraining data. Even more concerning is that in most cases the evaluation of\ngenerated codes has been done in terms of mere lexical overlap with a reference\ncode rather than actual execution. We introduce xCodeEval, the largest\nexecutable multilingual multitask benchmark to date consisting of $25$M\ndocument-level coding examples ($16.5$B tokens) from about $7.5$K unique\nproblems covering up to $11$ programming languages with execution-level\nparallelism. It features a total of $7$ tasks involving code understanding,\ngeneration, translation and retrieval. xCodeEval adopts an execution-based\nevaluation and offers a multilingual code execution engine, ExecEval that\nsupports unit test based execution in all the $11$ languages. To address the\nchallenge of balancing the distributions of text-code samples over multiple\nattributes in validation/test sets, we propose a novel data splitting and a\ndata selection schema based on the geometric mean and graph-theoretic\nprinciple. Our experiments with OpenAI's LLMs (zero-shot) and open-LLMs\n(zero-shot and fine-tuned) on the tasks and languages demonstrate **xCodeEval**\nto be quite challenging as per the current advancements in language models.", "authors": ["Mohammad Abdullah Matin Khan", "M Saiful Bari", "Xuan Long Do", "Weishi Wang", "Md Rizwan Parvez", "Shafiq Joty"], "categories": ["cs.CL"], "published": "2023-03-06T10:08:51+00:00", "updated": "2023-11-06T07:16:58+00:00", "pdf_url": "http://arxiv.org/pdf/2303.03004v4", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2302.14838v3", "title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search", "abstract": "Given the recent impressive accomplishments of language models (LMs) for code\ngeneration, we explore the use of LMs as adaptive mutation and crossover\noperators for an evolutionary neural architecture search (NAS) algorithm. While\nNAS still proves too difficult a task for LMs to succeed at solely through\nprompting, we find that the combination of evolutionary prompt engineering with\nsoft prompt-tuning, a method we term EvoPrompting, consistently finds diverse\nand high performing models. We first demonstrate that EvoPrompting is effective\non the computationally efficient MNIST-1D dataset, where EvoPrompting produces\nconvolutional architecture variants that outperform both those designed by\nhuman experts and naive few-shot prompting in terms of accuracy and model size.\nWe then apply our method to searching for graph neural networks on the CLRS\nAlgorithmic Reasoning Benchmark, where EvoPrompting is able to design novel\narchitectures that outperform current state-of-the-art models on 21 out of 30\nalgorithmic reasoning tasks while maintaining similar model size. EvoPrompting\nis successful at designing accurate and efficient neural network architectures\nacross a variety of machine learning tasks, while also being general enough for\neasy adaptation to other tasks beyond neural network design.", "authors": ["Angelica Chen", "David M. Dohan", "David R. So"], "categories": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "published": "2023-02-28T18:37:25+00:00", "updated": "2023-11-16T18:02:19+00:00", "pdf_url": "http://arxiv.org/pdf/2302.14838v3", "primary_category": "cs.NE"}
{"id": "http://arxiv.org/abs/2302.05226v1", "title": "Shrinking the Inductive Programming Search Space with Instruction Subsets", "abstract": "Inductive programming frequently relies on some form of search in order to\nidentify candidate solutions. However, the size of the search space limits the\nuse of inductive programming to the production of relatively small programs. If\nwe could somehow correctly predict the subset of instructions required for a\ngiven problem then inductive programming would be more tractable. We will show\nthat this can be achieved in a high percentage of cases. This paper presents a\nnovel model of programming language instruction co-occurrence that was built to\nsupport search space partitioning in the Zoea distributed inductive programming\nsystem. This consists of a collection of intersecting instruction subsets\nderived from a large sample of open source code. Using the approach different\nparts of the search space can be explored in parallel. The number of subsets\nrequired does not grow linearly with the quantity of code used to produce them\nand a manageable number of subsets is sufficient to cover a high percentage of\nunseen code. This approach also significantly reduces the overall size of the\nsearch space - often by many orders of magnitude.", "authors": ["Edward McDaid", "Sarah McDaid"], "categories": ["cs.AI", "cs.PL", "I.2.2; I.2.5; I.2.8; I.2.11; F.3.1; D.3.1"], "published": "2023-02-10T12:51:35+00:00", "updated": "2023-02-10T12:51:35+00:00", "pdf_url": "http://arxiv.org/pdf/2302.05226v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2302.01578v1", "title": "Searching Large Neighborhoods for Integer Linear Programs with Contrastive Learning", "abstract": "Integer Linear Programs (ILPs) are powerful tools for modeling and solving a\nlarge number of combinatorial optimization problems. Recently, it has been\nshown that Large Neighborhood Search (LNS), as a heuristic algorithm, can find\nhigh quality solutions to ILPs faster than Branch and Bound. However, how to\nfind the right heuristics to maximize the performance of LNS remains an open\nproblem. In this paper, we propose a novel approach, CL-LNS, that delivers\nstate-of-the-art anytime performance on several ILP benchmarks measured by\nmetrics including the primal gap, the primal integral, survival rates and the\nbest performing rate. Specifically, CL-LNS collects positive and negative\nsolution samples from an expert heuristic that is slow to compute and learns a\nnew one with a contrastive loss. We use graph attention networks and a richer\nset of features to further improve its performance.", "authors": ["Taoan Huang", "Aaron Ferber", "Yuandong Tian", "Bistra Dilkina", "Benoit Steiner"], "categories": ["cs.AI"], "published": "2023-02-03T07:15:37+00:00", "updated": "2023-02-03T07:15:37+00:00", "pdf_url": "http://arxiv.org/pdf/2302.01578v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2212.10692v1", "title": "Generation-Augmented Query Expansion For Code Retrieval", "abstract": "Pre-trained language models have achieved promising success in code retrieval\ntasks, where a natural language documentation query is given to find the most\nrelevant existing code snippet. However, existing models focus only on\noptimizing the documentation code pairs by embedding them into latent space,\nwithout the association of external knowledge. In this paper, we propose a\ngeneration-augmented query expansion framework. Inspired by the human retrieval\nprocess - sketching an answer before searching, in this work, we utilize the\npowerful code generation model to benefit the code retrieval task.\nSpecifically, we demonstrate that rather than merely retrieving the target code\nsnippet according to the documentation query, it would be helpful to augment\nthe documentation query with its generation counterpart - generated code\nsnippets from the code generation model. To the best of our knowledge, this is\nthe first attempt that leverages the code generation model to enhance the code\nretrieval task. We achieve new state-of-the-art results on the CodeSearchNet\nbenchmark and surpass the baselines significantly.", "authors": ["Dong Li", "Yelong Shen", "Ruoming Jin", "Yi Mao", "Kuan Wang", "Weizhu Chen"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2022-12-20T23:49:37+00:00", "updated": "2022-12-20T23:49:37+00:00", "pdf_url": "http://arxiv.org/pdf/2212.10692v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2212.08221v1", "title": "SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval", "abstract": "Pre-trained giant code models (PCMs) start coming into the developers' daily\npractices. Understanding what types of and how much software knowledge is\npacked into PCMs is the foundation for incorporating PCMs into software\nengineering (SE) tasks and fully releasing their potential. In this work, we\nconduct the first systematic study on the SE factual knowledge in the\nstate-of-the-art PCM CoPilot, focusing on APIs' Fully Qualified Names (FQNs),\nthe fundamental knowledge for effective code analysis, search and reuse. Driven\nby FQNs' data distribution properties, we design a novel lightweight in-context\nlearning on Copilot for FQN inference, which does not require code compilation\nas traditional methods or gradient update by recent FQN prompt-tuning. We\nsystematically experiment with five in-context-learning design factors to\nidentify the best in-context learning configuration that developers can adopt\nin practice. With this best configuration, we investigate the effects of amount\nof example prompts and FQN data properties on Copilot's FQN inference\ncapability. Our results confirm that Copilot stores diverse FQN knowledge and\ncan be applied for the FQN inference due to its high inference accuracy and\nnon-reliance on code analysis. Based on our experience interacting with\nCopilot, we discuss various opportunities to improve human-CoPilot interaction\nin the FQN inference task.", "authors": ["Qing Huang", "Dianshu Liao", "Zhenchang Xing", "Zhiqiang Yuan", "Qinghua Lu", "Xiwei Xu", "Jiaxing Lu"], "categories": ["cs.SE"], "published": "2022-12-16T01:18:49+00:00", "updated": "2022-12-16T01:18:49+00:00", "pdf_url": "http://arxiv.org/pdf/2212.08221v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2212.03459v1", "title": "You Don't Know Search: Helping Users Find Code by Automatically Evaluating Alternative Queries", "abstract": "Tens of thousands of engineers use Sourcegraph day-to-day to search for code\nand rely on it to make progress on software development tasks. We face a key\nchallenge in designing a query language that accommodates the needs of a broad\nspectrum of users. Our experience shows that users express different and often\ncontradictory preferences for how queries should be interpreted. These\npreferences stem from users with differing usage contexts, technical\nexperience, and implicit expectations from using prior tools. At the same time,\ndesigning a code search query language poses unique challenges because it\nintersects traditional search engines and full-fledged programming languages.\nFor example, code search queries adopt certain syntactic conventions in the\ninterest of simplicity and terseness but invariably risk encoding implicit\nsemantics that are ambiguous at face-value (a single space in a query could\nmean three or more semantically different things depending on surrounding\nterms). Users often need to disambiguate intent with additional syntax so that\na query expresses what they actually want to search. This need to disambiguate\nis one of the primary frustrations we've seen users experience with writing\nsearch queries in the last three years. We share our observations that lead us\nto a fresh perspective where code search behavior can straddle seemingly\nambiguous queries. We develop Automated Query Evaluation (AQE), a new technique\nthat automatically generates and adaptively runs alternative query\ninterpretations in frustration-prone conditions. We evaluate AQE with an A/B\ntest across more than 10,000 unique users on our publicly-available code search\ninstance. Our main result shows that relative to the control group, users are\non average 22% more likely to click on a search result at all on any given day\nwhen AQE is active.", "authors": ["Rijnard van Tonder"], "categories": ["cs.SE", "cs.IR", "D.2.3; I.2.5; D.3.2; D.0"], "published": "2022-12-07T04:49:42+00:00", "updated": "2022-12-07T04:49:42+00:00", "pdf_url": "http://arxiv.org/pdf/2212.03459v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2211.14409v2", "title": "Domain-Independent Dynamic Programming: Generic State Space Search for Combinatorial Optimization", "abstract": "For combinatorial optimization problems, model-based approaches such as\nmixed-integer programming (MIP) and constraint programming (CP) aim to decouple\nmodeling and solving a problem: the 'holy grail' of declarative problem\nsolving. We propose domain-independent dynamic programming (DIDP), a new\nmodel-based paradigm based on dynamic programming (DP). While DP is not new, it\nhas typically been implemented as a problem-specific method. We propose Dynamic\nProgramming Description Language (DyPDL), a formalism to define DP models, and\ndevelop Cost-Algebraic A* Solver for DyPDL (CAASDy), a generic solver for DyPDL\nusing state space search. We formalize existing problem-specific DP and state\nspace search methods for combinatorial optimization problems as DP models in\nDyPDL. Using CAASDy and commercial MIP and CP solvers, we experimentally\ncompare the DP models with existing MIP and CP models, showing that, despite\nits nascent nature, CAASDy outperforms MIP and CP on a number of common problem\nclasses.", "authors": ["Ryo Kuroiwa", "J. Christopher Beck"], "categories": ["cs.AI"], "published": "2022-11-26T00:15:45+00:00", "updated": "2023-03-01T19:35:47+00:00", "pdf_url": "http://arxiv.org/pdf/2211.14409v2", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2211.08516v2", "title": "Phenotype Search Trajectory Networks for Linear Genetic Programming", "abstract": "Genotype-to-phenotype mappings translate genotypic variations such as\nmutations into phenotypic changes. Neutrality is the observation that some\nmutations do not lead to phenotypic changes. Studying the search trajectories\nin genotypic and phenotypic spaces, especially through neutral mutations, helps\nus to better understand the progression of evolution and its algorithmic\nbehaviour. In this study, we visualise the search trajectories of a genetic\nprogramming system as graph-based models, where nodes are genotypes/phenotypes\nand edges represent their mutational transitions. We also quantitatively\nmeasure the characteristics of phenotypes including their genotypic abundance\n(the requirement for neutrality) and Kolmogorov complexity. We connect these\nquantified metrics with search trajectory visualisations, and find that more\ncomplex phenotypes are under-represented by fewer genotypes and are harder for\nevolution to discover. Less complex phenotypes, on the other hand, are\nover-represented by genotypes, are easier to find, and frequently serve as\nstepping-stones for evolution.", "authors": ["Ting Hu", "Gabriela Ochoa", "Wolfgang Banzhaf"], "categories": ["q-bio.PE", "cs.AI"], "published": "2022-11-15T21:20:50+00:00", "updated": "2023-06-23T16:42:01+00:00", "pdf_url": "http://arxiv.org/pdf/2211.08516v2", "primary_category": "q-bio.PE"}
{"id": "http://arxiv.org/abs/2210.16269v2", "title": "ATM: Black-box Test Case Minimization based on Test Code Similarity and Evolutionary Search", "abstract": "Executing large test suites is time and resource consuming, sometimes\nimpossible, and such test suites typically contain many redundant test cases.\nHence, test case minimization is used to remove redundant test cases that are\nunlikely to detect new faults. However, most test case (suite) minimization\ntechniques rely on code coverage (white-box), model-based features, or\nrequirements specifications, which are not always accessible by test engineers.\nRecently, a set of novel techniques was proposed, called FAST-R, relying solely\non test case code for test case minimization, which appeared to be much more\nefficient than white-box techniques. However, it achieved a comparable low\nfault detection capability for Java projects, making its application\nchallenging in practice. This paper proposes ATM (AST-based Test case\nMinimizer), a similarity-based, search-based test case minimization technique,\ntaking a specific budget as input, that also relies exclusively on the source\ncode of test cases but attempts to achieve higher fault detection through\nfiner-grained similarity analysis and a dedicated search algorithm. ATM\ntransforms test case code into Abstract Syntax Trees (AST) and relies on four\ntree-based similarity measures to apply evolutionary search, specifically\ngenetic algorithms, to minimize test cases. We evaluated the effectiveness and\nefficiency of ATM on a large dataset of 16 Java projects with 661 faulty\nversions using three budgets ranging from 25% to 75% of test suites. ATM\nachieved significantly higher fault detection rates (0.82 on average), compared\nto FAST-R (0.61 on average) and random minimization (0.52 on average), when\nrunning only 50% of the test cases, within practically acceptable time (1.1-4.3\nhours, on average), given that minimization is only occasionally applied when\nmany new test cases are created (major releases). Results achieved for other\nbudgets were consistent.", "authors": ["Rongqi Pan", "Taher A. Ghaleb", "Lionel Briand"], "categories": ["cs.SE"], "published": "2022-10-28T16:59:13+00:00", "updated": "2022-12-21T01:42:55+00:00", "pdf_url": "http://arxiv.org/pdf/2210.16269v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2210.15845v1", "title": "I Know What You Are Searching For: Code Snippet Recommendation from Stack Overflow Posts", "abstract": "Stack Overflow has been heavily used by software developers to seek\nprogramming-related information. More and more developers use Community\nQuestion and Answer forums, such as Stack Overflow, to search for code examples\nof how to accomplish a certain coding task. This is often considered to be more\nefficient than working from source documentation, tutorials or full worked\nexamples. However, due to the complexity of these online Question and Answer\nforums and the very large volume of information they contain, developers can be\noverwhelmed by the sheer volume of available information. This makes it hard to\nfind and/or even be aware of the most relevant code examples to meet their\nneeds. To alleviate this issue, in this work we present a query-driven code\nrecommendation tool, named Que2Code, that identifies the best code snippets for\na user query from Stack Overflow posts. Our approach has two main stages: (i)\nsemantically-equivalent question retrieval and (ii) best code snippet\nrecommendation. To evaluate the performance of our proposed model, we conduct a\nlarge scale experiment to evaluate the effectiveness of the\nsemantically-equivalent question retrieval task and best code snippet\nrecommendation task separately on Python and Java datasets in Stack Overflow.\nWe also perform a human study to measure how real-world developers perceive the\nresults generated by our model. Both the automatic and human evaluation results\ndemonstrate the promising performance of our model, and we have released our\ncode and data to assist other researchers.", "authors": ["Zhipeng Gao", "Xin Xia", "David Lo", "John Grundy", "Xindong Zhang", "Zhenchang Xing"], "categories": ["cs.SE"], "published": "2022-10-28T02:34:48+00:00", "updated": "2022-10-28T02:34:48+00:00", "pdf_url": "http://arxiv.org/pdf/2210.15845v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2210.12285v1", "title": "Exploring Representation-Level Augmentation for Code Search", "abstract": "Code search, which aims at retrieving the most relevant code fragment for a\ngiven natural language query, is a common activity in software development\npractice. Recently, contrastive learning is widely used in code search\nresearch, where many data augmentation approaches for source code (e.g.,\nsemantic-preserving program transformation) are proposed to learn better\nrepresentations. However, these augmentations are at the raw-data level, which\nrequires additional code analysis in the preprocessing stage and additional\ntraining costs in the training stage. In this paper, we explore augmentation\nmethods that augment data (both code and query) at representation level which\ndoes not require additional data processing and training, and based on this we\npropose a general format of representation-level augmentation that unifies\nexisting methods. Then, we propose three new augmentation methods (linear\nextrapolation, binary interpolation, and Gaussian scaling) based on the general\nformat. Furthermore, we theoretically analyze the advantages of the proposed\naugmentation methods over traditional contrastive learning methods on code\nsearch. We experimentally evaluate the proposed representation-level\naugmentation methods with state-of-the-art code search models on a large-scale\npublic dataset consisting of six programming languages. The experimental\nresults show that our approach can consistently boost the performance of the\nstudied code search models. Our source code is available at\nhttps://github.com/Alex-HaochenLi/RACS.", "authors": ["Haochen Li", "Chunyan Miao", "Cyril Leung", "Yanxian Huang", "Yuan Huang", "Hongyu Zhang", "Yanlin Wang"], "categories": ["cs.SE", "cs.IR", "cs.LG"], "published": "2022-10-21T22:47:37+00:00", "updated": "2022-10-21T22:47:37+00:00", "pdf_url": "http://arxiv.org/pdf/2210.12285v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2210.00328v1", "title": "CodeDSI: Differentiable Code Search", "abstract": "Reimplementing solutions to previously solved software engineering problems\nis not only inefficient but also introduces inadequate and error-prone code.\nMany existing methods achieve impressive performance on this issue by using\nautoregressive text-generation models trained on code. However, these methods\nare not without their flaws. The generated code from these models can be buggy,\nlack documentation, and introduce vulnerabilities that may go unnoticed by\ndevelopers. An alternative to code generation -- neural code search -- is a\nfield of machine learning where a model takes natural language queries as input\nand, in turn, relevant code samples from a database are returned. Due to the\nnature of this pre-existing database, code samples can be documented, tested,\nlicensed, and checked for vulnerabilities before being used by developers in\nproduction. In this work, we present CodeDSI, an end-to-end unified approach to\ncode search. CodeDSI is trained to directly map natural language queries to\ntheir respective code samples, which can be retrieved later. In an effort to\nimprove the performance of code search, we have investigated docid\nrepresentation strategies, impact of tokenization on docid structure, and\ndataset sizes on overall code search performance. Our results demonstrate\nCodeDSI strong performance, exceeding conventional robust baselines by 2-6%\nacross varying dataset sizes.", "authors": ["Usama Nadeem", "Noah Ziems", "Shaoen Wu"], "categories": ["cs.SE", "cs.IR"], "published": "2022-10-01T17:39:57+00:00", "updated": "2022-10-01T17:39:57+00:00", "pdf_url": "http://arxiv.org/pdf/2210.00328v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2209.09804v1", "title": "Assisted Specification of Code Using Search", "abstract": "We describe an intelligent assistant based on mining existing software\nrepositories to help the developer interactively create checkable\nspecifications of code. To be most useful we apply this at the subsystem level,\nthat is chunks of code of 1000-10000 lines that can be standalone or integrated\ninto an existing application to provide additional functionality or\ncapabilities. The resultant specifications include both a syntactic description\nof what should be written and a semantic specification of what it should do,\ninitially in the form of test cases. The generated specification is designed to\nbe used for automatic code generation using various technologies that have been\nproposed including machine learning, code search, and program synthesis. Our\nresearch goal is to enable these technologies to be used effectively for\ncreating subsystems without requiring the developer to write detailed\nspecifications from scratch.", "authors": ["Steven P. Reiss"], "categories": ["cs.SE", "ACM-class: D.2.2"], "published": "2022-09-20T15:49:35+00:00", "updated": "2022-09-20T15:49:35+00:00", "pdf_url": "http://arxiv.org/pdf/2209.09804v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2208.11274v3", "title": "Revisiting Code Search in a Two-Stage Paradigm", "abstract": "With a good code search engine, developers can reuse existing code snippets\nand accelerate software development process. Current code search methods can be\ndivided into two categories: traditional information retrieval (IR) based and\ndeep learning (DL) based approaches. DL-based approaches include the\ncross-encoder paradigm and the bi-encoder paradigm. However, both approaches\nhave certain limitations. The inference of IR-based and bi-encoder models are\nfast, however, they are not accurate enough; while cross-encoder models can\nachieve higher search accuracy but consume more time. In this work, we propose\nTOSS, a two-stage fusion code search framework that can combine the advantages\nof different code search methods. TOSS first uses IR-based and bi-encoder\nmodels to efficiently recall a small number of top-k code candidates, and then\nuses fine-grained cross-encoders for finer ranking. Furthermore, we conduct\nextensive experiments on different code candidate volumes and multiple\nprogramming languages to verify the effectiveness of TOSS. We also compare TOSS\nwith six data fusion methods. Experimental results show that TOSS is not only\nefficient, but also achieves state-of-the-art accuracy with an overall mean\nreciprocal ranking (MRR) score of 0.763, compared to the best baseline result\non the CodeSearchNet benchmark of 0.713. Our source code and experimental data\nare available at: https://github.com/fly-dragon211/TOSS.", "authors": ["Fan Hu", "Yanlin Wang", "Lun Du", "Xirong Li", "Hongyu Zhang", "Shi Han", "Dongmei Zhang"], "categories": ["cs.SE"], "published": "2022-08-24T02:34:27+00:00", "updated": "2024-03-28T03:51:21+00:00", "pdf_url": "http://arxiv.org/pdf/2208.11274v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2208.11271v3", "title": "Tackling Long Code Search with Splitting, Encoding, and Aggregating", "abstract": "Code search with natural language helps us reuse existing code snippets.\nThanks to the Transformer-based pretraining models, the performance of code\nsearch has been improved significantly. However, due to the quadratic\ncomplexity of multi-head self-attention, there is a limit on the input token\nlength. For efficient training on standard GPUs like V100, existing pretrained\ncode models, including GraphCodeBERT, CodeBERT, RoBERTa (code), take the first\n256 tokens by default, which makes them unable to represent the complete\ninformation of long code that is greater than 256 tokens. To tackle the long\ncode problem, we propose a new baseline SEA (Split, Encode and Aggregate),\nwhich splits long code into code blocks, encodes these blocks into embeddings,\nand aggregates them to obtain a comprehensive long code representation. With\nSEA, we could directly use Transformer-based pretraining models to model long\ncode without changing their internal structure and re-pretraining. We also\ncompare SEA with sparse Trasnformer methods. With GraphCodeBERT as the encoder,\nSEA achieves an overall mean reciprocal ranking score of 0.785, which is 10.1%\nhigher than GraphCodeBERT on the CodeSearchNet benchmark, justifying SEA as a\nstrong baseline for long code search. Our source code and experimental data are\navailable at: https://github.com/fly-dragon211/SEA.", "authors": ["Fan Hu", "Yanlin Wang", "Lun Du", "Hongyu Zhang", "Shi Han", "Dongmei Zhang", "Xirong Li"], "categories": ["cs.SE"], "published": "2022-08-24T02:27:30+00:00", "updated": "2024-03-26T14:51:38+00:00", "pdf_url": "http://arxiv.org/pdf/2208.11271v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2208.03922v1", "title": "CSSAM:Code Search via Attention Matching of Code Semantics and Structures", "abstract": "Despite the continuous efforts in improving both the effectiveness and\nefficiency of code search, two issues remained unsolved. First, programming\nlanguages have inherent strong structural linkages, and feature mining of code\nas text form would omit the structural information contained inside it. Second,\nthere is a potential semantic relationship between code and query, it is\nchallenging to align code and text across sequences so that vectors are\nspatially consistent during similarity matching. To tackle both issues, in this\npaper, a code search model named CSSAM (Code Semantics and Structures Attention\nMatching) is proposed. By introducing semantic and structural matching\nmechanisms, CSSAM effectively extracts and fuses multidimensional code\nfeatures. Specifically, the cross and residual layer was developed to\nfacilitate high-latitude spatial alignment of code and query at the token\nlevel. By leveraging the residual interaction, a matching module is designed to\npreserve more code semantics and descriptive features, that enhances the\nadhesion between the code and its corresponding query text. Besides, to improve\nthe model's comprehension of the code's inherent structure, a code\nrepresentation structure named CSRG (Code Semantic Representation Graph) is\nproposed for jointly representing abstract syntax tree nodes and the data flow\nof the codes. According to the experimental results on two publicly available\ndatasets containing 540k and 330k code segments, CSSAM significantly\noutperforms the baselines in terms of achieving the highest SR@1/5/10, MRR, and\nNDCG@50 on both datasets respectively. Moreover, the ablation study is\nconducted to quantitatively measure the impact of each key component of CSSAM\non the efficiency and effectiveness of code search, which offers the insights\ninto the improvement of advanced code search solutions.", "authors": ["Yi Hu", "Bo Cai", "Yaoxiang Yu"], "categories": ["cs.SE", "cs.AI", "cs.IR"], "published": "2022-08-08T05:45:40+00:00", "updated": "2022-08-08T05:45:40+00:00", "pdf_url": "http://arxiv.org/pdf/2208.03922v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2208.03713v1", "title": "Study of Encoder-Decoder Architectures for Code-Mix Search Query Translation", "abstract": "With the broad reach of the internet and smartphones, e-commerce platforms\nhave an increasingly diversified user base. Since native language users are not\nconversant in English, their preferred browsing mode is their regional language\nor a combination of their regional language and English. From our recent study\non the query data, we noticed that many of the queries we receive are code-mix,\nspecifically Hinglish i.e. queries with one or more Hindi words written in\nEnglish (Latin) script. We propose a transformer-based approach for code-mix\nquery translation to enable users to search with these queries. We demonstrate\nthe effectiveness of pre-trained encoder-decoder models trained on a large\ncorpus of the unlabeled English text for this task. Using generic domain\ntranslation models, we created a pseudo-labelled dataset for training the model\non the search queries and verified the effectiveness of various data\naugmentation techniques. Further, to reduce the latency of the model, we use\nknowledge distillation and weight quantization. Effectiveness of the proposed\nmethod has been validated through experimental evaluations and A/B testing. The\nmodel is currently live on Flipkart app and website, serving millions of\nqueries.", "authors": ["Mandar Kulkarni", "Soumya Chennabasavaraj", "Nikesh Garera"], "categories": ["cs.CL"], "published": "2022-08-07T12:59:50+00:00", "updated": "2022-08-07T12:59:50+00:00", "pdf_url": "http://arxiv.org/pdf/2208.03713v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2207.08365v1", "title": "CausNet : Generational orderings based search for optimal Bayesian networks via dynamic programming with parent set constraints", "abstract": "Finding a globally optimal Bayesian Network using exhaustive search is a\nproblem with super-exponential complexity, which severely restricts the number\nof variables that it can work for. We implement a dynamic programming based\nalgorithm with built-in dimensionality reduction and parent set identification.\nThis reduces the search space drastically and can be applied to\nlarge-dimensional data. We use what we call generational orderings based search\nfor optimal networks, which is a novel way to efficiently search the space of\npossible networks given the possible parent sets. The algorithm supports both\ncontinuous and categorical data, and categorical as well as survival outcomes.\nWe demonstrate the efficacy of our algorithm on both synthetic and real data.\nIn simulations, our algorithm performs better than three state-of-art\nalgorithms that are currently used extensively. We then apply it to an Ovarian\nCancer gene expression dataset with 513 genes and a survival outcome. Our\nalgorithm is able to find an optimal network describing the disease pathway\nconsisting of 6 genes leading to the outcome node in a few minutes on a basic\ncomputer. Our generational orderings based search for optimal networks, is both\nefficient and highly scalable approach to finding optimal Bayesian Networks,\nthat can be applied to 1000s of variables. Using specifiable parameters -\ncorrelation, FDR cutoffs, and in-degree - one can increase or decrease the\nnumber of nodes and density of the networks. Availability of two scoring\noption-BIC and Bge-and implementation of survival outcomes and mixed data types\nmakes our algorithm very suitable for many types of high dimensional biomedical\ndata to find disease pathways.", "authors": ["Nand Sharma", "Joshua Millstein"], "categories": ["cs.AI"], "published": "2022-07-18T03:26:41+00:00", "updated": "2022-07-18T03:26:41+00:00", "pdf_url": "http://arxiv.org/pdf/2207.08365v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2207.05987v3", "title": "DocPrompting: Generating Code by Retrieving the Docs", "abstract": "Publicly available source-code libraries are continuously growing and\nchanging. This makes it impossible for models of code to keep current with all\navailable APIs by simply training these models on existing code repositories.\nThus, existing models inherently cannot generalize to using unseen functions\nand libraries, because these would never appear in the training data. In\ncontrast, when human programmers use functions and libraries for the first\ntime, they frequently refer to textual resources such as code manuals and\ndocumentation, to explore and understand the available functionality. Inspired\nby this observation, we introduce DocPrompting: a natural-language-to-code\ngeneration approach that explicitly leverages documentation by (1) retrieving\nthe relevant documentation pieces given an NL intent, and (2) generating code\nbased on the NL intent and the retrieved documentation. DocPrompting is\ngeneral: it can be applied to any programming language and is agnostic to the\nunderlying neural model. We demonstrate that DocPrompting consistently improves\nNL-to-code models: DocPrompting improves strong base models such as CodeT5 by\n2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in\nexecution-based evaluation on the popular Python CoNaLa benchmark; on a new\nBash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to\nabsolute 6.9% exact match.", "authors": ["Shuyan Zhou", "Uri Alon", "Frank F. Xu", "Zhiruo Wang", "Zhengbao Jiang", "Graham Neubig"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "published": "2022-07-13T06:47:51+00:00", "updated": "2023-02-18T18:27:49+00:00", "pdf_url": "http://arxiv.org/pdf/2207.05987v3", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2206.13325v1", "title": "BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT", "abstract": "Developers use shell commands for many tasks, such as file system management,\nnetwork control, and process management. Bash is one of the most commonly used\nshells and plays an important role in Linux system development and maintenance.\nDue to the language flexibility of Bash code, developers who are not familiar\nwith Bash often have difficulty understanding the purpose and functionality of\nBash code. In this study, we study Bash code comment generation problem and\nproposed an automatic method BashExplainer based on two-stage training\nstrategy. In the first stage, we train a Bash encoder by fine-tuning CodeBERT\non our constructed Bash code corpus. In the second stage, we first retrieve the\nmost similar code from the code repository for the target code based on\nsemantic and lexical similarity. Then we use the trained Bash encoder to\ngenerate two vector representations. Finally, we fuse these two vector\nrepresentations via the fusion layer and generate the code comment through the\ndecoder. To show the competitiveness of our proposed method, we construct a\nhigh-quality corpus by combining the corpus shared in the previous NL2Bash\nstudy and the corpus shared in the NLC2CMD competition. This corpus contains\n10,592 Bash codes and corresponding comments. Then we selected ten baselines\nfrom previous studies on automatic code comment generation, which cover\ninformation retrieval methods, deep learning methods, and hybrid methods.", "authors": ["Chi Yu", "Guang Yang", "Xiang Chen", "Ke Liu", "Yanlin Zhou"], "categories": ["cs.SE"], "published": "2022-06-27T14:13:37+00:00", "updated": "2022-06-27T14:13:37+00:00", "pdf_url": "http://arxiv.org/pdf/2206.13325v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2205.06259v1", "title": "Computing Programs for Generalized Planning as Heuristic Search", "abstract": "Although heuristic search is one of the most successful approaches to\nclassical planning, this planning paradigm does not apply straightforwardly to\nGeneralized Planning (GP). This paper adapts the planning as heuristic search\nparadigm to the particularities of GP, and presents the first native heuristic\nsearch approach to GP. First, the paper defines a program-based solution space\nfor GP that is independent of the number of planning instances in a GP problem,\nand the size of these instances. Second, the paper defines the BFGP algorithm\nfor GP, that implements a best-first search in our program-based solution\nspace, and that is guided by different evaluation and heuristic functions.", "authors": ["Javier Segovia-Aguas", "Sergio Jim\u00e9nez", "Anders Jonsson"], "categories": ["cs.AI"], "published": "2022-05-12T17:57:09+00:00", "updated": "2022-05-12T17:57:09+00:00", "pdf_url": "http://arxiv.org/pdf/2205.06259v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2204.08561v1", "title": "QuSBT: Search-Based Testing of Quantum Programs", "abstract": "Generating a test suite for a quantum program such that it has the maximum\nnumber of failing tests is an optimization problem. For such optimization,\nsearch-based testing has shown promising results in the context of classical\nprograms. To this end, we present a test generation tool for quantum programs\nbased on a genetic algorithm, called QuSBT (Search-based Testing of Quantum\nPrograms). QuSBT automates the testing of quantum programs, with the aim of\nfinding a test suite having the maximum number of failing test cases. QuSBT\nutilizes IBM's Qiskit as the simulation framework for quantum programs. We\npresent the tool architecture in addition to the implemented methodology (i.e.,\nthe encoding of the search individual, the definition of the fitness function\nexpressing the search problem, and the test assessment w.r.t. two types of\nfailures). Finally, we report results of the experiments in which we tested a\nset of faulty quantum programs with QuSBT to assess its effectiveness.\nRepository (code and experimental results):\nhttps://github.com/Simula-COMPLEX/qusbt-tool Video:\nhttps://youtu.be/3apRCtluAn4", "authors": ["Xinyi Wang", "Paolo Arcaini", "Tao Yue", "Shaukat Ali"], "categories": ["cs.SE"], "published": "2022-04-18T21:03:57+00:00", "updated": "2022-04-18T21:03:57+00:00", "pdf_url": "http://arxiv.org/pdf/2204.08561v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2204.11594v1", "title": "Addressing Leakage in Self-Supervised Contextualized Code Retrieval", "abstract": "We address contextualized code retrieval, the search for code snippets\nhelpful to fill gaps in a partial input program. Our approach facilitates a\nlarge-scale self-supervised contrastive training by splitting source code\nrandomly into contexts and targets. To combat leakage between the two, we\nsuggest a novel approach based on mutual identifier masking, dedentation, and\nthe selection of syntax-aligned targets. Our second contribution is a new\ndataset for direct evaluation of contextualized code retrieval, based on a\ndataset of manually aligned subpassages of code clones. Our experiments\ndemonstrate that our approach improves retrieval substantially, and yields new\nstate-of-the-art results for code clone and defect detection.", "authors": ["Johannes Villmow", "Viola Campos", "Adrian Ulges", "Ulrich Schwanecke"], "categories": ["cs.SE", "cs.IR", "cs.LG"], "published": "2022-04-17T12:58:38+00:00", "updated": "2022-04-17T12:58:38+00:00", "pdf_url": "http://arxiv.org/pdf/2204.11594v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2204.07023v1", "title": "Composite Code Sparse Autoencoders for first stage retrieval", "abstract": "We propose a Composite Code Sparse Autoencoder (CCSA) approach for\nApproximate Nearest Neighbor (ANN) search of document representations based on\nSiamese-BERT models. In Information Retrieval (IR), the ranking pipeline is\ngenerally decomposed in two stages: the first stage focus on retrieving a\ncandidate set from the whole collection. The second stage re-ranks the\ncandidate set by relying on more complex models. Recently, Siamese-BERT models\nhave been used as first stage ranker to replace or complement the traditional\nbag-of-word models. However, indexing and searching a large document collection\nrequire efficient similarity search on dense vectors and this is why ANN\ntechniques come into play. Since composite codes are naturally sparse, we first\nshow how CCSA can learn efficient parallel inverted index thanks to an\nuniformity regularizer. Second, CCSA can be used as a binary quantization\nmethod and we propose to combine it with the recent graph based ANN techniques.\nOur experiments on MSMARCO dataset reveal that CCSA outperforms IVF with\nproduct quantization. Furthermore, CCSA binary quantization is beneficial for\nthe index size, and memory usage for the graph-based HNSW method, while\nmaintaining a good level of recall and MRR. Third, we compare with recent\nsupervised quantization methods for image retrieval and find that CCSA is able\nto outperform them.", "authors": ["Carlos Lassance", "Thibault Formal", "Stephane Clinchant"], "categories": ["cs.IR", "cs.AI"], "published": "2022-04-14T15:20:46+00:00", "updated": "2022-04-14T15:20:46+00:00", "pdf_url": "http://arxiv.org/pdf/2204.07023v1", "primary_category": "cs.IR"}
{"id": "http://arxiv.org/abs/2204.03293v3", "title": "CoCoSoDa: Effective Contrastive Learning for Code Search", "abstract": "Code search aims to retrieve semantically relevant code snippets for a given\nnatural language query. Recently, many approaches employing contrastive\nlearning have shown promising results on code representation learning and\ngreatly improved the performance of code search. However, there is still a lot\nof room for improvement in using contrastive learning for code search. In this\npaper, we propose CoCoSoDa to effectively utilize contrastive learning for code\nsearch via two key factors in contrastive learning: data augmentation and\nnegative samples. Specifically, soft data augmentation is to dynamically\nmasking or replacing some tokens with their types for input sequences to\ngenerate positive samples. Momentum mechanism is used to generate large and\nconsistent representations of negative samples in a mini-batch through\nmaintaining a queue and a momentum encoder. In addition, multimodal contrastive\nlearning is used to pull together representations of code-query pairs and push\napart the unpaired code snippets and queries. We conduct extensive experiments\nto evaluate the effectiveness of our approach on a large-scale dataset with six\nprogramming languages. Experimental results show that: (1) CoCoSoDa outperforms\n14 baselines and especially exceeds CodeBERT, GraphCodeBERT, and UniXcoder by\n13.3%, 10.5%, and 5.9% on average MRR scores, respectively. (2) The ablation\nstudies show the effectiveness of each component of our approach. (3) We adapt\nour techniques to several different pre-trained models such as RoBERTa,\nCodeBERT, and GraphCodeBERT and observe a significant boost in their\nperformance in code search. (4) Our model performs robustly under different\nhyper-parameters. Furthermore, we perform qualitative and quantitative analyses\nto explore reasons behind the good performance of our model.", "authors": ["Ensheng Shi", "Yanlin Wang", "Wenchao Gu", "Lun Du", "Hongyu Zhang", "Shi Han", "Dongmei Zhang", "Hongbin Sun"], "categories": ["cs.SE", "cs.AI", "cs.LG"], "published": "2022-04-07T08:49:27+00:00", "updated": "2023-02-12T10:47:18+00:00", "pdf_url": "http://arxiv.org/pdf/2204.03293v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2204.02787v2", "title": "DiffSearch: A Scalable and Precise Search Engine for Code Changes", "abstract": "The source code of successful projects is evolving all the time, resulting in\nhundreds of thousands of code changes stored in source code repositories. This\nwealth of data can be useful, e.g., to find changes similar to a planned code\nchange or examples of recurring code improvements. This paper presents\nDiffSearch, a search engine that, given a query that describes a code change,\nreturns a set of changes that match the query. The approach is enabled by three\nkey contributions. First, we present a query language that extends the\nunderlying programming language with wildcards and placeholders, providing an\nintuitive way of formulating queries that is easy to adapt to different\nprogramming languages. Second, to ensure scalability, the approach indexes code\nchanges in a one-time preprocessing step, mapping them into a feature space,\nand then performs an efficient search in the feature space for each query.\nThird, to guarantee precision, i.e., that any returned code change indeed\nmatches the given query, we present a tree-based matching algorithm that checks\nwhether a query can be expanded to a concrete code change. We present\nimplementations for Java, JavaScript, and Python, and show that the approach\nresponds within seconds to queries across one million code changes, has a\nrecall of 80.7% for Java, 89.6% for Python, and 90.4% for JavaScript, enables\nusers to find relevant code changes more effectively than a regular\nexpression-based search, and is helpful for gathering a large-scale dataset of\nreal-world bug fixes.", "authors": ["Luca Di Grazia", "Paul Bredl", "Michael Pradel"], "categories": ["cs.SE"], "published": "2022-04-06T12:57:57+00:00", "updated": "2022-10-31T09:36:43+00:00", "pdf_url": "http://arxiv.org/pdf/2204.02787v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2204.02765v3", "title": "Code Search: A Survey of Techniques for Finding Code", "abstract": "The immense amounts of source code provide ample challenges and opportunities\nduring software development. To handle the size of code bases, developers\ncommonly search for code, e.g., when trying to find where a particular feature\nis implemented or when looking for code examples to reuse. To support\ndevelopers in finding relevant code, various code search engines have been\nproposed. This article surveys 30 years of research on code search, giving a\ncomprehensive overview of challenges and techniques that address them. We\ndiscuss the kinds of queries that code search engines support, how to\npreprocess and expand queries, different techniques for indexing and retrieving\ncode, and ways to rank and prune search results. Moreover, we describe\nempirical studies of code search in practice. Based on the discussion of prior\nwork, we conclude the article with an outline of challenges and opportunities\nto be addressed in the future.", "authors": ["Luca Di Grazia", "Michael Pradel"], "categories": ["cs.SE"], "published": "2022-04-06T12:17:58+00:00", "updated": "2022-10-05T13:17:29+00:00", "pdf_url": "http://arxiv.org/pdf/2204.02765v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2203.15287v2", "title": "Accelerating Code Search with Deep Hashing and Code Classification", "abstract": "Code search is to search reusable code snippets from source code corpus based\non natural languages queries. Deep learning-based methods of code search have\nshown promising results. However, previous methods focus on retrieval accuracy\nbut lacked attention to the efficiency of the retrieval process. We propose a\nnovel method CoSHC to accelerate code search with deep hashing and code\nclassification, aiming to perform an efficient code search without sacrificing\ntoo much accuracy. To evaluate the effectiveness of CoSHC, we apply our method\nto five code search models. Extensive experimental results indicate that\ncompared with previous code search baselines, CoSHC can save more than 90% of\nretrieval time meanwhile preserving at least 99% of retrieval accuracy.", "authors": ["Wenchao Gu", "Yanlin Wang", "Lun Du", "Hongyu Zhang", "Shi Han", "Dongmei Zhang", "Michael R. Lyu"], "categories": ["cs.SE", "cs.AI"], "published": "2022-03-29T07:05:30+00:00", "updated": "2022-03-31T03:01:55+00:00", "pdf_url": "http://arxiv.org/pdf/2203.15287v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2203.07736v4", "title": "CSRS: Code Search with Relevance Matching and Semantic Matching", "abstract": "Developers often search and reuse existing code snippets in the process of\nsoftware development. Code search aims to retrieve relevant code snippets from\na codebase according to natural language queries entered by the developer. Up\nto now, researchers have already proposed information retrieval (IR) based\nmethods and deep learning (DL) based methods. The IR-based methods focus on\nkeyword matching, that is to rank codes by relevance between queries and code\nsnippets, while DL-based methods focus on capturing the semantic correlations.\nHowever, the existing methods do not consider capturing two matching signals\nsimultaneously. Therefore, in this paper, we propose CSRS, a code search model\nwith relevance matching and semantic matching. CSRS comprises (1) an embedding\nmodule containing convolution kernels of different sizes which can extract\nn-gram embeddings of queries and codes, (2) a relevance matching module that\nmeasures lexical matching signals, and (3) a co-attention based semantic\nmatching module to capture the semantic correlation. We train and evaluate CSRS\non a dataset with 18.22M and 10k code snippets. The experimental results\ndemonstrate that CSRS achieves an MRR of 0.614, which outperforms two\nstate-of-the-art models DeepCS and CARLCS-CNN by 33.77% and 18.53%\nrespectively. In addition, we also conducted several experiments to prove the\neffectiveness of each component of CSRS.", "authors": ["Yi Cheng", "Li Kuang"], "categories": ["cs.SE"], "published": "2022-03-15T09:10:18+00:00", "updated": "2022-04-27T06:56:00+00:00", "pdf_url": "http://arxiv.org/pdf/2203.07736v4", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2203.07722v1", "title": "ReACC: A Retrieval-Augmented Code Completion Framework", "abstract": "Code completion, which aims to predict the following code token(s) according\nto the code context, can improve the productivity of software development.\nRecent work has proved that statistical language modeling with transformers can\ngreatly improve the performance in the code completion task via learning from\nlarge-scale source code datasets. However, current approaches focus only on\ncode context within the file or project, i.e. internal context. Our distinction\nis utilizing \"external\" context, inspired by human behaviors of copying from\nthe related code snippets when writing code. Specifically, we propose a\nretrieval-augmented code completion framework, leveraging both lexical copying\nand referring to code with similar semantics by retrieval. We adopt a\nstage-wise training approach that combines a source code retriever and an\nauto-regressive language model for programming language. We evaluate our\napproach in the code completion task in Python and Java programming languages,\nachieving a state-of-the-art performance on CodeXGLUE benchmark.", "authors": ["Shuai Lu", "Nan Duan", "Hojae Han", "Daya Guo", "Seung-won Hwang", "Alexey Svyatkovskiy"], "categories": ["cs.SE", "cs.AI", "cs.CL"], "published": "2022-03-15T08:25:08+00:00", "updated": "2022-03-15T08:25:08+00:00", "pdf_url": "http://arxiv.org/pdf/2203.07722v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2203.04519v1", "title": "Efficient Search of Live-Coding Screencasts from Online Videos", "abstract": "Programming videos on the Internet are valuable resources for learning\nprogramming skills. To find relevant videos, developers typically search online\nvideo platforms (e.g., YouTube) with keywords on topics they wish to learn.\nDevelopers often look for live-coding screencasts, in which the videos' authors\nperform live coding. Yet, not all programming videos are live-coding\nscreencasts. In this work, we develop a tool named PSFinder to identify\nlive-coding screencasts. PSFinder leverages a classifier to identify whether a\nvideo frame contains an IDE window. It uses a sampling strategy to pick a\nnumber of frames from an input video, runs the classifer on these frames, and\nthen determines whether the video is a live-coding screencast based on frames\nclassified as containing IDE window. In our preliminary experiment, PSFinder\ncan effectively identify live-coding screencasts as it achieves an F1-score of\n0.97.", "authors": ["Chengran Yang", "Ferdian Thung", "David Lo"], "categories": ["cs.SE", "cs.MM"], "published": "2022-03-09T04:32:37+00:00", "updated": "2022-03-09T04:32:37+00:00", "pdf_url": "http://arxiv.org/pdf/2203.04519v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2202.09806v2", "title": "Learning logic programs by discovering where not to search", "abstract": "The goal of inductive logic programming (ILP) is to search for a hypothesis\nthat generalises training examples and background knowledge (BK). To improve\nperformance, we introduce an approach that, before searching for a hypothesis,\nfirst discovers where not to search. We use given BK to discover constraints on\nhypotheses, such as that a number cannot be both even and odd. We use the\nconstraints to bootstrap a constraint-driven ILP system. Our experiments on\nmultiple domains (including program synthesis and game playing) show that our\napproach can (i) substantially reduce learning times by up to 97%, and (ii)\nscale to domains with millions of facts.", "authors": ["Andrew Cropper", "C\u00e9line Hocquette"], "categories": ["cs.LG", "cs.AI", "cs.LO"], "published": "2022-02-20T12:32:03+00:00", "updated": "2022-12-05T09:42:29+00:00", "pdf_url": "http://arxiv.org/pdf/2202.09806v2", "primary_category": "cs.LG"}
{"id": "http://arxiv.org/abs/2202.09555v1", "title": "A Probabilistic Programming Idiom for Active Knowledge Search", "abstract": "In this paper, we derive and implement a probabilistic programming idiom for\nthe problem of acquiring new knowledge about an environment. The idiom is\nimplemented utilizing a modern probabilistic programming language. We\ndemonstrate the utility of this idiom by implementing an algorithm for the\nspecific problem of active mapping and robot exploration. Finally, we evaluate\nthe functionality of the implementation through an extensive simulation study\nutilizing the HouseExpo dataset.", "authors": ["Malte R. Damgaard", "Rasmus Pedersen", "Thomas Bak"], "categories": ["cs.RO", "cs.AI", "G.3; I.2.8; I.2.9"], "published": "2022-02-19T09:13:14+00:00", "updated": "2022-02-19T09:13:14+00:00", "pdf_url": "http://arxiv.org/pdf/2202.09555v1", "primary_category": "cs.RO"}
{"id": "http://arxiv.org/abs/2202.08029v1", "title": "Code Search based on Context-aware Code Translation", "abstract": "Code search is a widely used technique by developers during software\ndevelopment. It provides semantically similar implementations from a large code\ncorpus to developers based on their queries. Existing techniques leverage deep\nlearning models to construct embedding representations for code snippets and\nqueries, respectively. Features such as abstract syntactic trees, control flow\ngraphs, etc., are commonly employed for representing the semantics of code\nsnippets. However, the same structure of these features does not necessarily\ndenote the same semantics of code snippets, and vice versa. In addition, these\ntechniques utilize multiple different word mapping functions that map query\nwords/code tokens to embedding representations. This causes diverged embeddings\nof the same word/token in queries and code snippets. We propose a novel\ncontext-aware code translation technique that translates code snippets into\nnatural language descriptions (called translations). The code translation is\nconducted on machine instructions, where the context information is collected\nby simulating the execution of instructions. We further design a shared word\nmapping function using one single vocabulary for generating embeddings for both\ntranslations and queries. We evaluate the effectiveness of our technique,\ncalled TranCS, on the CodeSearchNet corpus with 1,000 queries. Experimental\nresults show that TranCS significantly outperforms state-of-the-art techniques\nby 49.31% to 66.50% in terms of MRR (mean reciprocal rank).", "authors": ["Weisong Sun", "Chunrong Fang", "Yuchen Chen", "Guanhong Tao", "Tingxu Han", "Quanjun Zhang"], "categories": ["cs.SE", "cs.AI", "D.2"], "published": "2022-02-16T12:45:47+00:00", "updated": "2022-02-16T12:45:47+00:00", "pdf_url": "http://arxiv.org/pdf/2202.08029v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2202.06649v1", "title": "On the Importance of Building High-quality Training Datasets for Neural Code Search", "abstract": "The performance of neural code search is significantly influenced by the\nquality of the training data from which the neural models are derived. A large\ncorpus of high-quality query and code pairs is demanded to establish a precise\nmapping from the natural language to the programming language. Due to the\nlimited availability, most widely-used code search datasets are established\nwith compromise, such as using code comments as a replacement of queries. Our\nempirical study on a famous code search dataset reveals that over one-third of\nits queries contain noises that make them deviate from natural user queries.\nModels trained through noisy data are faced with severe performance degradation\nwhen applied in real-world scenarios. To improve the dataset quality and make\nthe queries of its samples semantically identical to real user queries is\ncritical for the practical usability of neural code search. In this paper, we\npropose a data cleaning framework consisting of two subsequent filters: a\nrule-based syntactic filter and a model-based semantic filter. This is the\nfirst framework that applies semantic query cleaning to code search datasets.\nExperimentally, we evaluated the effectiveness of our framework on two\nwidely-used code search models and three manually-annotated code retrieval\nbenchmarks. Training the popular DeepCS model with the filtered dataset from\nour framework improves its performance by 19.2% MRR and 21.3% Answer@1, on\naverage with the three validation benchmarks.", "authors": ["Zhensu Sun", "Li Li", "Yan Liu", "Xiaoning Du", "Li Li"], "categories": ["cs.SE", "cs.AI"], "published": "2022-02-14T12:02:41+00:00", "updated": "2022-02-14T12:02:41+00:00", "pdf_url": "http://arxiv.org/pdf/2202.06649v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2201.11313v1", "title": "Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus", "abstract": "Semantic code search is the task of retrieving relevant code snippet given a\nnatural language query. Different from typical information retrieval tasks,\ncode search requires to bridge the semantic gap between the programming\nlanguage and natural language, for better describing intrinsic concepts and\nsemantics. Recently, deep neural network for code search has been a hot\nresearch topic. Typical methods for neural code search first represent the code\nsnippet and query text as separate embeddings, and then use vector distance\n(e.g. dot-product or cosine) to calculate the semantic similarity between them.\nThere exist many different ways for aggregating the variable length of code or\nquery tokens into a learnable embedding, including bi-encoder, cross-encoder,\nand poly-encoder. The goal of the query encoder and code encoder is to produce\nembeddings that are close with each other for a related pair of query and the\ncorresponding desired code snippet, in which the choice and design of encoder\nis very significant.\n  In this paper, we propose a novel deep semantic model which makes use of the\nutilities of not only the multi-modal sources, but also feature extractors such\nas self-attention, the aggregated vectors, combination of the intermediate\nrepresentations. We apply the proposed model to tackle the CodeSearchNet\nchallenge about semantic code search. We align cross-lingual embedding for\nmulti-modality learning with large batches and hard example mining, and combine\ndifferent learned representations for better enhancing the representation\nlearning. Our model is trained on CodeSearchNet corpus and evaluated on the\nheld-out data, the final model achieves 0.384 NDCG and won the first place in\nthis benchmark. Models and code are available at\nhttps://github.com/overwindows/SemanticCodeSearch.git.", "authors": ["Chen Wu", "Ming Yan"], "categories": ["cs.CL", "cs.IR"], "published": "2022-01-27T04:15:59+00:00", "updated": "2022-01-27T04:15:59+00:00", "pdf_url": "http://arxiv.org/pdf/2201.11313v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2201.10866v3", "title": "CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search", "abstract": "In this paper, we propose the CodeRetriever model, which learns the\nfunction-level code semantic representations through large-scale code-text\ncontrastive pre-training. We adopt two contrastive learning schemes in\nCodeRetriever: unimodal contrastive learning and bimodal contrastive learning.\nFor unimodal contrastive learning, we design an unsupervised learning approach\nto build semantic-related code pairs based on the documentation and function\nname. For bimodal contrastive learning, we leverage the documentation and\nin-line comments of code to build code-text pairs. Both contrastive objectives\ncan fully leverage large-scale code corpus for pre-training. Extensive\nexperimental results show that CodeRetriever achieves new state-of-the-art with\nsignificant improvement over existing code pre-trained models, on eleven\ndomain/language-specific code search tasks with six programming languages in\ndifferent code granularity (function-level, snippet-level and statement-level).\nThese results demonstrate the effectiveness and robustness of CodeRetriever.", "authors": ["Xiaonan Li", "Yeyun Gong", "Yelong Shen", "Xipeng Qiu", "Hang Zhang", "Bolun Yao", "Weizhen Qi", "Daxin Jiang", "Weizhu Chen", "Nan Duan"], "categories": ["cs.CL", "cs.SE"], "published": "2022-01-26T10:54:30+00:00", "updated": "2022-10-26T03:06:58+00:00", "pdf_url": "http://arxiv.org/pdf/2201.10866v3", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2201.09974v1", "title": "Generating Clarifying Questions for Query Refinement in Source Code Search", "abstract": "In source code search, a common information-seeking strategy involves\nproviding a short initial query with a broad meaning, and then iteratively\nrefining the query using terms gleaned from the results of subsequent searches.\nThis strategy requires programmers to spend time reading search results that\nare irrelevant to their development needs. In contrast, when programmers seek\ninformation from other humans, they typically refine queries by asking and\nanswering clarifying questions. Clarifying questions have been shown to benefit\ngeneral-purpose search engines, but have not been examined in the context of\ncode search. We present a method for generating natural-sounding clarifying\nquestions using information extracted from function names and comments. Our\nmethod outperformed a keyword-based method for single-turn refinement in\nsynthetic studies, and was associated with shorter search duration in human\nstudies.", "authors": ["Zachary Eberhart", "Collin McMillan"], "categories": ["cs.SE", "cs.HC", "cs.IR"], "published": "2022-01-24T22:05:13+00:00", "updated": "2022-01-24T22:05:13+00:00", "pdf_url": "http://arxiv.org/pdf/2201.09974v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2201.00150v6", "title": "Cross-Domain Deep Code Search with Meta Learning", "abstract": "Recently, pre-trained programming language models such as CodeBERT have\ndemonstrated substantial gains in code search. Despite showing great\nperformance, they rely on the availability of large amounts of parallel data to\nfine-tune the semantic mappings between queries and code. This restricts their\npracticality in domain-specific languages with relatively scarce and expensive\ndata. In this paper, we propose CroCS, a novel approach for domain-specific\ncode search. CroCS employs a transfer learning framework where an initial\nprogram representation model is pre-trained on a large corpus of common\nprogramming languages (such as Java and Python) and is further adapted to\ndomain-specific languages such as SQL and Solidity. Unlike cross-language\nCodeBERT, which is directly fine-tuned in the target language, CroCS adapts a\nfew-shot meta-learning algorithm called MAML to learn the good initialization\nof model parameters, which can be best reused in a domain-specific language. We\nevaluate the proposed approach on two domain-specific languages, namely, SQL\nand Solidity, with model transferred from two widely used languages (Python and\nJava). Experimental results show that CDCS significantly outperforms\nconventional pre-trained code models that are directly fine-tuned in\ndomain-specific languages, and it is particularly effective for scarce data.", "authors": ["Yitian Chai", "Hongyu Zhang", "Beijun Shen", "Xiaodong Gu"], "categories": ["cs.SE"], "published": "2022-01-01T09:00:48+00:00", "updated": "2024-03-12T05:31:50+00:00", "pdf_url": "http://arxiv.org/pdf/2201.00150v6", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2111.14139v1", "title": "Semantic Code Search for Smart Contracts", "abstract": "Semantic code search technology allows searching for existing code snippets\nthrough natural language, which can greatly improve programming efficiency.\nSmart contracts, programs that run on the blockchain, have a code reuse rate of\nmore than 90%, which means developers have a great demand for semantic code\nsearch tools. However, the existing code search models still have a semantic\ngap between code and query, and perform poorly on specialized queries of smart\ncontracts. In this paper, we propose a Multi-Modal Smart contract Code Search\n(MM-SCS) model. Specifically, we construct a Contract Elements Dependency Graph\n(CEDG) for MM-SCS as an additional modality to capture the data-flow and\ncontrol-flow information of the code. To make the model more focused on the key\ncontextual information, we use a multi-head attention network to generate\nembeddings for code features. In addition, we use a fine-tuned pretrained model\nto ensure the model's effectiveness when the training data is small. We\ncompared MM-SCS with four state-of-the-art models on a dataset with 470K (code,\ndocstring) pairs collected from Github and Etherscan. Experimental results show\nthat MM-SCS achieves an MRR (Mean Reciprocal Rank) of 0.572, outperforming four\nstate-of-the-art models UNIF, DeepCS, CARLCS-CNN, and TAB-CS by 34.2%, 59.3%,\n36.8%, and 14.1%, respectively. Additionally, the search speed of MM-SCS is\nsecond only to UNIF, reaching 0.34s/query.", "authors": ["Chaochen Shi", "Yong Xiang", "Jiangshan Yu", "Longxiang Gao"], "categories": ["cs.SE", "cs.IR"], "published": "2021-11-28T13:36:24+00:00", "updated": "2021-11-28T13:36:24+00:00", "pdf_url": "http://arxiv.org/pdf/2111.14139v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2111.02671v5", "title": "GraphSearchNet: Enhancing GNNs via Capturing Global Dependencies for Semantic Code Search", "abstract": "Code search aims to retrieve accurate code snippets based on a natural\nlanguage query to improve software productivity and quality. With the massive\namount of available programs such as (on GitHub or Stack Overflow), identifying\nand localizing the precise code is critical for the software developers. In\naddition, Deep learning has recently been widely applied to different\ncode-related scenarios, e.g., vulnerability detection, source code\nsummarization. However, automated deep code search is still challenging since\nit requires a high-level semantic mapping between code and natural language\nqueries. Most existing deep learning-based approaches for code search rely on\nthe sequential text i.e., feeding the program and the query as a flat sequence\nof tokens to learn the program semantics while the structural information is\nnot fully considered. Furthermore, the widely adopted Graph Neural Networks\n(GNNs) have proved their effectiveness in learning program semantics, however,\nthey also suffer the problem of capturing the global dependencies in the\nconstructed graph, which limits the model learning capacity. To address these\nchallenges, in this paper, we design a novel neural network framework, named\nGraphSearchNet, to enable an effective and accurate source code search by\njointly learning the rich semantics of both source code and natural language\nqueries. Specifically, we propose to construct graphs for the source code and\nqueries with bidirectional GGNN (BiGGNN) to capture the local structural\ninformation of the source code and queries. Furthermore, we enhance BiGGNN by\nutilizing the multi-head attention module to supplement the global dependencies\nthat BiGGNN missed to improve the model learning capacity. The extensive\nexperiments on Java and Python programming language from the public benchmark\nCodeSearchNet confirm that GraphSearchNet outperforms current state-of-the-art\nworks.", "authors": ["Shangqing Liu", "Xiaofei Xie", "Jingkai Siow", "Lei Ma", "Guozhu Meng", "Yang Liu"], "categories": ["cs.SE", "cs.AI"], "published": "2021-11-04T07:38:35+00:00", "updated": "2023-02-13T13:57:39+00:00", "pdf_url": "http://arxiv.org/pdf/2111.02671v5", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2111.03466v1", "title": "Learning Large Neighborhood Search Policy for Integer Programming", "abstract": "We propose a deep reinforcement learning (RL) method to learn large\nneighborhood search (LNS) policy for integer programming (IP). The RL policy is\ntrained as the destroy operator to select a subset of variables at each step,\nwhich is reoptimized by an IP solver as the repair operator. However, the\ncombinatorial number of variable subsets prevents direct application of typical\nRL algorithms. To tackle this challenge, we represent all subsets by\nfactorizing them into binary decisions on each variable. We then design a\nneural network to learn policies for each variable in parallel, trained by a\ncustomized actor-critic algorithm. We evaluate the proposed method on four\nrepresentative IP problems. Results show that it can find better solutions than\nSCIP in much less time, and significantly outperform other LNS baselines with\nthe same runtime. Moreover, these advantages notably persist when the policies\ngeneralize to larger problems. Further experiments with Gurobi also reveal that\nour method can outperform this state-of-the-art commercial solver within the\nsame time limit.", "authors": ["Yaoxin Wu", "Wen Song", "Zhiguang Cao", "Jie Zhang"], "categories": ["cs.AI", "cs.LG", "math.OC"], "published": "2021-11-01T09:10:49+00:00", "updated": "2021-11-01T09:10:49+00:00", "pdf_url": "http://arxiv.org/pdf/2111.03466v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2110.12485v1", "title": "Scaling Neural Program Synthesis with Distribution-based Search", "abstract": "We consider the problem of automatically constructing computer programs from\ninput-output examples. We investigate how to augment probabilistic and neural\nprogram synthesis methods with new search algorithms, proposing a framework\ncalled distribution-based search. Within this framework, we introduce two new\nsearch algorithms: Heap Search, an enumerative method, and SQRT Sampling, a\nprobabilistic method. We prove certain optimality guarantees for both methods,\nshow how they integrate with probabilistic and neural techniques, and\ndemonstrate how they can operate at scale across parallel compute environments.\nCollectively these findings offer theoretical and applied studies of search\nalgorithms for program synthesis that integrate with recent developments in\nmachine-learned program synthesizers.", "authors": ["Nathana\u00ebl Fijalkow", "Guillaume Lagarde", "Th\u00e9o Matricon", "Kevin Ellis", "Pierre Ohlmann", "Akarsh Potta"], "categories": ["cs.LG", "cs.AI", "cs.PL"], "published": "2021-10-24T16:46:01+00:00", "updated": "2021-10-24T16:46:01+00:00", "pdf_url": "http://arxiv.org/pdf/2110.12485v1", "primary_category": "cs.LG"}
{"id": "http://arxiv.org/abs/2110.11536v2", "title": "Neural-guided, Bidirectional Program Search for Abstraction and Reasoning", "abstract": "One of the challenges facing artificial intelligence research today is\ndesigning systems capable of utilizing systematic reasoning to generalize to\nnew tasks. The Abstraction and Reasoning Corpus (ARC) measures such a\ncapability through a set of visual reasoning tasks. In this paper we report\nincremental progress on ARC and lay the foundations for two approaches to\nabstraction and reasoning not based in brute-force search. We first apply an\nexisting program synthesis system called DreamCoder to create symbolic\nabstractions out of tasks solved so far, and show how it enables solving of\nprogressively more challenging ARC tasks. Second, we design a reasoning\nalgorithm motivated by the way humans approach ARC. Our algorithm constructs a\nsearch graph and reasons over this graph structure to discover task solutions.\nMore specifically, we extend existing execution-guided program synthesis\napproaches with deductive reasoning based on function inverse semantics to\nenable a neural-guided bidirectional search algorithm. We demonstrate the\neffectiveness of the algorithm on three domains: ARC, 24-Game tasks, and a\n'double-and-add' arithmetic puzzle.", "authors": ["Simon Alford", "Anshula Gandhi", "Akshay Rangamani", "Andrzej Banburski", "Tony Wang", "Sylee Dandekar", "John Chin", "Tomaso Poggio", "Peter Chin"], "categories": ["cs.AI", "cs.LG"], "published": "2021-10-22T00:41:47+00:00", "updated": "2021-10-26T15:26:31+00:00", "pdf_url": "http://arxiv.org/pdf/2110.11536v2", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2110.08512v1", "title": "AugmentedCode: Examining the Effects of Natural Language Resources in Code Retrieval Models", "abstract": "Code retrieval is allowing software engineers to search codes through a\nnatural language query, which relies on both natural language processing and\nsoftware engineering techniques. There have been several attempts on code\nretrieval from searching snippet codes to function codes. In this paper, we\nintroduce Augmented Code (AugmentedCode) retrieval which takes advantage of\nexisting information within the code and constructs augmented programming\nlanguage to improve the code retrieval models' performance. We curated a large\ncorpus of Python and showcased the the framework and the results of augmented\nprogramming language which outperforms on CodeSearchNet and CodeBERT with a\nMean Reciprocal Rank (MRR) of 0.73 and 0.96, respectively. The outperformed\nfine-tuned augmented code retrieval model is published in HuggingFace at\nhttps://huggingface.co/Fujitsu/AugCode and a demonstration video is available\nat: https://youtu.be/mnZrUTANjGs .", "authors": ["Mehdi Bahrami", "N. C. Shrikanth", "Yuji Mizobuchi", "Lei Liu", "Masahiro Fukuyori", "Wei-Peng Chen", "Kazuki Munakata"], "categories": ["cs.SE", "cs.AI"], "published": "2021-10-16T08:44:48+00:00", "updated": "2021-10-16T08:44:48+00:00", "pdf_url": "http://arxiv.org/pdf/2110.08512v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2110.08423v2", "title": "Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework", "abstract": "In Mixed Integer Linear Programming (MIP), a (strong) backdoor is a \"small\"\nsubset of an instance's integer variables with the following property: in a\nbranch-and-bound procedure, the instance can be solved to global optimality by\nbranching only on the variables in the backdoor. Constructing datasets of\npre-computed backdoors for widely used MIP benchmark sets or particular problem\nfamilies can enable new questions around novel structural properties of a MIP,\nor explain why a problem that is hard in theory can be solved efficiently in\npractice. Existing algorithms for finding backdoors rely on sampling candidate\nvariable subsets in various ways, an approach which has demonstrated the\nexistence of backdoors for some instances from MIPLIB2003 and MIPLIB2010.\nHowever, these algorithms fall short of consistently succeeding at the task due\nto an imbalance between exploration and exploitation. We propose BaMCTS, a\nMonte Carlo Tree Search framework for finding backdoors to MIPs. Extensive\nalgorithmic engineering, hybridization with traditional MIP concepts, and close\nintegration with the CPLEX solver have enabled our method to outperform\nbaselines on MIPLIB2017 instances, finding backdoors more frequently and more\nefficiently.", "authors": ["Elias B. Khalil", "Pashootan Vaezipoor", "Bistra Dilkina"], "categories": ["cs.AI"], "published": "2021-10-16T00:36:53+00:00", "updated": "2022-07-07T19:23:40+00:00", "pdf_url": "http://arxiv.org/pdf/2110.08423v2", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2110.07811v1", "title": "Cascaded Fast and Slow Models for Efficient Semantic Code Search", "abstract": "The goal of natural language semantic code search is to retrieve a\nsemantically relevant code snippet from a fixed set of candidates using a\nnatural language query. Existing approaches are neither effective nor efficient\nenough towards a practical semantic code search system. In this paper, we\npropose an efficient and accurate semantic code search framework with cascaded\nfast and slow models, in which a fast transformer encoder model is learned to\noptimize a scalable index for fast retrieval followed by learning a slow\nclassification-based re-ranking model to improve the performance of the top K\nresults from the fast retrieval. To further reduce the high memory cost of\ndeploying two separate models in practice, we propose to jointly train the fast\nand slow model based on a single transformer encoder with shared parameters.\nThe proposed cascaded approach is not only efficient and scalable, but also\nachieves state-of-the-art results with an average mean reciprocal ranking (MRR)\nscore of 0.7795 (across 6 programming languages) as opposed to the previous\nstate-of-the-art result of 0.713 MRR on the CodeSearchNet benchmark.", "authors": ["Akhilesh Deepak Gotmare", "Junnan Li", "Shafiq Joty", "Steven C. H. Hoi"], "categories": ["cs.CL", "cs.PL"], "published": "2021-10-15T02:23:35+00:00", "updated": "2021-10-15T02:23:35+00:00", "pdf_url": "http://arxiv.org/pdf/2110.07811v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2109.06966v1", "title": "Searching for More Efficient Dynamic Programs", "abstract": "Computational models of human language often involve combinatorial problems.\nFor instance, a probabilistic parser may marginalize over exponentially many\ntrees to make predictions. Algorithms for such problems often employ dynamic\nprogramming and are not always unique. Finding one with optimal asymptotic\nruntime can be unintuitive, time-consuming, and error-prone. Our work aims to\nautomate this laborious process. Given an initial correct declarative program,\nwe search for a sequence of semantics-preserving transformations to improve its\nrunning time as much as possible. To this end, we describe a set of program\ntransformations, a simple metric for assessing the efficiency of a transformed\nprogram, and a heuristic search procedure to improve this metric. We show that\nin practice, automated search -- like the mental search performed by human\nprogrammers -- can find substantial improvements to the initial program.\nEmpirically, we show that many common speed-ups described in the NLP literature\ncould have been discovered automatically by our system.", "authors": ["Tim Vieira", "Ryan Cotterell", "Jason Eisner"], "categories": ["cs.CL"], "published": "2021-09-14T20:52:55+00:00", "updated": "2021-09-14T20:52:55+00:00", "pdf_url": "http://arxiv.org/pdf/2109.06966v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2109.05205v2", "title": "Contrastive Quantization with Code Memory for Unsupervised Image Retrieval", "abstract": "The high efficiency in computation and storage makes hashing (including\nbinary hashing and quantization) a common strategy in large-scale retrieval\nsystems. To alleviate the reliance on expensive annotations, unsupervised deep\nhashing becomes an important research problem. This paper provides a novel\nsolution to unsupervised deep quantization, namely Contrastive Quantization\nwith Code Memory (MeCoQ). Different from existing reconstruction-based\nstrategies, we learn unsupervised binary descriptors by contrastive learning,\nwhich can better capture discriminative visual semantics. Besides, we uncover\nthat codeword diversity regularization is critical to prevent contrastive\nlearning-based quantization from model degeneration. Moreover, we introduce a\nnovel quantization code memory module that boosts contrastive learning with\nlower feature drift than conventional feature memories. Extensive experiments\non benchmark datasets show that MeCoQ outperforms state-of-the-art methods.\nCode and configurations are publicly available at\nhttps://github.com/gimpong/AAAI22-MeCoQ.", "authors": ["Jinpeng Wang", "Ziyun Zeng", "Bin Chen", "Tao Dai", "Shu-Tao Xia"], "categories": ["cs.CV", "cs.AI", "cs.IR"], "published": "2021-09-11T07:16:18+00:00", "updated": "2022-03-08T05:41:45+00:00", "pdf_url": "http://arxiv.org/pdf/2109.05205v2", "primary_category": "cs.CV"}
{"id": "http://arxiv.org/abs/2108.11601v2", "title": "Retrieval Augmented Code Generation and Summarization", "abstract": "Software developers write a lot of source code and documentation during\nsoftware development. Intrinsically, developers often recall parts of source\ncode or code summaries that they had written in the past while implementing\nsoftware or documenting them. To mimic developers' code or summary generation\nbehavior, we propose a retrieval augmented framework, REDCODER, that retrieves\nrelevant code or summaries from a retrieval database and provides them as a\nsupplement to code generation or summarization models. REDCODER has a couple of\nuniqueness. First, it extends the state-of-the-art dense retrieval technique to\nsearch for relevant code or summaries. Second, it can work with retrieval\ndatabases that include unimodal (only code or natural language description) or\nbimodal instances (code-description pairs). We conduct experiments and\nextensive analysis on two benchmark datasets of code generation and\nsummarization in Java and Python, and the promising results endorse the\neffectiveness of our proposed retrieval augmented framework.", "authors": ["Md Rizwan Parvez", "Wasi Uddin Ahmad", "Saikat Chakraborty", "Baishakhi Ray", "Kai-Wei Chang"], "categories": ["cs.SE", "cs.CL"], "published": "2021-08-26T06:48:13+00:00", "updated": "2021-09-10T17:54:13+00:00", "pdf_url": "http://arxiv.org/pdf/2108.11601v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2108.09646v2", "title": "A Systematic Review of Automated Query Reformulations in Source Code Search", "abstract": "Fixing software bugs and adding new features are two of the major maintenance\ntasks. Software bugs and features are reported as change requests. Developers\nconsult these requests and often choose a few keywords from them as an ad hoc\nquery. Then they execute the query with a search engine to find the exact\nlocations within software code that need to be changed. Unfortunately, even\nexperienced developers often fail to choose appropriate queries, which leads to\ncostly trials and errors during a code search. Over the years, many studies\nattempt to reformulate the ad hoc queries from developers to support them. In\nthis systematic literature review, we carefully select 70 primary studies on\nquery reformulations from 2,970 candidate studies, perform an in-depth\nqualitative analysis (e.g., Grounded Theory), and then answer seven research\nquestions with major findings. First, to date, eight major methodologies (e.g.,\nterm weighting, term co-occurrence analysis, thesaurus lookup) have been\nadopted to reformulate queries. Second, the existing studies suffer from\nseveral major limitations (e.g., lack of generalizability, vocabulary mismatch\nproblem, subjective bias) that might prevent their wide adoption. Finally, we\ndiscuss the best practices and future opportunities to advance the state of\nresearch in search query reformulations.", "authors": ["Mohammad Masudur Rahman", "Chanchal K. Roy"], "categories": ["cs.SE", "cs.IR", "cs.LG", "cs.NE", "D.2.5; D.2.1; D.2.7; D.2.13"], "published": "2021-08-22T05:47:10+00:00", "updated": "2023-06-08T22:10:08+00:00", "pdf_url": "http://arxiv.org/pdf/2108.09646v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2108.07114v1", "title": "Improving Readability of Scratch Programs with Search-based Refactoring", "abstract": "Block-based programming languages like Scratch have become increasingly\npopular as introductory languages for novices. These languages are intended to\nbe used with a \"tinkering\" approach which allows learners and teachers to\nquickly assemble working programs and games, but this often leads to low code\nquality. Such code can be hard to comprehend, changing it is error-prone, and\nlearners may struggle and lose interest. The general solution to improve code\nquality is to refactor the code. However, Scratch lacks many of the common\nabstraction mechanisms used when refactoring programs written in higher\nprogramming languages. In order to improve Scratch code, we therefore propose a\nset of atomic code transformations to optimise readability by (1) rewriting\ncontrol structures and (2) simplifying scripts using the inherently concurrent\nnature of Scratch programs. By automating these transformations it is possible\nto explore the space of possible variations of Scratch programs. In this paper,\nwe describe a multi-objective search-based approach that determines sequences\nof code transformations which improve the readability of a given Scratch\nprogram and therefore form refactorings. Evaluation on a random sample of 1000\nScratch programs demonstrates that the generated refactorings reduce complexity\nand entropy in 70.4% of the cases, and 354 projects are improved in at least\none metric without making any other metric worse. The refactored programs can\nhelp both novices and their teachers to improve their code.", "authors": ["Felix Adler", "Gordon Fraser", "Eva Gr\u00fcndinger", "Nina K\u00f6rber", "Simon Labrenz", "Jonas Lerchenberger", "Stephan Lukasczyk", "Sebastian Schweikl"], "categories": ["cs.SE", "68N99", "D.2.5"], "published": "2021-08-16T14:35:07+00:00", "updated": "2021-08-16T14:35:07+00:00", "pdf_url": "http://arxiv.org/pdf/2108.07114v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2108.05890v2", "title": "On the Effectiveness of Transfer Learning for Code Search", "abstract": "The Transformer architecture and transfer learning have marked a quantum leap\nin natural language processing, improving the state of the art across a range\nof text-based tasks. This paper examines how these advancements can be applied\nto and improve code search. To this end, we pre-train a BERT-based model on\ncombinations of natural language and source code data and fine-tune it on pairs\nof StackOverflow question titles and code answers. Our results show that the\npre-trained models consistently outperform the models that were not\npre-trained. In cases where the model was pre-trained on natural language \"and\"\nsource code data, it also outperforms an information retrieval baseline based\non Lucene. Also, we demonstrated that the combined use of an information\nretrieval-based approach followed by a Transformer leads to the best results\noverall, especially when searching into a large search pool. Transfer learning\nis particularly effective when much pre-training data is available and\nfine-tuning data is limited. We demonstrate that natural language processing\nmodels based on the Transformer architecture can be directly applied to source\ncode analysis tasks, such as code search. With the development of Transformer\nmodels designed more specifically for dealing with source code data, we believe\nthe results of source code analysis tasks can be further improved.", "authors": ["Pasquale Salza", "Christoph Schwizer", "Jian Gu", "Harald C. Gall"], "categories": ["cs.SE"], "published": "2021-08-12T17:59:15+00:00", "updated": "2022-08-25T19:44:14+00:00", "pdf_url": "http://arxiv.org/pdf/2108.05890v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2108.04455v1", "title": "Searching for Multi-Fault Programs in Defects4J", "abstract": "Defects4J has enabled numerous software testing and debugging research work\nsince its introduction. A large part of its contribution, and the resulting\npopularity, lies in the clear separation and distillation of the root cause of\neach individual test failure based on careful manual analysis, which in turn\nallowed researchers to easily study individual faults in isolation. However, in\na realistic debugging scenario, multiple faults can coexist and affect test\nresults collectively. Study of automated debugging techniques for these\nsituations, such as failure clustering or fault localisation for multiple\nfaults, would significantly benefit from a reliable benchmark of multiple,\ncoexisting faults. We search for versions of Defects4J subjects that contain\nmultiple faults, by iteratively transplanting fault-revealing test cases across\nDefects4J versions. Out of 326 studied versions of Defects4J subjects, we\nreport that over 95% (311 versions) actually contain from two to 24 faults. We\nhope that the extended, multi-fault Defects4J can provide a platform for future\nresearch of testing and debugging techniques for multi-fault programs.", "authors": ["Gabin An", "Juyeon Yoon", "Shin Yoo"], "categories": ["cs.SE"], "published": "2021-08-10T05:34:15+00:00", "updated": "2021-08-10T05:34:15+00:00", "pdf_url": "http://arxiv.org/pdf/2108.04455v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2108.02702v1", "title": "Improved Retrieval of Programming Solutions With Code Examples Using a Multi-featured Score", "abstract": "Developers often depend on code search engines to obtain solutions for their\nprogramming tasks. However, finding an expected solution containing code\nexamples along with their explanations is challenging due to several issues.\nThere is a vocabulary mismatch between the search keywords (the query) and the\nappropriate solutions. Semantic gap may increase for similar bag of words due\nto antonyms and negation. Moreover, documents retrieved by search engines might\nnot contain solutions containing both code examples and their explanations. So,\nwe propose CRAR (Crowd Answer Recommender) to circumvent those issues aiming at\nimproving retrieval of relevant answers from Stack Overflow containing not only\nthe expected code examples for the given task but also their explanations.\nGiven a programming task, we investigate the effectiveness of combining\ninformation retrieval techniques along with a set of features to enhance the\nranking of important threads (i.e., the units containing questions along with\ntheir answers) for the given task and then selects relevant answers contained\nin those threads, including semantic features, like word embeddings and\nsentence embeddings, for instance, a Convolutional Neural Network (CNN). CRAR\nalso leverages social aspects of Stack Overflow discussions like popularity to\nselect relevant answers for the tasks. Our experimental evaluation shows that\nthe combination of the different features performs better than each one\nindividually. We also compare the retrieval performance with the state-of-art\nCROKAGE (Crowd Knowledge Answer Generator), which is also a system aimed at\nretrieving relevant answers from Stack Overflow. We show that CRAR outperforms\nCROKAGE in Mean Reciprocal Rank and Mean Recall with small and medium effect\nsizes, respectively.", "authors": ["Rodrigo F. Silva", "M. Masudur Rahman", "Carlos Eduardo Dantas", "Chanchal Roy", "Foutse Khomh", "Marcelo A. Maia"], "categories": ["cs.SE"], "published": "2021-08-05T16:11:00+00:00", "updated": "2021-08-05T16:11:00+00:00", "pdf_url": "http://arxiv.org/pdf/2108.02702v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2107.04773v2", "title": "Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning for Semantic Code Search", "abstract": "Recently, deep learning methods have become mainstream in code search since\nthey do better at capturing semantic correlations between code snippets and\nsearch queries and have promising performance. However, code snippets have\ndiverse information from different dimensions, such as business logic, specific\nalgorithm, and hardware communication, so it is hard for a single code\nrepresentation module to cover all the perspectives. On the other hand, as a\nspecific query may focus on one or several perspectives, it is difficult for a\nsingle query representation module to represent different user intents. In this\npaper, we propose MuCoS, a multi-model ensemble learning architecture for\nsemantic code search. It combines several individual learners, each of which\nemphasizes a specific perspective of code snippets. We train the individual\nlearners on different datasets which contain different perspectives of code\ninformation, and we use a data augmentation strategy to get these different\ndatasets. Then we ensemble the learners to capture comprehensive features of\ncode snippets.", "authors": ["Lun Du", "Xiaozhou Shi", "Yanlin Wang", "Ensheng Shi", "Shi Han", "Dongmei Zhang"], "categories": ["cs.SE", "cs.LG"], "published": "2021-07-10T06:40:44+00:00", "updated": "2021-07-13T02:42:51+00:00", "pdf_url": "http://arxiv.org/pdf/2107.04773v2", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2107.00992v3", "title": "Multimodal Representation for Neural Code Search", "abstract": "Semantic code search is about finding semantically relevant code snippets for\na given natural language query. In the state-of-the-art approaches, the\nsemantic similarity between code and query is quantified as the distance of\ntheir representation in the shared vector space. In this paper, to improve the\nvector space, we introduce tree-serialization methods on a simplified form of\nAST and build the multimodal representation for the code data. We conduct\nextensive experiments using a single corpus that is large-scale and\nmulti-language: CodeSearchNet. Our results show that both our tree-serialized\nrepresentations and multimodal learning model improve the performance of code\nsearch. Last, we define intuitive quantification metrics oriented to the\ncompleteness of semantic and syntactic information of the code data, to help\nunderstand the experimental findings.", "authors": ["Jian Gu", "Zimin Chen", "Martin Monperrus"], "categories": ["cs.SE", "cs.LG"], "published": "2021-07-02T12:08:19+00:00", "updated": "2022-01-13T18:38:33+00:00", "pdf_url": "http://arxiv.org/pdf/2107.00992v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2106.11053v3", "title": "Leveraging Language to Learn Program Abstractions and Search Heuristics", "abstract": "Inductive program synthesis, or inferring programs from examples of desired\nbehavior, offers a general paradigm for building interpretable, robust, and\ngeneralizable machine learning systems. Effective program synthesis depends on\ntwo key ingredients: a strong library of functions from which to build\nprograms, and an efficient search strategy for finding programs that solve a\ngiven task. We introduce LAPS (Language for Abstraction and Program Search), a\ntechnique for using natural language annotations to guide joint learning of\nlibraries and neurally-guided search models for synthesis. When integrated into\na state-of-the-art library learning system (DreamCoder), LAPS produces\nhigher-quality libraries and improves search efficiency and generalization on\nthree domains -- string editing, image composition, and abstract reasoning\nabout scenes -- even when no natural language hints are available at test time.", "authors": ["Catherine Wong", "Kevin Ellis", "Joshua B. Tenenbaum", "Jacob Andreas"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "published": "2021-06-18T15:08:47+00:00", "updated": "2022-05-03T22:19:55+00:00", "pdf_url": "http://arxiv.org/pdf/2106.11053v3", "primary_category": "cs.LG"}
{"id": "http://arxiv.org/abs/2106.09173v1", "title": "Cross-Language Code Search using Static and Dynamic Analyses", "abstract": "As code search permeates most activities in software development,code-to-code\nsearch has emerged to support using code as a query and retrieving similar code\nin the search results. Applications include duplicate code detection for\nrefactoring, patch identification for program repair, and language translation.\nExisting code-to-code search tools rely on static similarity approaches such as\nthe comparison of tokens and abstract syntax trees (AST) to approximate dynamic\nbehavior, leading to low precision. Most tools do not support cross-language\ncode-to-code search, and those that do, rely on machine learning models that\nrequire labeled training data.\n  We present Code-to-Code Search Across Languages (COSAL), a cross-language\ntechnique that uses both static and dynamic analyses to identify similar code\nand does not require a machine learning model. Code snippets are ranked using\nnon-dominated sorting based on code token similarity, structural similarity,\nand behavioral similarity. We empirically evaluate COSAL on two datasets of\n43,146Java and Python files and 55,499 Java files and find that 1) code search\nbased on non-dominated ranking of static and dynamic similarity measures is\nmore effective compared to single or weighted measures; and 2) COSAL has better\nprecision and recall compared to state-of-the-art within-language and\ncross-language code-to-code search tools. We explore the potential for using\nCOSAL on large open-source repositories and discuss scalability to more\nlanguages and similarity metrics, providing a gateway for\npractical,multi-language code-to-code search.", "authors": ["George Mathew", "Kathryn T. Stolee"], "categories": ["cs.SE"], "published": "2021-06-16T23:27:01+00:00", "updated": "2021-06-16T23:27:01+00:00", "pdf_url": "http://arxiv.org/pdf/2106.09173v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2106.03042v1", "title": "Clone-Seeker: Effective Code Clone Search Using Annotations", "abstract": "Source code search plays an important role in software development, e.g. for\nexploratory development or opportunistic reuse of existing code from a code\nbase. Often, exploration of different implementations with the same\nfunctionality is needed for tasks like automated software transplantation,\nsoftware diversification, and software repair. Code clones, which are\nsyntactically or semantically similar code fragments, are perfect candidates\nfor such tasks. Searching for code clones involves a given search query to\nretrieve the relevant code fragments. We propose a novel approach called\nClone-Seeker that focuses on utilizing clone class features in retrieving code\nclones. For this purpose, we generate metadata for each code clone in the form\nof a natural language document. The metadata includes a pre-processed list of\nidentifiers from the code clones augmented with a list of keywords indicating\nthe semantics of the code clone. This keyword list can be extracted from a\nmanually annotated general description of the clone class, or automatically\ngenerated from the source code of the entire clone class. This approach helps\ndevelopers to perform code clone search based on a search query written either\nas source code terms, or as natural language. In our quantitative evaluation,\nwe show that (1) Clone-Seeker has a higher recall when searching for semantic\ncode clones (i.e., Type-4) in BigCloneBench than the state-of-the-art; and (2)\nClone-Seeker can accurately search for relevant code clones by applying natural\nlanguage queries.", "authors": ["Muhammad Hammad", "\u00d6nder Babur", "Hamid Abdul Basit", "Mark van den Brand"], "categories": ["cs.SE", "cs.IR"], "published": "2021-06-06T06:14:05+00:00", "updated": "2021-06-06T06:14:05+00:00", "pdf_url": "http://arxiv.org/pdf/2106.03042v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2105.13239v1", "title": "CoSQA: 20,000+ Web Queries for Code Search and Question Answering", "abstract": "Finding codes given natural language query isb eneficial to the productivity\nof software developers. Future progress towards better semantic matching\nbetween query and code requires richer supervised training resources. To remedy\nthis, we introduce the CoSQA dataset.It includes 20,604 labels for pairs of\nnatural language queries and codes, each annotated by at least 3 human\nannotators. We further introduce a contrastive learning method dubbed CoCLR to\nenhance query-code matching, which works as a data augmenter to bring more\nartificially generated training instances. We show that evaluated on CodeXGLUE\nwith the same CodeBERT model, training on CoSQA improves the accuracy of code\nquestion answering by 5.1%, and incorporating CoCLR brings a further\nimprovement of 10.5%.", "authors": ["Junjie Huang", "Duyu Tang", "Linjun Shou", "Ming Gong", "Ke Xu", "Daxin Jiang", "Ming Zhou", "Nan Duan"], "categories": ["cs.CL", "cs.SE"], "published": "2021-05-27T15:37:21+00:00", "updated": "2021-05-27T15:37:21+00:00", "pdf_url": "http://arxiv.org/pdf/2105.13239v1", "primary_category": "cs.CL"}
{"id": "http://arxiv.org/abs/2105.09630v1", "title": "Enriching Query Semantics for Code Search with Reinforcement Learning", "abstract": "Code search is a common practice for developers during software\nimplementation. The challenges of accurate code search mainly lie in the\nknowledge gap between source code and natural language (i.e., queries). Due to\nthe limited code-query pairs and large code-description pairs available, the\nprior studies based on deep learning techniques focus on learning the semantic\nmatching relation between source code and corresponding description texts for\nthe task, and hypothesize that the semantic gap between descriptions and user\nqueries is marginal. In this work, we found that the code search models trained\non code-description pairs may not perform well on user queries, which indicates\nthe semantic distance between queries and code descriptions. To mitigate the\nsemantic distance for more effective code search, we propose QueCos, a\nQuery-enriched Code search model. QueCos learns to generate semantic enriched\nqueries to capture the key semantics of given queries with reinforcement\nlearning (RL). With RL, the code search performance is considered as a reward\nfor producing accurate semantic enriched queries. The enriched queries are\nfinally employed for code search. Experiments on the benchmark datasets show\nthat QueCos can significantly outperform the state-of-the-art code search\nmodels.", "authors": ["Chaozheng Wang", "Zhenghao Nong", "Cuiyun Gao", "Zongjie Li", "Jichuan Zeng", "Zhenchang Xing", "Yang Liu"], "categories": ["cs.SE"], "published": "2021-05-20T09:52:02+00:00", "updated": "2021-05-20T09:52:02+00:00", "pdf_url": "http://arxiv.org/pdf/2105.09630v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2104.08017v1", "title": "BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?", "abstract": "Millions of repetitive code snippets are submitted to code repositories every\nday. To search from these large codebases using simple natural language queries\nwould allow programmers to ideate, prototype, and develop easier and faster.\nAlthough the existing methods have shown good performance in searching codes\nwhen the natural language description contains keywords from the code, they are\nstill far behind in searching codes based on the semantic meaning of the\nnatural language query and semantic structure of the code. In recent years,\nboth natural language and programming language research communities have\ncreated techniques to embed them in vector spaces. In this work, we leverage\nthe efficacy of these embedding models using a simple, lightweight 2-layer\nneural network in the task of semantic code search. We show that our model\nlearns the inherent relationship between the embedding spaces and further\nprobes into the scope of improvement by empirically analyzing the embedding\nmethods. In this analysis, we show that the quality of the code embedding model\nis the bottleneck for our model's performance, and discuss future directions of\nstudy in this area.", "authors": ["Abdullah Al Ishtiaq", "Masum Hasan", "Md. Mahim Anjum Haque", "Kazi Sajeed Mehrab", "Tanveer Muttaqueen", "Tahmid Hasan", "Anindya Iqbal", "Rifat Shahriyar"], "categories": ["cs.SE", "cs.CL"], "published": "2021-04-16T10:28:27+00:00", "updated": "2021-04-16T10:28:27+00:00", "pdf_url": "http://arxiv.org/pdf/2104.08017v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2103.13020v3", "title": "deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search", "abstract": "With the rapid increase in the amount of public code repositories, developers\nmaintain a great desire to retrieve precise code snippets by using natural\nlanguage. Despite existing deep learning based approaches(e.g., DeepCS and\nMMAN) have provided the end-to-end solutions (i.e., accepts natural language as\nqueries and shows related code fragments retrieved directly from code corpus),\nthe accuracy of code search in the large-scale repositories is still limited by\nthe code representation (e.g., AST) and modeling (e.g., directly fusing the\nfeatures in the attention stage). In this paper, we propose a novel learnable\ndeep Graph for Code Search (calleddeGraphCS), to transfer source code into\nvariable-based flow graphs based on the intermediate representation technique,\nwhich can model code semantics more precisely compared to process the code as\ntext directly or use the syntactic tree representation. Furthermore, we propose\na well-designed graph optimization mechanism to refine the code representation,\nand apply an improved gated graph neural network to model variable-based flow\ngraphs. To evaluate the effectiveness of deGraphCS, we collect a large-scale\ndataset from GitHub containing 41,152 code snippets written in C language, and\nreproduce several typical deep code search methods for comparison. Besides, we\ndesign a qualitative user study to verify the practical value of our approach.\nThe experimental results have shown that deGraphCS can achieve state-of-the-art\nperformances, and accurately retrieve code snippets satisfying the needs of the\nusers.", "authors": ["Chen Zeng", "Yue Yu", "Shanshan Li", "Xin Xia", "Zhiming Wang", "Mingyang Geng", "Bailin Xiao", "Wei Dong", "Xiangke Liao"], "categories": ["cs.SE", "cs.AI"], "published": "2021-03-24T06:57:44+00:00", "updated": "2021-10-16T01:49:18+00:00", "pdf_url": "http://arxiv.org/pdf/2103.13020v3", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2103.12797v1", "title": "RPT: Effective and Efficient Retrieval of Program Translations from Big Code", "abstract": "Program translation is a growing demand in software engineering. Manual\nprogram translation requires programming expertise in source and target\nlanguage. One way to automate this process is to make use of the big data of\nprograms, i.e., Big Code. In particular, one can search for program\ntranslations in Big Code. However, existing code retrieval techniques are not\ndesigned for cross-language code retrieval. Other data-driven approaches\nrequire human efforts in constructing cross-language parallel datasets to train\ntranslation models. In this paper, we present RPT, a novel code translation\nretrieval system. We propose a lightweight but informative program\nrepresentation, which can be generalized to all imperative PLs. Furthermore, we\npresent our index structure and hierarchical filtering mechanism for efficient\ncode retrieval from a Big Code database.", "authors": ["Binger Chen", "Ziawasch Abedjan"], "categories": ["cs.SE", "cs.DB"], "published": "2021-03-23T19:01:43+00:00", "updated": "2021-03-23T19:01:43+00:00", "pdf_url": "http://arxiv.org/pdf/2103.12797v1", "primary_category": "cs.SE"}
{"id": "http://arxiv.org/abs/2102.12553v1", "title": "Refinement Type Directed Search for Meta-Interpretive-Learning of Higher-Order Logic Programs", "abstract": "The program synthesis problem within the Inductive Logic Programming (ILP)\ncommunity has typically been seen as untyped. We consider the benefits of user\nprovided types on background knowledge. Building on the Meta-Interpretive\nLearning (MIL) framework, we show that type checking is able to prune large\nparts of the hypothesis space of programs. The introduction of polymorphic type\nchecking to the MIL approach to logic program synthesis is validated by strong\ntheoretical and experimental results, showing a cubic reduction in the size of\nthe search space and synthesis time, in terms of the number of typed background\npredicates. Additionally we are able to infer polymorphic types of synthesized\nclauses and of entire programs. The other advancement is in developing an\napproach to leveraging refinement types in ILP. Here we show that further\npruning of the search space can be achieved, though the SMT solving used for\nrefinement type checking comes", "authors": ["Rolf Morel"], "categories": ["cs.AI", "cs.LO"], "published": "2021-02-18T13:40:16+00:00", "updated": "2021-02-18T13:40:16+00:00", "pdf_url": "http://arxiv.org/pdf/2102.12553v1", "primary_category": "cs.AI"}
{"id": "http://arxiv.org/abs/2102.05299v1", "title": "Searching CUDA code autotuning spaces with hardware performance counters: data from benchmarks running on various GPU architectures", "abstract": "We have developed several autotuning benchmarks in CUDA that take into\naccount performance-relevant source-code parameters and reach near\npeak-performance on various GPU architectures. We have used them during the\ndevelopment and evaluation of a novel search method for tuning space proposed\nin [1]. With our framework Kernel Tuning Toolkit, freely available at Github,\nwe measured computation times and hardware performance counters on several GPUs\nfor the complete tuning spaces of five benchmarks. These data, which we provide\nhere, might benefit research of search algorithms for the tuning spaces of GPU\ncodes or research of relation between applied code optimization, hardware\nperformance counters, and GPU kernels' performance.\n  Moreover, we describe the scripts we used for robust evaluation of our\nsearcher and comparison to others in detail. In particular, the script that\nsimulates the tuning, i.e., replaces time-demanding compiling and executing the\ntuned kernels with a quick reading of the computation time from our measured\ndata, makes it possible to inspect the convergence of tuning search over a\nlarge number of experiments. These scripts, freely available with our other\ncodes, make it easier to experiment with search algorithms and compare them in\na robust way.\n  During our research, we generated models for predicting values of performance\ncounters from values of tuning parameters of our benchmarks. Here, we provide\nthe models themselves and describe the scripts we implemented for their\ntraining. These data might benefit researchers who want to reproduce or build\non our research.", "authors": ["Ji\u0159\u00ed Filipovi\u010d", "Jana Hozzov\u00e1", "Amin Nezarat", "Jaroslav O\u013eha", "Filip Petrovi\u010d"], "categories": ["cs.DC", "cs.AI"], "published": "2021-02-10T07:51:09+00:00", "updated": "2021-02-10T07:51:09+00:00", "pdf_url": "http://arxiv.org/pdf/2102.05299v1", "primary_category": "cs.DC"}
{"id": "http://arxiv.org/abs/2101.07910v1", "title": "A Search-Based Testing Framework for Deep Neural Networks of Source Code Embedding", "abstract": "Over the past few years, deep neural networks (DNNs) have been continuously\nexpanding their real-world applications for source code processing tasks across\nthe software engineering domain, e.g., clone detection, code search, comment\ngeneration. Although quite a few recent works have been performed on testing of\nDNNs in the context of image and speech processing, limited progress has been\nachieved so far on DNN testing in the context of source code processing, that\nexhibits rather unique characteristics and challenges.\n  In this paper, we propose a search-based testing framework for DNNs of source\ncode embedding and its downstream processing tasks like Code Search. To\ngenerate new test inputs, we adopt popular source code refactoring tools to\ngenerate the semantically equivalent variants. For more effective testing, we\nleverage the DNN mutation testing to guide the testing direction. To\ndemonstrate the usefulness of our technique, we perform a large-scale\nevaluation on popular DNNs of source code processing based on multiple\nstate-of-the-art code embedding methods (i.e., Code2vec, Code2seq and\nCodeBERT). The testing results show that our generated adversarial samples can\non average reduce the performance of these DNNs from 5.41% to 9.58%. Through\nretraining the DNNs with our generated adversarial samples, the robustness of\nDNN can improve by 23.05% on average. The evaluation results also show that our\nadversarial test generation strategy has the least negative impact (median of\n3.56%), on the performance of the DNNs for regular test data, compared to the\nother methods.", "authors": ["Maryam Vahdat Pour", "Zhuo Li", "Lei Ma", "Hadi Hemmati"], "categories": ["cs.SE"], "published": "2021-01-20T00:40:44+00:00", "updated": "2021-01-20T00:40:44+00:00", "pdf_url": "http://arxiv.org/pdf/2101.07910v1", "primary_category": "cs.SE"}
