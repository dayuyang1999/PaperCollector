title,keywords,release_date,overall_summary,structure,methodology,example,abstract,arxiv
From Code to Play: Benchmarking Program Search for Games Using Large Language Models,"program synthesis, game AI, Python, Java, evolutionary search",2024-12,"This paper explores the potential of using large language models (LLMs) to directly synthesize usable code for a wide range of gaming applications in Python and Java. An evolutionary hill-climbing algorithm is used, where LLMs generate and refine the initial programs and mutations.",1. Introduction 2. Related Work 3. Framework Description 4. Large Language Models Used 5. Game Applications 5.1. Programmatic Policies: Minatar (Python) 5.2. Baba is You (Python) 5.3. Vehicle Driving (Python) 5.4. Maze Generation (Python) 5.5. Tabletop Games (Java) 6. Experiments and Results 7. Discussion 8. Conclusion,"['An evolutionary hill-climbing algorithm is used to search for programs', 'LLMs generate initial programs and perform mutations based on prompts', 'Programs are executed and evaluated in a subprocess environment', 'Prompts are updated based on program execution results to refine the programs', 'Process iterates until evaluation criteria are met or maximum attempts reached']","For the Seaquest game in Minatar, the LLM is prompted to generate a Python function that controls a submarine to save divers while shooting enemies. The function is executed, and the resulting score/reward is used to update the prompt. The LLM then refines the function based on this feedback. This process continues until a satisfactory strategy emerges or the maximum attempts are exhausted.","Large language models (LLMs) have shown impressive capabilities in generating
program code, opening exciting opportunities for applying program synthesis to
games. In this work, we explore the potential of LLMs to directly synthesize
usable code for a wide range of gaming applications, focusing on two
programming languages, Python and Java. We use an evolutionary hill-climbing
algorithm, where the mutations and seeds of the initial programs are controlled
by LLMs. For Python, the framework covers various game-related tasks, including
five miniature versions of Atari games, ten levels of Baba is You, an
environment inspired by Asteroids, and a maze generation task. For Java, the
framework contains 12 games from the TAG tabletop games framework. Across 29
tasks, we evaluated 12 language models for Python and 8 for Java. Our findings
suggest that the performance of LLMs depends more on the task than on model
size. While larger models generate more executable programs, these do not
always result in higher-quality solutions but are much more expensive. No model
has a clear advantage, although on any specific task, one model may be better.
Trying many models on a problem and using the best results across them is more
reliable than using just one.",http://arxiv.org/abs/2412.04057v1
Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search,"Isotropy, Code Search, Embedding Whitening, ZCA Whitening, Soft-ZCA",2024-11,"The paper investigates the impact of isotropy (uniform distribution of vectors) in embedding spaces on semantic code search performance. It proposes Soft-ZCA, a modified ZCA whitening technique to control isotropy levels, and demonstrates improved code search results across multiple programming languages.","1. Introduction (motivation and background) 2. Whitening of Embeddings (technical details of ZCA whitening and Soft-ZCA) 3. Experimental Apparatus (datasets, models, procedure) 4. Results (analysis of isotropy, ranking performance, impact of Soft-ZCA) 5. Conclusion 6. Appendix (additional experiments and details)","['- Analyze isotropy of embedding spaces in CodeBERT, CodeT5+, and Code Llama models using IsoScore metric', '- Introduce Soft-ZCA, an extension of ZCA whitening with an eigenvalue regularizer to control whitening degree', '- Fine-tune CodeBERT on code-comment pairs with contrastive loss', '- Apply Soft-ZCA whitening to embeddings and evaluate on code search task using MRR']","For the Python dataset, applying Soft-ZCA whitening with epsilon=0.0001 to the fine-tuned CodeBERT model increased the IsoScore from 0.555 (code) / 0.381 (comment) to 0.998 for both. This near-perfect isotropy resulted in an MRR improvement of +0.062 over the non-whitened embeddings.","Low isotropy in an embedding space impairs performance on tasks involving
semantic inference. Our study investigates the impact of isotropy on semantic
code search performance and explores post-processing techniques to mitigate
this issue. We analyze various code language models, examine isotropy in their
embedding spaces, and its influence on search effectiveness. We propose a
modified ZCA whitening technique to control isotropy levels in embeddings. Our
results demonstrate that Soft-ZCA whitening improves the performance of
pre-trained code language models and can complement contrastive fine-tuning.",http://arxiv.org/abs/2411.17538v2
Fault Localization from the Semantic Code Search Perspective,"semantic code search, fault localization, large language models, multi-granularity analysis, software knowledge base",2024-11,"This paper proposes CosFL, a novel approach to fault localization that treats it as a semantic code search problem. By leveraging large language models and constructing a multi-granularity software knowledge base, CosFL generates natural language queries describing buggy functionalities and retrieves semantically related code for locating faults.","{'1. Introduction': 'Motivates the idea of using code search for fault localization and outlines the main challenges.', '2. Motivation': 'Provides a case study illustrating how CosFL localizes a real bug from a code search perspective.', '3. Approach': 'Describes the two main components: software knowledge base construction and the query generation + fault retrieval pipeline.', '4. Implementation': 'Details on implementing the different components of CosFL.', '5. Experimental Setup': 'Describes the benchmarks, baselines, evaluation metrics used.', '6. Results and Analysis': 'Presents quantitative results, ablation studies, case studies comparing to baselines.', '7. Discussion': 'Discusses implications, limitations and future work.', '8. Related Work': 'Surveys related work in fault localization and code search.'}","['- Construct a multi-granularity software knowledge base by static/dynamic analysis and LLM comprehension', '- Use LLM to generate natural language queries describing buggy functionalities at different granularities', '- Retrieve code semantically related to the queries using multi-granularity code search', '- Employ a voting mechanism to pinpoint the final fault localization results']","For a bug in the Closure compiler where function arguments were incorrectly removed, CosFL generates the query 'not correctly handling the parameters of nested functions' and successfully retrieves the removeUnreferencedFunctionArgs method as the top result.","The software development process is characterized by an iterative cycle of
continuous functionality implementation and debugging, essential for the
enhancement of software quality and adaptability to changing requirements. This
process incorporates two isolatedly studied tasks: Code Search (CS), which
retrieves reference code from a code corpus to aid in code implementation, and
Fault Localization (FL), which identifies code entities responsible for bugs
within the software project to boost software debugging. These two tasks
exhibit similarities since they both address search problems. Notably, CS
techniques have demonstrated greater effectiveness than FL ones, possibly
because of the precise semantic details of the required code offered by natural
language queries, which are not readily accessible to FL methods. Drawing
inspiration from this, we hypothesize that a fault localizer could achieve
greater proficiency if semantic information about the buggy methods were made
available. Based on this idea, we propose CosFL, an FL approach that decomposes
the FL task into two steps: query generation, which describes the functionality
of the problematic code in natural language, and fault retrieval, which uses CS
to find program elements semantically related to the query. Specifically, to
depict the buggy functionalities and generate high-quality queries, CosFL
extensively harnesses the code analysis, semantic comprehension, and
decision-making capabilities of LLMs. Moreover, to enhance the accuracy of CS,
CosFL captures varying levels of context information and employs a
multi-granularity code search strategy, which facilitates a more precise
identification of buggy methods from a holistic view. The evaluation on 835
real bugs from 23 Java projects shows that CosFL successfully localizes 324
bugs within Top-1, which significantly outperforms the state-of-the-art
approaches by 26.6%-57.3%.",http://arxiv.org/abs/2411.17230v1
CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval,"code retrieval, multimodal embeddings, unified training, programming languages, code-related tasks",2024-11,"This paper introduces CodeXEmbed, a family of large-scale embedding models tailored for code and text retrieval across multiple programming languages and code-related tasks. The unified training approach achieves state-of-the-art performance on code retrieval benchmarks.","1. Introduction (motivation and background)
2. Method (training approach, retrieval settings)
3. Experiments (benchmarks, implementation details, results)
4. Conclusion","['Unified training framework that converts diverse code tasks into retrieval problems (text-to-code, code-to-text, code-to-code, hybrid)', 'Contrastive loss to maximize similarity between query and correct answer while minimizing similarity to negative samples', 'Models trained on code/text data ranging from 400M to 7B parameters']","For the code-to-text retrieval setting, the task of code summarization is used as an example. Given a code file or repository as input, the goal is to retrieve a concise textual summary describing the code's functionality.","Despite the success of text retrieval in many NLP tasks, code retrieval
remains a largely underexplored area. Most text retrieval systems are tailored
for natural language queries, often neglecting the specific challenges of
retrieving code. This gap leaves existing models unable to effectively capture
the diversity of programming languages and tasks across different domains,
highlighting the need for more focused research in code retrieval. To address
this, we introduce CodeXEmbed, a family of large-scale code embedding models
ranging from 400M to 7B parameters. Our novel training pipeline unifies
multiple programming languages and transforms various code-related tasks into a
common retrieval framework, enhancing model generalizability and retrieval
performance. Our 7B model sets a new state-of-the-art (SOTA) in code retrieval,
outperforming the previous leading model, Voyage-Code, by over 20% on CoIR
benchmark. In addition to excelling in code retrieval, our models demonstrate
competitive performance on the widely adopted BeIR text retrieval benchmark,
offering versatility across domains. Experimental results demonstrate that
improving retrieval performance significantly enhances end-to-end
Retrieval-Augmented Generation (RAG) performance for code-related tasks.",http://arxiv.org/abs/2411.12644v2
SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation,"self-driven reasoning, monte carlo tree search, code generation, data augmentation, chain-of-thought",2024-11,"The paper proposes SRA-MCTS, a self-driven reasoning augmentation method that guides language models to autonomously generate high-quality reasoning paths for complex code generation tasks. By synthesizing natural language plans and translating them to code, SRA-MCTS improves performance without additional supervision.","1. Introduction - Motivation and background
2. Related Work - Prior approaches like data distillation, chain-of-thought
3. Method - SRA-MCTS algorithm details
4. Experiments - Setup, baselines, datasets, evaluation
5. Results and Analysis","['Use Monte Carlo Tree Search (MCTS) to guide an LLM to generate diverse reasoning plans in natural language for code problems', 'Evaluate and select high-quality plans based on correctness, completeness and coherence scores from the LLM itself', 'Use the LLM to translate the selected reasoning plans into executable code', 'Fine-tune the LLM on the generated (problem, reasoning plan, code) triples']","For the LeetCode problem 'Longest Palindromic Substring', SRA-MCTS may generate a reasoning plan like:
1. Iterate through the string to find potential palindromes
2. For each potential palindrome, check if it is a valid palindrome
3. Keep track of the longest valid palindrome found
4. Return the longest palindrome
The LLM then converts this into Python code based on the reasoning plan.","Large language models demonstrate exceptional performance in simple code
generation tasks but still face challenges in tackling complex problems. These
challenges may stem from insufficient reasoning and problem decomposition
capabilities. To address this issue, we propose a reasoning-augmented data
generation process, SRA-MCTS, which guides the model to autonomously generate
high-quality intermediate reasoning paths. This creates a positive feedback
loop, enabling continuous improvement. Our method operates entirely through the
model itself without requiring additional supervision. By synthesizing natural
language reasoning paths and translating them into executable code, the
approach ensures analytical accuracy and enhances the success rate in solving
complex tasks. Experimental results show that, even without additional
supervisory signals, our method achieves performance improvements across
different model scales, demonstrating the significant potential of
self-improvement in small models. Furthermore, the method remains robust when
traditional Chain-of-Thought (CoT) approaches exhibit performance degradation,
with notable improvements observed in diversity metrics such as pass@10. We
encourage further exploration of reasoning processes within training data to
enhance the ability of language models to address complex problems. Our code
and data are public at https://github.com/DIRECT-BIT/SRA-MCTS.",http://arxiv.org/abs/2411.11053v4
Searching Latent Program Spaces,"latent program space, gradient-based search, program induction, test-time adaptation, ARC-AGI benchmark",2024-11,"This paper introduces the Latent Program Network (LPN), a neural architecture that learns a continuous latent space representing programs. LPN enables efficient search and test-time adaptation in this latent space to solve program synthesis tasks, outperforming methods without such adaptation on the challenging ARC-AGI benchmark.","1. Introduction - Motivation and background on program synthesis 2. Related Work - Existing symbolic, neural, and neuro-symbolic approaches 3. Background - Formal problem definition 4. Latent Program Network - Proposed model architecture and training 5. Experiments - Evaluation on ARC-AGI and ablation studies 6. Conclusion","['Learn a continuous latent space encoding programs using a variational autoencoder framework', 'Train encoder to map input-output pairs to latent codes, and decoder to execute latent codes on new inputs', 'Novel training objective prevents encoding outputs directly in latent space', 'Perform gradient-based search in latent space during training and inference to find program best explaining specification', 'Use test-time adaptation by optimizing latent code to solve new tasks']","For an ARC-AGI task with specification showing how to draw a line segment, LPN would: 1) Encode the line drawing examples to a latent code 2) Optimize this code to best reconstruct the line outputs 3) Execute the optimized code on a new input to predict how to extend the line segment","Program synthesis methods aim to automatically generate programs restricted
to a language that can explain a given specification of input-output pairs.
While purely symbolic approaches suffer from a combinatorial search space,
recent methods leverage neural networks to learn distributions over program
structures to narrow this search space significantly, enabling more efficient
search. However, for challenging problems, it remains difficult to train models
to perform program synthesis in one shot, making test-time search essential.
Most neural methods lack structured search mechanisms during inference, relying
instead on stochastic sampling or gradient updates, which can be inefficient.
In this work, we propose the Latent Program Network (LPN), a general algorithm
for program induction that learns a distribution over latent programs in a
continuous space, enabling efficient search and test-time adaptation. We
explore how to train these networks to optimize for test-time computation and
demonstrate the use of gradient-based search both during training and at test
time. We evaluate LPN on ARC-AGI, a program synthesis benchmark that evaluates
performance by generalizing programs to new inputs rather than explaining the
underlying specification. We show that LPN can generalize beyond its training
distribution and adapt to unseen tasks by utilizing test-time computation,
outperforming algorithms without test-time adaptation mechanisms.",http://arxiv.org/abs/2411.08706v1
Automatically Write Code Checker: An LLM-based Approach with Logic-guided API Retrieval and Case by Case Iteration,"iterative test-driven checker generation, logic-guided API retrieval, static code analysis, large language model, automated code generation",2024-11,"This paper proposes AutoChecker, an approach that leverages large language models to automatically generate static code checkers based on a rule description and test suite. It uses an iterative generation process guided by individual test cases to handle complex checking logic, and employs logic-guided API retrieval to provide relevant API knowledge for the code generation.",1. Introduction 2. Background and Motivation 3. Methodology 3.1 Overview 3.2 API-Context Retriever 3.3 Checker Generator 3.3.1 Initial Checker Generation 3.3.2 Iterative Checker Generation 4. Experimental Setup 5. Experimental Results 5.1 Effectiveness Evaluation 5.2 Practicality Evaluation 6. Discussion 7. Related Work 8. Conclusion,"['- Iterative test-driven checker generation to handle complex logic by updating the checker with one test case at a time', '- Logic-guided API retrieval to extract relevant API contexts for each sub-operation in the checking logic', '- Construction of a Meta-Op database containing 354 atomic checking operations to guide API retrieval', '- Use of an initial checker generator and an iterative checker generator leveraging the retrieved API contexts']","For the PMD rule 'AssignmentToNonFinalStaticRule', AutoChecker generates a checker that visits all assignment expressions within constructors and checks if the assigned identifier is a static and non-final field. It retrieves relevant APIs like ASTVariableAccess, ASTFieldAccess, and methods to check symbol properties, enabling correct implementation of the logic.","With the rising demand for code quality assurance, developers are not only
utilizing existing static code checkers but also seeking custom checkers to
satisfy their specific needs. Nowadays, various code-checking frameworks
provide extensive checker customization interfaces to meet this need. However,
both the abstract checking logic as well as the complex API usage of
large-scale frameworks make this task challenging. To this end, automated code
checker generation is anticipated to ease the burden of checker development. In
this paper, we explore the feasibility of automated checker generation and
propose AutoChecker, an innovative LLM-powered approach that can write code
checkers automatically based on only a rule description and a test suite.
Instead of generating the checker at once, AutoChecker incrementally updates
the checker with the rule and one single test case each time, i.e., it
iteratively generates the checker case by case. During each iteration,
AutoChecker first decomposes the whole logic into a series of sub-operations
and then uses the logic-guided API-context retrieval strategy to search related
API-contexts from all the framework APIs. To evaluate the effectiveness of
AutoChecker, we apply AutoChecker and two LLM-based baseline approaches to
automatically generate checkers for 20 built-in PMD rules, including easy rules
and hard rules. Experimental results demonstrate that AutoChecker significantly
outperforms baseline approaches across all effectiveness metrics, where its
average test pass rate improved over 4.2 times. Moreover, the checkers
generated by AutoChecker are successfully applied to real-world projects,
matching the performance of official checkers.",http://arxiv.org/abs/2411.06796v1
Assessing the Answerability of Queries in Retrieval-Augmented Code Generation,"answerability assessment, retrieval-augmented code generation, hallucination mitigation, benchmark dataset, few-shot learning",2024-11,"This paper proposes a new task of assessing the answerability of queries in retrieval-augmented code generation (RaCG) systems, where a language model generates code based on a user query and retrieved API documentation. It introduces a benchmark dataset called RaCGEval to evaluate models on this task.",1. Introduction 2. Constructing the RaCGEval Benchmark Dataset 2.1 Task description 2.2 API documentation 2.3 Generating partially answerable and unanswerable samples 2.4 Annotation 3. Methods for Assessing Answerability 3.1 Zero-shot inference on instruction-following LLMs 3.2 Fine-tuning LLMs with automatically generated training datasets 4. Experiments 4.1 Experimental settings 4.2 Experimental results 5. Discussion 5.1 In-context learning for domain adaptation 5.2 Trade-off between coverage and precision 6. Conclusion,"['Use private/modified API documentation datasets to avoid prior knowledge in language models', 'Generate partially answerable and unanswerable samples by substituting gold APIs, concatenating unrelated queries, or using out-of-database queries', 'Annotate samples using multiple experts to ensure reliability', 'Evaluate zero-shot inference on instruction-tuned LLMs', 'Fine-tune LLMs on automatically generated training data using CoNaLa dataset', 'Explore in-context learning for domain adaptation', 'Analyze trade-off between coverage and precision of code generation']","For the query 'How to create a web page using NetsPresso API?' and retrieved NetsPresso API documentation on model optimization, the language model may generate plausible but incorrect code, since web page creation is out of scope for this API. The system should identify this query as unanswerable based on the retrieved documentation.","Thanks to unprecedented language understanding and generation capabilities of
large language model (LLM), Retrieval-augmented Code Generation (RaCG) has
recently been widely utilized among software developers. While this has
increased productivity, there are still frequent instances of incorrect codes
being provided. In particular, there are cases where plausible yet incorrect
codes are generated for queries from users that cannot be answered with the
given queries and API descriptions. This study proposes a task for evaluating
answerability, which assesses whether valid answers can be generated based on
users' queries and retrieved APIs in RaCG. Additionally, we build a benchmark
dataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to
evaluate the performance of models performing this task. Experimental results
show that this task remains at a very challenging level, with baseline models
exhibiting a low performance of 46.7%. Furthermore, this study discusses
methods that could significantly improve performance.",http://arxiv.org/abs/2411.05547v2
RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval,"Code-mixing, Roman transliteration, Bengali language, Prompting, Sequential relevance",2024-11,This paper presents a novel approach to extract relevant information from code-mixed conversations in Roman transliterated Bengali mixed with English. It leverages GPT-3.5 Turbo via prompting and integrates the model's outputs into a mathematical model that accounts for sequential dependencies among documents.,"1. Introduction, 2. Related Work, 3. Dataset, 4. Task Definition, 5. Methodology (5.1 Why Prompting?, 5.2 Merging Prompt and Mathematical Model-Based Approaches), 6. Results, 7. Conclusion","['Used GPT-3.5 Turbo model via prompting to obtain relevance scores for documents given a query', 'Designed a mathematical model that incorporates sequential dependencies among documents to refine relevance probabilities', 'If previous document is relevant and current score >= 0.3, probability = 0.2 + current score', 'If previous document is relevant and current score < 0.3, probability = current score', 'Considered documents with probability > 0.5 as relevant to the query']","For the query 'Where can I find good Bengali food in Delhi?', the system would analyze a sequence of documents from a Facebook group like 'Bengali in Delhi'. It would use GPT-3.5's understanding of the query and each document to get relevance scores. The mathematical model would then boost the scores for documents that follow a relevant one in the sequence. Highly scored documents recommending Bengali restaurants would be retrieved as the most relevant results.","Code-mixing, the integration of lexical and grammatical elements from
multiple languages within a single sentence, is a widespread linguistic
phenomenon, particularly prevalent in multilingual societies. In India, social
media users frequently engage in code-mixed conversations using the Roman
script, especially among migrant communities who form online groups to share
relevant local information. This paper focuses on the challenges of extracting
relevant information from code-mixed conversations, specifically within Roman
transliterated Bengali mixed with English. This study presents a novel approach
to address these challenges by developing a mechanism to automatically identify
the most relevant answers from code-mixed conversations. We have experimented
with a dataset comprising of queries and documents from Facebook, and Query
Relevance files (QRels) to aid in this task. Our results demonstrate the
effectiveness of our approach in extracting pertinent information from complex,
code-mixed digital conversations, contributing to the broader field of natural
language processing in multilingual and informal text environments. We use
GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant
documents to frame a mathematical model which helps to detect relevant
documents corresponding to a query.",http://arxiv.org/abs/2411.04752v1
CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models,"tree-based code generation, multi-agent collaboration, strategy exploration, code refinement, execution feedback",2024-11,"The paper proposes CodeTree, a framework that uses multiple collaborating language model agents to efficiently explore the search space for code generation through a tree-based structure. Agents generate coding strategies, initial solutions, refinements, and evaluations, guided by execution feedback and critic scoring to find optimal code.","1. Introduction 2. Related Work 3. Method: 3.1 Coding Task Agents (Thinker, Solver, Debugger), 3.2 Tree Expanding with Critic Agent, 3.3 Multi-Agent Collaboration, 3.4 Training 4. Experiments 5. Results 6. Analysis 7. Conclusion","['Define Thinker, Solver, Debugger agents for strategy generation, initial coding, refinement', 'Build tree with root as problem, nodes as candidate solutions', 'Critic agent scores nodes based on execution feedback and strategy match', 'Critic expands tree by refining, aborting or accepting nodes based on scores', 'Agents collaborate, passing intermediate solutions and getting critic feedback']","For the problem of 'reversing a string', the Thinker may generate strategies like 'use slicing' or 'convert to list'. The Solver generates initial code attempts for each strategy. The Debugger refines code with Critic feedback on test cases. The tree expands promising nodes until an acceptable solution is found.","Pre-trained on massive amounts of code and text data, large language models
(LLMs) have demonstrated remarkable achievements in performing code generation
tasks. With additional execution-based feedback, these models can act as agents
with capabilities to self-refine and improve generated code autonomously.
However, on challenging coding tasks with extremely large search space, current
agentic approaches still struggle with multi-stage planning, generating, and
debugging. To address this problem, we propose CodeTree, a framework for LLM
agents to efficiently explore the search space in different stages of the code
generation process. Specifically, we adopted a unified tree structure to
explicitly explore different coding strategies, generate corresponding coding
solutions, and subsequently refine the solutions. In each stage, critical
decision-making (ranking, termination, expanding) of the exploration process is
guided by both the environmental execution-based feedback and
LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code
generation benchmarks and demonstrated the significant performance gains of
CodeTree against strong baselines. Using GPT-4o as the base model, we
consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0
on CodeContests. On the challenging SWEBench benchmark, our approach led to
significant performance gains.",http://arxiv.org/abs/2411.04329v2
BinEnhance: An Enhancement Framework Based on External Environment Semantics for Binary Code Search,"external environment semantics, binary code search, semantic enhancement, relational graph convolutional networks, function inlining",2024-11,"The paper proposes BinEnhance, a framework that leverages external environment semantics to enhance internal code semantics for improving binary code search. It constructs an External Environment Semantic Graph (EESG) and uses a Semantic Enhancement Model with Relational Graph Convolutional Networks to incorporate external semantics into function embeddings.",1. Introduction 2. Preliminaries 3. Design of BinEnhance 4. Evaluation 5. Related Work 6. Conclusion,"['Construct an External Environment Semantic Graph (EESG) with four edge types (call-dependency, data-co-use, address-adjacency, string-use) to model external environment', 'Initialize EESG node embeddings using existing internal code semantic models', 'Use Relational Graph Convolutional Networks (RGCNs) in a Semantic Enhancement Model to update node embeddings with external semantics', 'Merge internal and external embeddings via a residual block to get enhanced embeddings', 'Combine semantic similarity with data feature similarity for final ranking']","For the 'transfer' function compiled with different optimization levels (O0 and O3), BinEnhance's EESG remains largely stable despite function inlining changes to the internal code, allowing it to still match the two versions based on the enhanced embeddings capturing external semantics.","Binary code search plays a crucial role in applications like software reuse
detection. Currently, existing models are typically based on either internal
code semantics or a combination of function call graphs (CG) and internal code
semantics. However, these models have limitations. Internal code semantic
models only consider the semantics within the function, ignoring the
inter-function semantics, making it difficult to handle situations such as
function inlining. The combination of CG and internal code semantics is
insufficient for addressing complex real-world scenarios. To address these
limitations, we propose BinEnhance, a novel framework designed to leverage the
inter-function semantics to enhance the expression of internal code semantics
for binary code search. Specifically, BinEnhance constructs an External
Environment Semantic Graph (EESG), which establishes a stable and analogous
external environment for homologous functions by using different inter-function
semantic relations (e.g., call, location, data-co-use). After the construction
of EESG, we utilize the embeddings generated by existing internal code semantic
models to initialize nodes of EESG. Finally, we design a Semantic Enhancement
Model (SEM) that uses Relational Graph Convolutional Networks (RGCNs) and a
residual block to learn valuable external semantics on the EESG for generating
the enhanced semantics embedding. In addition, BinEnhance utilizes data feature
similarity to refine the cosine similarity of semantic embeddings. We conduct
experiments under six different tasks (e.g., under function inlining scenario)
and the results illustrate the performance and robustness of BinEnhance. The
application of BinEnhance to HermesSim, Asm2vec, TREX, Gemini, and Asteria on
two public datasets results in an improvement of Mean Average Precision (MAP)
from 53.6% to 69.7%. Moreover, the efficiency increases fourfold.",http://arxiv.org/abs/2411.01102v3
Are Decoder-Only Large Language Models the Silver Bullet for Code Search?,"decoder-only language models, code search, fine-tuning, zero-shot performance, model generalization",2024-10,"This paper systematically explores the use of decoder-only large language models (LLMs) for code search tasks. It evaluates nine state-of-the-art decoder-only LLMs using different fine-tuning methods, datasets, and model sizes, finding that fine-tuned CodeGemma significantly outperforms encoder-only models like UniXcoder in code search.",1. Introduction 2. Background 2.1 Code Search 2.2 Decoder-only LLMs for Information Retrieval 3. Study Setup 3.1 Benchmark Datasets 3.2 Evaluation Metrics 3.3 Models 4. Results and Analysis 5. Conclusion,"['Evaluated 9 decoder-only LLMs (e.g., CodeGemma, CodeLlama, DeepSeekCoder) and 2 encoder-only models (CodeBERT, UniXcoder) on code search tasks', 'Used two datasets: CodeSearchNet (CSN) covering multiple languages, and CoSQA+ for Python', 'Employed two fine-tuning methods and three model sizes for comprehensive analysis', 'Measured performance using Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) metrics']","On the CoSQA+ dataset, fine-tuned CodeGemma achieved a 49.6% increase in MAP and a 41.3% improvement in MRR compared to UniXcoder, demonstrating superior generalization to new datasets even without explicit training on them.","Code search is crucial for code reuse, enabling developers to efficiently
locate relevant snippets. Current methods rely on encoder-based models, which
suffer from limitations such as poor generalization and restricted input
lengths. Decoder-only large language models (LLMs), with their extensive
pre-training, larger size, and longer input capabilities, offer potential
solutions to these issues, yet their effectiveness in code search remains
underexplored. To fill this gap, our study presents the first systematic
exploration of decoder-only LLMs for code search. We evaluate nine
state-of-the-art decoder-only models using two fine-tuning methods, two
datasets (CSN and CoSQA$^+$), and three model sizes. Our findings reveal that
fine-tuned CodeGemma significantly outperforms encoder-only models like
UniXcoder, achieving a 5.57% improvement in MRR on CSN and a 49.6% increase in
MAP on CoSQA$^+$ compared to zero-shot UniXcoder. These results highlight the
superior performance and adaptability of decoder-only models. Additionally, we
provide valuable insights into optimizing these models for code search,
covering aspects such as model selection, fine-tuning methods, training data,
and model size, and discussing their strengths and limitations.",http://arxiv.org/abs/2410.22240v1
Semantic-guided Search for Efficient Program Repair with Large Language Models,"semantic-guided search, greedy decoding, test validation feedback, memory-efficient program repair, large language model fine-tuning",2024-10,"This paper introduces FLAMES, a novel approach for efficient and effective automated program repair using large language models (LLMs). FLAMES employs semantic-guided search with test validation feedback to steer LLM-based patch generation, improving repair performance while significantly reducing memory consumption.",1. Introduction 2. Motivation Study 3. FLAMES Approach 4. Experimental Evaluation 5. Related Work 6. Conclusion,"['- Uses greedy decoding (beam size 1) for initial patch generation instead of beam search', '- Employs best-first search algorithm (PG-TD) with semantic feedback from test validations to refine patches', '- Combines LLM-based and search-based program repair techniques', '- Fine-tunes large language models (1B-7B parameters) on program repair datasets']","For a buggy Java program with failing test cases, FLAMES first generates initial patch candidates using greedy decoding from a fine-tuned LLM. It then iteratively refines these candidates using a best-first search guided by semantic feedback from test case execution, such as the number of passing/failing tests for each candidate. This allows FLAMES to efficiently explore the search space and prioritize more promising patch candidates.","In this paper, we first show that increases in beam size of even just
small-sized LLM (1B-7B parameters) require an extensive GPU resource
consumption, leading to up to 80% of recurring crashes due to memory overloads
in LLM-based APR. Seemingly simple solutions to reduce memory consumption are
(1) to quantize LLM models, i.e., converting the weights of a LLM from
high-precision values to lower-precision ones. and (2) to make beam search
sequential, i.e., forwarding each beam through the model sequentially and then
concatenate them back into a single model output. However, we show that these
approaches still do not work via both theoretical analysis and experiments. To
address this, we introduce FLAMES, a novel LLM-based APR technique that employs
semantic-guided patch generation to enhance repair effectiveness and memory
efficiency. Unlike conventional methods that rely on beam search, FLAMES
utilizes greedy decoding to enhance memory efficiency while steering the search
to more potentially good repair candidates via a semantic-guided best-first
search algorithm. At each decoding step, FLAMES uses semantic feedback from
test validation such as the number of passing and failing test cases to select
the most promising token to explore further. Our empirical evaluation on the
Defects4J and HumanEval-Java datasets shows that FLAMES not only substantially
reduces memory consumption by up to 83% compared to conventional LLM-based APR,
but also accelerates the repair process. Remarkably, FLAMES successfully
generated 133 and 103 correct fixes for 333 and 163 bugs in the Defects4J and
HumanEval-Java datasets, respectively. This suggests that FLAMES is not only
more efficient but also outperforms state-of-the-art techniques, fixing at
least 10 and 11 more bugs than SOTA baselines in the Defects4J and
HumanEval-Java datasets, respectively.",http://arxiv.org/abs/2410.16655v1
Scattered Forest Search: Smarter Code Space Exploration with LLMs,"scattered forest search, code space exploration, textual optimization directions, multi-start optimization, ant colony optimization",2024-10,The paper proposes a novel approach called Scattered Forest Search (SFS) to enhance the exploration and exploitation of language models for code generation tasks. SFS frames code generation as a black-box optimization problem and employs techniques inspired by optimization methods to efficiently search the code space.,1. Introduction 2. Background 3. Methodology (describing SFS techniques) 4. Theoretical Perspective 5. Experiments 6. Conclusion,"['- Scattering: Dynamically vary input prompts to LLM to generate diverse textual optimization directions for new solutions', '- Foresting: Perform tree search from multiple random seed solutions to enhance exploration breadth', '- Scouting: Share feedback on successful/unsuccessful directions across search branches to guide exploitation']","For the task of finding min/max k elements from a tuple: 1) Scattering prompts LLM to propose directions like 'Modify return statement' or 'Handle k > len(tuple)' 2) Each child branch follows one direction to generate a new solution 3) Scouting updates global memory on whether that direction worked, guiding future branches","We propose a novel approach to scaling LLM inference for code generation. We
frame code generation as a black box optimization problem within the code
space, and employ optimization-inspired techniques to enhance exploration.
Specifically, we introduce Scattered Forest Search to enhance solution
diversity while searching for solutions. Our theoretical analysis illustrates
how these methods avoid local optima during optimization. Extensive experiments
on HumanEval, MBPP, APPS, CodeContests, and Leetcode reveal significant
performance improvements. For instance, our method achieves a pass@1 rate of
67.1% on HumanEval+ and 87.2% on HumanEval with GPT-3.5, marking improvements
of 8.6% and 4.3% over the state-of-the-art, while also halving the iterations
needed to find the correct solution. Furthermore, our method scales more
efficiently than existing search techniques, including tree search, line
search, and repeated sampling.",http://arxiv.org/abs/2411.05010v1
Building A Coding Assistant via the Retrieval-Augmented Language Model,"code structure-aware retrieval, dual-view code representation, retrieval-augmented code generation, code-documentation alignment, masked entity prediction",2024-10,"This paper proposes CONAN, a framework for building a coding assistant that mimics human knowledge-seeking behaviors during coding. It consists of a code structure-aware retriever (CONAN-R) and a dual-view code representation-based retrieval-augmented generation model (CONAN-G).",1. Introduction 2. Related Work 3. Methodology 3.1 Preliminary 3.2 Retrieval Module (CONAN-R) 3.3 Generation Module (CONAN-G) 3.4 Assisting LLMs 4. Experiments 5. Analysis 6. Conclusion,"['- CONAN-R pretrains CodeT5 using Code-Documentation Alignment (CDA) and Masked Entity Prediction (MEP) tasks to learn code structure-aware representations', '- CDA aligns matched code-documentation pairs to bridge modality gap', '- MEP masks code entities and predicts them to capture code semantics', '- CONAN-G uses Fusion-in-Decoder (FID) to incorporate multiple retrieved code snippets', '- CONAN-G treats documentation as prompts to better understand code semantics']","For code generation, CONAN takes a natural language description as input, retrieves relevant code snippets and documentation using CONAN-R, and then uses the retrieved information along with the description in CONAN-G to generate the target code.","Pretrained language models have shown strong effectiveness in code-related
tasks, such as code retrieval, code generation, code summarization, and code
completion tasks. In this paper, we propose COde assistaNt viA
retrieval-augmeNted language model (CONAN), which aims to build a code
assistant by mimicking the knowledge-seeking behaviors of humans during coding.
Specifically, it consists of a code structure aware retriever (CONAN-R) and a
dual-view code representation-based retrieval-augmented generation model
(CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and
Masked Entity Prediction tasks to make language models code structure-aware and
learn effective representations for code snippets and documentation. Then
CONAN-G designs a dual-view code representation mechanism for implementing a
retrieval-augmented code generation model. CONAN-G regards the code
documentation descriptions as prompts, which help language models better
understand the code semantics. Our experiments show that CONAN achieves
convincing performance on different code generation tasks and significantly
outperforms previous retrieval augmented code generation models. Our further
analyses show that CONAN learns tailored representations for both code snippets
and documentation by aligning code-documentation data pairs and capturing
structural semantics by masking and predicting entities in the code data.
Additionally, the retrieved code snippets and documentation provide necessary
information from both program language and natural language to assist the code
generation process. CONAN can also be used as an assistant for Large Language
Models (LLMs), providing LLMs with external knowledge in shorter code document
lengths to improve their effectiveness on various code tasks. It shows the
ability of CONAN to extract necessary information and help filter out the noise
from retrieved code documents.",http://arxiv.org/abs/2410.16229v2
Instructive Code Retriever: Learn from Large Language Model's Feedback for Code Intelligence Tasks,"contrastive learning, syntax tree encoding, code retrieval, in-context learning, large language model feedback",2024-10,"This paper proposes a novel approach called Instructive Code Retriever (ICR) that leverages large language model feedback and syntax tree encoding to retrieve high-quality examples for enhancing in-context learning performance on code intelligence tasks like summarization, synthesis, and bug fixing.",1. Introduction 2. Motivation 3. Instructive Code Retriever 3.1 Overview 3.2 Preprocessing and Preparation 3.3 Retrieval Model 3.4 Training 4. Experiments 4.1 Experimental Setup 4.2 Results 5. Related Work 6. Conclusion,"['Use syntax trees to encode structural information of code and natural language', 'Compute tree-based similarity between query and examples using tree encodings', 'Use large language model to score examples based on their helpfulness for the task', 'Train retriever with contrastive loss to distinguish good vs bad examples based on LLM feedback', 'During inference, retrieve top examples using trained ICR and use them as prompts for LLM']","For the program synthesis task, the query could be 'Write a function to adjust log10 to handle values less or equal to zero'. ICR would retrieve a training example like: 

Input: 'Write a function to adjust squareRoot to handle negative values'
Output: 'public static double squareRoot(double val) { ... }'

Despite textual differences, the syntax trees indicate this is a good example for adjusting a math function, so ICR would retrieve it to help the LLM generate the desired log10 function.","Recent studies proposed to leverage large language models (LLMs) with
In-Context Learning (ICL) to handle code intelligence tasks without
fine-tuning. ICL employs task instructions and a set of examples as
demonstrations to guide the model in generating accurate answers without
updating its parameters. While ICL has proven effective for code intelligence
tasks, its performance heavily relies on the selected examples. Previous work
has achieved some success in using BM25 to retrieve examples for code
intelligence tasks. However, existing approaches lack the ability to understand
the semantic and structural information of queries, resulting in less helpful
demonstrations. Moreover, they do not adapt well to the complex and dynamic
nature of user queries in diverse domains. In this paper, we introduce a novel
approach named Instructive Code Retriever (ICR), which is designed to retrieve
examples that enhance model inference across various code intelligence tasks
and datasets. We enable ICR to learn the semantic and structural information of
the corpus by a tree-based loss function. To better understand the correlation
between queries and examples, we incorporate the feedback from LLMs to guide
the training of the retriever. Experimental results demonstrate that our
retriever significantly outperforms state-of-the-art approaches. We evaluate
our model's effectiveness on various tasks, i.e., code summarization, program
synthesis, and bug fixing. Compared to previous state-of-the-art algorithms,
our method achieved improvements of 50.0% and 90.0% in terms of BLEU-4 for two
code summarization datasets, 74.6% CodeBLEU on program synthesis dataset, and
increases of 3.6 and 3.2 BLEU-4 on two bug fixing datasets.",http://arxiv.org/abs/2410.11300v1
Exploring Demonstration Retrievers in RAG for Coding Tasks: Yeas and Nays!,"approximate nearest neighbor search, retrieval efficiency, code generation tasks, demonstration retrieval, retrieval-augmented generation",2024-10,"This paper systematically evaluates the efficiency-effectiveness trade-off of different retrievers (sparse and dense) in retrieval-augmented generation (RAG) for coding tasks like program synthesis, commit message generation, and assertion generation. It finds that while sparse retrievers like BM25 excel in effectiveness for small knowledge bases, approximate dense retrievers like HNSW offer significant efficiency gains with minimal quality drop for large-scale retrieval.","1. Introduction 2. Background (RAG, retrieval approaches) 3. Experimental Design (research questions, datasets, retrievers, metrics) 4. Results (effectiveness, efficiency, trade-off analysis) 5. Discussion 6. Conclusion","['Evaluated 6 retrievers: 2 sparse (BM25, BM25L), 1 exhaustive dense (SBERT Semantic Search), 3 approximate dense (ANNOY, LSH, HNSW)', 'Used SBERT embeddings to encode queries and demonstrations', 'Approximate retrievers build indexes to retrieve subset of neighbors instead of full scan', 'Compared retrieval time and output quality metrics across tasks and knowledge base sizes']","For the commit message generation task with a large knowledge base, the HNSW approximate dense retriever achieved a 44x speedup over BM25 with only a 1.74% drop in RougeL score. This demonstrates HNSW's ability to provide an efficient-effective retrieval trade-off for RAG in large-scale coding tasks.","Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
integrating external knowledge bases, achieving state-of-the-art results in
various coding tasks. The core of RAG is retrieving demonstration examples,
which is essential to balance effectiveness (generation quality) and efficiency
(retrieval time) for optimal performance. However, the high-dimensional nature
of code representations and large knowledge bases often create efficiency
bottlenecks, which are overlooked in previous research. This paper
systematically evaluates the efficiency-effectiveness trade-off of retrievers
across three coding tasks: Program Synthesis, Commit Message Generation, and
Assertion Generation. We examined six retrievers: two sparse (BM25 and BM25L)
and four dense retrievers, including one exhaustive dense retriever (SBERT's
Semantic Search) and three approximate dense retrievers (ANNOY, LSH, and HNSW).
Our findings show that while BM25 excels in effectiveness, it suffers in
efficiency as the knowledge base grows beyond 1000 entries. In large-scale
retrieval, efficiency differences become more pronounced, with approximate
dense retrievers offering the greatest gains. For instance, in Commit
Generation task, HNSW achieves a 44x speed up, while only with a 1.74% drop in
RougeL compared with BM25. Our results also show that increasing the number of
demonstrations in the prompt doesn't always improve the effectiveness and can
increase latency and lead to incorrect outputs. Our findings provide valuable
insights for practitioners aiming to build efficient and effective RAG systems
for coding tasks.",http://arxiv.org/abs/2410.09662v1
Approaching Code Search for Python as a Translation Retrieval Problem with Dual Encoders,"unified language model, dual encoders, cosine similarity loss, FastText embeddings, code search for Python",2024-10,"This paper proposes a dual encoder approach using a unified language model and cosine similarity loss for the code search task, specifically for Python. It demonstrates improved performance over state-of-the-art methods while being more efficient.",1. Introduction 1.1 Proposed Approach 1.2 Research Questions 1.3 Contributions 2. Background and Related Work 2.1 Code Search 2.2 Code Search with LLMs 2.3 Dual Encoders 2.4 Code Search with Dual Encoders 3. Methodology 4. Experiments and Results 5. Analysis and Discussion 6. Conclusion and Future Work,"['- Use a unified FastText language model to generate word embeddings for both natural language and code sequences', '- Project the embeddings onto a shared space using dual encoders (one for natural language, one for code)', '- Train the encoders using cosine similarity loss to make linked pairs closer in the shared space', '- At inference, compute cosine similarity between query and code embeddings to retrieve relevant code']","For the natural language query 'Decompresses data for Content-Encoding: deflate', the model embeds both the query and candidate Python functions like 'def undeflate(data): ...' onto the shared space. The Python function with the highest cosine similarity to the query is retrieved as the most relevant result.","Code search is vital in the maintenance and extension of software systems.
Past works have used separate language models for the natural language and
programming language artifacts on models with multiple encoders and different
loss functions. Similarly, this work approaches code search for Python as a
translation retrieval problem while the natural language queries and the
programming language are treated as two types of languages. By using dual
encoders, these two types of language sequences are projected onto a shared
embedding space, in which the distance reflects the similarity between a given
pair of query and code. However, in contrast to previous work, this approach
uses a unified language model, and a dual encoder structure with a cosine
similarity loss function. A unified language model helps the model take
advantage of the considerable overlap of words between the artifacts, making
the learning much easier. On the other hand, the dual encoders trained with
cosine similarity loss helps the model learn the underlining patterns of which
terms are important for predicting linked pairs of artifacts. Evaluation shows
the proposed model achieves performance better than state-of-the-art code
search models. In addition, this model is much less expensive in terms of time
and complexity, offering a cheaper, faster, and better alternative.",http://arxiv.org/abs/2410.03431v2
Local Search for Integer Quadratic Programming,"integer quadratic programming, local search operators, quadratic constraints, two-mode search, scoring functions",2024-09,"This paper proposes LS-IQCQP, a novel local search solver for solving general integer quadratic programming (IQP) problems. It introduces new local search operators capable of handling quadratic terms in both the objective and constraints, along with a two-mode algorithm utilizing specialized scoring functions.",1. Introduction 2. Preliminaries 3. Local Search Operators 3.1 Operator for Feasibility 3.2 Operators for Optimization 4. Weighting Scheme and Score Function 5. Local Search Algorithm,"['Proposed four new local search operators for IQP: - Quadratic satisfying move operator for violated constraints - Inequality exploration move operator for optimizing objective - Equality incremental move operator for satisfied equality constraints - Free move operator for unconstrained variables', 'Introduced a two-mode local search algorithm: - Satisfying mode uses operators to satisfy violated constraints - Optimization mode uses operators to improve objective value', 'Designed specialized scoring functions incorporating weights for constraints and objective', 'Implemented dynamic weight adjustment scheme to guide search']","For a quadratic equality constraint x^2 + y^2 = 25 involving variables x and y in the objective, the equality incremental move operator works as follows: 1) Increment/decrement x by 1 to decrease the objective value 2) Update y to a new integer value that satisfies the equality constraint 3) Accept the move if the new (x,y) assignment decreases the overall objective value","Integer Quadratic Programming (IQP) is an important problem in operations
research. Local search is a powerful method for solving hard problems, but the
research on local search algorithms for IQP solving is still on its early
stage. This paper develops an efficient local search solver for solving general
IQP, called LS-IQCQP. We propose four new local search operators for IQP that
can handle quadratic terms in the objective function, constraints or both.
Furthermore, a two-mode local search algorithm is introduced, utilizing newly
designed scoring functions to enhance the search process. Experiments are
conducted on standard IQP benchmarks QPLIB and MINLPLIB, comparing LS-IQCQP
with several state-of-the-art IQP solvers. Experimental results demonstrate
that LS-IQCQP is competitive with the most powerful commercial solver Gurobi
and outperforms other state-of-the-art solvers. Moreover, LS-IQCQP has
established 6 new records for QPLIB and MINLPLIB open instances.",http://arxiv.org/abs/2409.19668v1
Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation,"retrieval-augmented code generation, preference-guided refactorer, generative compression, preference gap, two-stage training",2024-09,"This paper proposes a novel framework called RRG (Retrieve, Refactor, Generate) to improve retrieval-augmented code generation by introducing a code refactorer module that bridges the preference gap between the retriever and generator. It uses a two-stage training approach to enable the refactorer to compress retrieved code into a concise, model-friendly form aligned with the generator's preferences.","1. Introduction - Motivates retrieval-augmented code generation and identifies limitations 2. Related Work - Covers pretrained models, retrieval-augmented generation, retrieval-augmented code generation 3. Approach - Describes the RRG framework, refactored fine-tuning, and preference-aware tuning 4. Experiments 5. Conclusion","['Introduces a code refactorer module between the retriever and generator', 'Two-stage training scheme:', '- Stage 1 (Refactored Fine-Tuning): Train refactorer for generative compression using supervised learning on target code', '- Stage 2 (Preference-Aware Tuning): Finetune refactorer using reinforcement learning to align its output with generator preferences']","For the query 'implement binary search in Python', the retriever fetches relevant code snippets. The refactorer compresses these into a concise, Python-specific code example focused on binary search, removing redundancies. This refactored code is then fed to the generator to produce the final implementation.","Retrieval-augmented code generation utilizes Large Language Models as the
generator and significantly expands their code generation capabilities by
providing relevant code, documentation, and more via the retriever. The current
approach suffers from two primary limitations: 1) information redundancy. The
indiscriminate inclusion of redundant information can result in resource
wastage and may misguide generators, affecting their effectiveness and
efficiency. 2) preference gap. Due to different optimization objectives, the
retriever strives to procure code with higher ground truth similarity, yet this
effort does not substantially benefit the generator. The retriever and the
generator may prefer different golden code, and this gap in preference results
in a suboptimal design. Additionally, differences in parameterization knowledge
acquired during pre-training result in varying preferences among different
generators.
  To address these limitations, in this paper, we propose RRG (Retrieve,
Refactor, Generate), a novel framework for effective and efficient code
generation. This framework introduces a code refactorer module between the
retriever and the generator to bridge them. The refactoring process transforms
the raw retrieved code into a more concise, efficient, and model-friendly
version. It eliminates redundant information and noise, reducing the input
length. Consequently, the generator receives higher-quality context, enabling
it to produce more accurate results with lower inference costs. We conducted
comprehensive experiments on multiple datasets. In the experiments, we
confirmed the existence of a preference gap between the retriever and the
generator, and RRG effectively bridges this gap. Specifically, RRG achieved
significant performance improvements, with increases of up to 28% on EM, 13% on
BLEU, and 6.8% on CodeBLEU.",http://arxiv.org/abs/2409.15895v1
RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal Reinforcement and Retrieval-Augmented Generation,"retrieval-augmented generation, verbal reinforcement learning, repository-level code completion, iterative feedback optimization, context-aware code generation",2024-09,"The paper proposes RepoGenReflex, a framework that enhances repository-level code completion by integrating Retrieval-Augmented Generation (RAG) with Verbal Reinforcement Learning (VRL). It optimizes the retrieval and generation process through iterative linguistic feedback, leading to improved accuracy and relevance of code completions.",1. Introduction 2. Related Work 2.1 Retrieval-Augmented Generation (RAG) 2.2 Verbal Reinforcement Learning (VRL) 2.3 Repository-Level Code Completion 3. Framework Design 3.1 Components - Retriever - Actor (Generative LLM) - Evaluator - Reflector - Experience 3.2 Leveraging Experience for Improved Retrieval 3.3 Iterative Process 3.4 Component Interdependency,"['- Uses Retrieval-Augmented Generation (RAG) to retrieve relevant code snippets from repositories', '- Employs a pre-trained language model (LLM) as the Actor to generate code completions', '- Evaluates generated code using Exact Match (EM) and Edit Similarity (ES) metrics', '- Utilizes Verbal Reinforcement Learning (VRL) through the Reflector component to provide linguistic feedback', '- Stores feedback in the Experience component to refine future retrievals and generations', '- Iteratively improves code completions by leveraging feedback and adjusting retrievals']","For example, if the Reflector provides feedback suggesting improvements to a function's error handling, the Experience component extracts these suggestions. The Retriever then constructs a new retrieval target by combining the suggestions with the remaining unfinished code. It performs a similarity search against the repository files to find the most relevant code snippets aligning with the new target, which are then used to generate an improved version of the code in the next iteration.","In real-world software engineering tasks, solving a problem often requires
understanding and modifying multiple functions, classes, and files across a
large codebase. Therefore, on the repository level, it is crucial to extract
the relevant information to achieve accurate code completion effectively.
Existing code completion tools have achieved some success, but they struggle to
optimize the retrieval and generation process dynamically. In this paper, we
propose RepoGenReflex, a generic, dynamic, effective framework to address this
challenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with
Verbal Reinforcement Learning (VRL), it can dynamically choose the optimal
results for repository-level code completion. RepoGenReflex uses Reflector to
give directional feedback to the next loop. RepoGenReflex chooses the optimal
results stored in the Experience cache based on the RAG-VRL loop. To validate
the framework's generalization ability, we propose a new benchmark RepoGenEval,
which consists of the latest, high-quality real-world repositories in line
completion scenarios. Our experiments demonstrate that RepoGenReflex achieves
significant improvements after optimizing the Reflector component, resulting in
enhanced accuracy and relevance of code completions. Additionally,
RepoGenReflex consistently demonstrates superior performance and effectiveness
across standard code completion tasks, highlighting the robustness and
adaptability of our framework.",http://arxiv.org/abs/2409.13122v2
RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation,"thought-level code generation, Monte Carlo tree search, code execution feedback, error correction, dual evaluation",2024-09,"This paper proposes RethinkMCTS, a framework that uses Monte Carlo Tree Search (MCTS) to explore the thought process of code generation and leverages detailed feedback from code execution to refine erroneous thoughts, thereby improving the overall search quality and code generation performance.",1. Introduction 2. Related Work 3. Preliminaries 4. RethinkMCTS 4.1 Overview 4.2 Selection 4.3 Expansion 4.4 Simulation 4.5 Evaluation 4.6 Backpropagation 4.7 Rethink 5. Experiments 6. Conclusion,"['- Use Monte Carlo Tree Search (MCTS) to explore the thought process of code generation', '- Obtain block-level code analysis as verbal feedback through code execution', ""- Introduce 'rethink' operation to refine erroneous thoughts based on verbal feedback"", '- Employ dual evaluation using public test cases and LLM evaluation to assess code quality']","For the problem of checking if any two numbers in a list are closer than a given threshold, RethinkMCTS may explore thoughts like: 'Use a nested loop to compare each pair' -> 'Utilize a more efficient data structure like sorting'. If the nested loop approach fails test cases, the verbal feedback guides rethinking to the more efficient sorted approach.","LLM agents enhanced by tree search algorithms have yielded notable
performances in code generation. However, current search algorithms in this
domain suffer from low search quality due to several reasons: 1) Ineffective
design of the search space for the high-reasoning demands of code generation
tasks, 2) Inadequate integration of code feedback with the search algorithm,
and 3) Poor handling of negative feedback during the search, leading to reduced
search efficiency and quality. To address these challenges, we propose to
search for the reasoning process of the code and use the detailed feedback of
code execution to refine erroneous thoughts during the search. In this paper,
we introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS)
algorithm to conduct thought-level searches before generating code, thereby
exploring a wider range of strategies. More importantly, we construct verbal
feedback from fine-grained code execution feedback to refine erroneous thoughts
during the search. This ensures that the search progresses along the correct
reasoning paths, thus improving the overall search quality of the tree by
leveraging execution feedback. Through extensive experiments, we demonstrate
that RethinkMCTS outperforms previous search-based and feedback-based code
generation baselines. On the HumanEval dataset, it improves the pass@1 of
GPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It
effectively conducts more thorough exploration through thought-level searches
and enhances the search quality of the entire tree by incorporating rethink
operation.",http://arxiv.org/abs/2409.09584v1
Planning In Natural Language Improves LLM Search For Code Generation,"natural language planning, code generation search, diverse idea exploration, solution sketches, combinatorial plan sampling",2024-09,The paper proposes a novel search algorithm called PlanSearch that improves large language model performance on code generation tasks by searching over diverse natural language plans and solution sketches before generating code. PlanSearch outperforms standard sampling methods by exploring a significantly more diverse space of potential solutions.,1. Introduction 2. Related Work 3. Motivation 4. PlanSearch Algorithm 5. Experiments 6. Analysis 7. Conclusion,"['Generate diverse first-order observations about the coding problem in natural language', 'Combinatorially sample subsets of observations to form candidate solution plans', 'Optionally generate second-order observations based on sampled plans', 'Translate final sampled plans into natural language strategy descriptions', 'Generate code by conditioning the language model on the strategy descriptions']","For the problem of finding the longest common subsequence between two strings: First-order observations: 'Use dynamic programming', 'Compare characters pairwise', 'Build up from smaller solutions' Sampled plan: 'Use dynamic programming', 'Compare characters pairwise' Second-order observation: 'Store results in 2D array to avoid recomputing' Strategy description: 'Use dynamic programming with a 2D array, comparing characters pairwise and building up the solution from smaller subproblems' Generated code: [Python code implementing dynamic programming solution]","While scaling training compute has led to remarkable improvements in large
language models (LLMs), scaling inference compute has not yet yielded analogous
gains. We hypothesize that a core missing component is a lack of diverse LLM
outputs, leading to inefficient search due to models repeatedly sampling highly
similar, yet incorrect generations. We empirically demonstrate that this lack
of diversity can be mitigated by searching over candidate plans for solving a
problem in natural language. Based on this insight, we propose PlanSearch, a
novel search algorithm which shows strong results across HumanEval+, MBPP+, and
LiveCodeBench (a contamination-free benchmark for competitive coding).
PlanSearch generates a diverse set of observations about the problem and then
uses these observations to construct plans for solving the problem. By
searching over plans in natural language rather than directly over code
solutions, PlanSearch explores a significantly more diverse range of potential
solutions compared to baseline search methods. Using PlanSearch on top of
Claude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on
LiveCodeBench, outperforming both the best score achieved without search
(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).
Finally, we show that, across all models, search algorithms, and benchmarks
analyzed, we can accurately predict performance gains due to search as a direct
function of the diversity over generated ideas. Code can be found at
https://github.com/scaleapi/plansearch.",http://arxiv.org/abs/2409.03733v2
"No Man is an Island: Towards Fully Automatic Programming by Code Search, Code Generation and Program Repair","retrieval-augmented code generation, test-driven program repair, large language model integration, automatic programming pipeline, code search and reuse",2024-09,"This paper proposes Cream, a novel framework that integrates code search, code generation, and program repair techniques leveraging large language models (LLMs) to advance automatic programming. The key idea is to emulate the real-world programming process by using code search to retrieve relevant code snippets, generating code candidates based on the retrieved code using LLMs, and refining the generated code through test-driven program repair with LLMs.","1. Introduction - Motivation and overview of automatic programming challenges 2. Background & Related Work - Overview of code search, generation, repair techniques 3. The Cream Framework - Detailed description of the proposed 3-stage framework 4. Preliminary Evaluation - Experiments on programming benchmarks, case studies 5. Discussion & Future Work","['Use information retrieval or deep learning techniques to search for relevant code from databases', 'Leverage LLMs to generate ranked code candidates based on requirements and retrieved code', 'Construct dynamic prompts with test case feedback to query LLMs for iterative code refinement']","For the problem 'Write a Python function to count true Booleans in a list', Cream first retrieves similar code demonstrations like 'def solution(arr): ...'. It then uses an LLM to generate an initial solution 'def count(a): count = 0 
 for i in a: 
  if i == True: 
   count += 1'. When this fails a test case, Cream provides the failure trace to the LLM to generate a patch correcting the condition to 'if i:' instead of 'if i == True:'.","Automatic programming attempts to minimize human intervention in the
generation of executable code, and has been a long-standing challenge in the
software engineering community. To advance automatic programming, researchers
are focusing on three primary directions: (1) code search that reuses existing
code snippets from external databases; (2) code generation that produces new
code snippets from natural language; and (3) program repair that refines
existing code snippets by fixing detected bugs. Despite significant
advancements, the effectiveness of state-of-the-art techniques is still
limited, such as the usability of searched code and the correctness of
generated code.
  Motivated by the real-world programming process, where developers usually use
various external tools to aid their coding processes, such as code search
engines and code testing tools, in this work, we propose \toolname{}, an
automatic programming framework that leverages recent large language models
(LLMs) to integrate the three research areas to address their inherent
limitations. In particular, our framework first leverages different code search
strategies to retrieve similar code snippets, which are then used to further
guide the code generation process of LLMs. Our framework further validates the
quality of generated code by compilers and test cases, and constructs repair
prompts to query LLMs for generating correct patches. We conduct preliminary
experiments to demonstrate the potential of our framework, \eg helping
CodeLlama solve 267 programming problems with an improvement of 62.53\%. As a
generic framework, \toolname{} can integrate various code search, generation,
and repair tools, combining these three research areas together for the first
time. More importantly, it demonstrates the potential of using traditional SE
tools to enhance the usability of LLMs in automatic programming.",http://arxiv.org/abs/2409.03267v1
Chain-of-Experts (CoE): Reverse Engineering Software Bills of Materials for JavaScript Application Bundles through Code Clone Search,"JavaScript application bundles, Software Bill of Materials, code segmentation, code classification, code clone retrieval, multi-task learning",2024-08,"This paper proposes Chain-of-Experts (CoE), a novel multi-task deep learning model to generate Software Bills of Materials (SBoMs) for JavaScript application bundles through code segmentation, classification, and clone retrieval. CoE offers an efficient end-to-end solution by jointly optimizing these tasks.",1. Introduction 2. Problem Formulation 3. Chain-of-Experts Architecture Design 4. Experimental Setup 5. Results and Analysis 6. Deployment and Operational Analysis 7. Related Work 8. Conclusion,"['Divides JavaScript bundles into sliding windows to handle long sequences', 'Uses a chain of backbone and expert neural network models for multi-task learning', 'Backbone models encode semantics and global context', 'Expert models predict segment boundaries, code classes, and code embeddings', 'Novel segmentation masking technique to prevent data overflow across segments', 'Optimizes for segmentation, classification and clone retrieval tasks jointly']","For a JavaScript application bundle containing code like: 

<cls>!function(e,t){""use strict"";""object""==typeof e...}

CoE would segment it into windows, predict segment boundaries and classes (e.g. library, application), and retrieve matching library files from a code repository using embeddings and cosine similarity.","A Software Bill of Materials (SBoM) is a detailed inventory of all
components, libraries, and modules in a software artifact, providing
traceability throughout the software supply chain. With the increasing
popularity of JavaScript in software engineering due to its dynamic syntax and
seamless supply chain integration, the exposure to vulnerabilities and attacks
has risen significantly. A JavaScript application bundle, which is a
consolidated, symbol-stripped, and optimized assembly of code for deployment
purpose. Generating a SBoM from a JavaScript application bundle through a
reverse-engineering process ensures the integrity, security, and compliance of
the supplier's software release, even without access to the original dependency
graphs.
  This paper presents the first study on SBoM generation for JavaScript
application bundles. We identify three key challenges for this task, i.e.,
nested code scopes, extremely long sequences, and large retrieval spaces. To
address these challenges, we introduce Chain-of-Experts (CoE), a multi-task
deep learning model designed to generate SBoMs through three tasks: code
segmentation, code classification, and code clone retrieval. We evaluate CoE
against individual task-specific solutions on 500 web application bundles with
over 66,000 dependencies. Our experimental results demonstrate that CoE offers
competitive outcomes with less training and inference time when compared with
combined individual task-specific solutions. Consequently, CoE provides the
first scalable, efficient, and end-to-end solution for the SBoM generation of
real-world JavaScript application bundles.",http://arxiv.org/abs/2408.16198v1
Search-Based LLMs for Code Optimization,"search-based optimization, evolutionary search, representative sample selection, adaptive pattern retrieval, genetic operator prompting",2024-08,"This paper proposes SBLLM, a search-based framework that integrates large language models (LLMs) with evolutionary search techniques for automated code optimization. It enables iterative refinement and discovery of improved optimization methods through representative sample selection, adaptive pattern retrieval, and genetic operator-inspired prompting.",1. Introduction 2. Proposed Framework a. Overview b. Execution-based Representative Sample Selection c. Adaptive Optimization Pattern Retrieval d. Genetic Operator-inspired Chain-of-thought Prompting 3. Evaluation 4. Conclusion,"['Execution-based representative sample selection to evaluate fitness of existing optimized code and prioritize promising samples', 'Adaptive optimization pattern retrieval to infuse targeted optimization patterns into the model for guiding LLMs', 'Genetic operator-inspired chain-of-thought prompting to aid LLMs in combining different optimization methods']","For a Python code snippet using the Eratosthenes Sieve algorithm, SBLLM retrieves a similar pattern showing the correct initialization to rectify errors in the current optimized code. For a C++ code truncating a string, it retrieves a different pattern using substr() to find an unexploited optimization method.","The code written by developers usually suffers from efficiency problems and
contain various performance bugs. These inefficiencies necessitate the research
of automated refactoring methods for code optimization. Early research in code
optimization employs rule-based methods and focuses on specific inefficiency
issues, which are labor-intensive and suffer from the low coverage issue.
Recent work regards the task as a sequence generation problem, and resorts to
deep learning (DL) techniques such as large language models (LLMs). These
methods typically prompt LLMs to directly generate optimized code. Although
these methods show state-of-the-art performance, such one-step generation
paradigm is hard to achieve an optimal solution. First, complex optimization
methods such as combinatorial ones are hard to be captured by LLMs. Second, the
one-step generation paradigm poses challenge in precisely infusing the
knowledge required for effective code optimization within LLMs, resulting in
under-optimized code.To address these problems, we propose to model this task
from the search perspective, and propose a search-based LLMs framework named
SBLLM that enables iterative refinement and discovery of improved optimization
methods. SBLLM synergistically integrate LLMs with evolutionary search and
consists of three key components: 1) an execution-based representative sample
selection part that evaluates the fitness of each existing optimized code and
prioritizes promising ones to pilot the generation of improved code; 2) an
adaptive optimization pattern retrieval part that infuses targeted optimization
patterns into the model for guiding LLMs towards rectifying and progressively
enhancing their optimization methods; and 3) a genetic operator-inspired
chain-of-thought prompting part that aids LLMs in combining different
optimization methods and generating improved optimization methods.",http://arxiv.org/abs/2408.12159v1
EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation,"evolutionary prompt engineering, code generation, test case evaluation, cost-effective, large language models",2024-08,"The paper proposes EPiC, an evolutionary algorithm-based approach for cost-effective prompt engineering of large language models for code generation tasks. EPiC optimizes prompts by assessing the generated code against test case pass rates, outperforming state-of-the-art methods while requiring lower computational cost.",1. Introduction 2. Background 3. Evolutionary Prompt Engineering for Code (EPiC) 4. Experimental Setup 5. Results and Discussion 6. Threats to Validity 7. Conclusion,"['Two phases: Initial Evaluation (IE) and Evolutionary Prompt Engineering (EPE)', 'IE: Generate code from initial prompt and evaluate against test cases', 'EPE: If IE fails, evolve population of prompts using mutation and selection', 'Mutations via LLM-guided prompts or word embedding replacements', 'Selection based on test case pass rates as fitness function']","For the prompt 'Write a function to print the given string', the initial code generated an incorrect print statement. After mutation to 'Write a function to publish the given string', the expected return statement passing the test cases was produced.","Large Language Models (LLMs) have seen increasing use in various software
development tasks, especially in code generation. The most advanced recent
methods attempt to incorporate feedback from code execution into prompts to
help guide LLMs in generating correct code, in an iterative process. While
effective, these methods could be costly and time-consuming due to numerous
interactions with the LLM and the extensive token usage. To address this issue,
we propose an alternative approach named Evolutionary Prompt Engineering for
Code (EPiC), which leverages a lightweight evolutionary algorithm to evolve the
original prompts toward better ones that produce high-quality code, with
minimal interactions with LLM. Our evaluation against state-of-the-art (SOTA)
LLM-based code generation models shows that EPiC outperforms all the baselines
in terms of cost-effectiveness.",http://arxiv.org/abs/2408.11198v1
Deep Code Search with Naming-Agnostic Contrastive Multi-View Learning,"naming-agnostic, contrastive multi-view learning, code search, graph self-supervised learning, abstract syntax tree modeling",2024-08,This paper proposes a naming-agnostic code search method (NACS) based on contrastive multi-view learning to overcome the challenge of different naming conventions in code. NACS strips variable name information from abstract syntax trees (ASTs) and uses contrastive learning on graph and path views to learn code representations robust to naming variations.,1. Introduction 2. Related Work 2.1 Code Representation Learning 2.2 Code Search 2.3 Graph Self-Supervised Learning 3. Methodology 3.1 Preliminaries 3.2 Overview of NACS 3.3 Graph-View Modeling 3.4 Path-View Modeling 3.5 Multi-View Learning 3.6 Model Training 4. Experiments 5. Conclusion,"['Strip variable name information from input ASTs to focus on capturing intrinsic AST structure properties', 'Use semantic-level and syntax-level data augmentation to prepare realistic training data', 'Adopt contrastive learning on the graph-view of ASTs to learn naming-agnostic code representations', 'Model AST paths in a path-view to complement the graph-view representations', 'Use multi-view learning to combine and mutually enhance the graph-view and path-view representations']","For the code snippets in Figure 1, NACS would strip the variable names 'lst', 'item', 'l', 'i' from the ASTs. It would then apply data augmentation like renaming variables to create positive/negative example pairs. The contrastive graph-view modeling would learn to map the augmented ASTs to similar representations despite naming differences. The path-view captures complementary structural information to further enhance the naming-agnostic representations.","Software development is a repetitive task, as developers usually reuse or get
inspiration from existing implementations. Code search, which refers to the
retrieval of relevant code snippets from a codebase according to the
developer's intent that has been expressed as a query, has become increasingly
important in the software development process. Due to the success of deep
learning in various applications, a great number of deep learning based code
search approaches have sprung up and achieved promising results. However,
developers may not follow the same naming conventions and the same variable may
have different variable names in different implementations, bringing a
challenge to deep learning based code search methods that rely on explicit
variable correspondences to understand source code. To overcome this challenge,
we propose a naming-agnostic code search method (NACS) based on contrastive
multi-view code representation learning. NACS strips information bound to
variable names from Abstract Syntax Tree (AST), the representation of the
abstract syntactic structure of source code, and focuses on capturing intrinsic
properties solely from AST structures. We use semantic-level and syntax-level
augmentation techniques to prepare realistically rational data and adopt
contrastive learning to design a graph-view modeling component in NACS to
enhance the understanding of code snippets. We further model ASTs in a path
view to strengthen the graph-view modeling component through multi-view
learning. Extensive experiments show that NACS provides superior code search
performance compared to baselines and NACS can be adapted to help existing code
search methods overcome the impact of different naming conventions.",http://arxiv.org/abs/2408.09345v1
ViC: Virtual Compiler Is All You Need For Assembly Code Search,"virtual compiler, assembly code search, large language model, code search dataset construction, contrastive learning",2024-08,"This paper introduces a novel approach called Virtual Compiler (ViC) that uses a large language model to emulate a general compiler and generate assembly code from source code across programming languages. ViC enables constructing large datasets for assembly code search, leading to substantial performance improvements over prior methods.","1. Introduction to assembly code search and dataset challenges 2. Background on assembly code analysis, modeling, and code search 3. Overview of the ViC approach 4. Training ViC as a virtual compiler on compiled Ubuntu packages 5. Using ViC to construct assembly code search dataset and train encoder 6. Evaluation of ViC and assembly code search performance","['Compile over 6,000 C/C++ packages from Ubuntu using different compilers/versions/optimizations to obtain source-assembly pairs', 'Pre-train CodeLlama model on this 20B token dataset to learn compiler behavior (ViC)', 'Use ViC to virtually compile existing code search datasets to assembly code', 'Train assembly code encoder on augmented dataset using contrastive learning']","For the bubble sort algorithm in C, ViC can take the source code as input and generate the corresponding assembly code, preserving semantic equivalence despite the loss of high-level structure like variable names and loops during compilation.","Assembly code search is vital for reducing the burden on reverse engineers,
allowing them to quickly identify specific functions using natural language
within vast binary programs. Despite its significance, this critical task is
impeded by the complexities involved in building high-quality datasets. This
paper explores training a Large Language Model (LLM) to emulate a general
compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion
tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC),
capable of compiling any source code of any language to assembly code. This
approach allows for virtual compilation across a wide range of programming
languages without the need for a real compiler, preserving semantic equivalency
and expanding the possibilities for assembly code dataset construction.
Furthermore, we use ViC to construct a sufficiently large dataset for assembly
code search. Employing this extensive dataset, we achieve a substantial
improvement in assembly code search performance, with our model surpassing the
leading baseline by 26%.",http://arxiv.org/abs/2408.06385v1
You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search,"ChatGPT data augmentation, code search, prompt engineering, data filtering, representation learning",2024-08,"The paper proposes ChatDANCE, a novel approach that leverages ChatGPT to generate high-quality augmented data for improving code search models. It uses carefully designed prompts to guide ChatGPT in rewriting code and queries, and employs a filtering mechanism to remove low-quality augmentations.",1. Introduction 2. Related Work 3. ChatDANCE Framework 4. Experiments 5. Analysis 6. Conclusion,"['Design prompts for ChatGPT to augment code and queries while preserving semantics', 'Use a cross-encoder model to filter out low-quality augmented data', 'Retrain a code search model (UniXcoder) on the filtered augmented dataset']","For code augmentation, the paper uses prompts to guide ChatGPT to rewrite code using 5 rewriting techniques like loop transformation and variable renaming. For example, the code 'def cumsum(inlist): newlist = copy.deepcopy(inlist) 
 for i in range(1, len(newlist)): 
    newlist[i] = newlist[i] + newlist[i - 1] 
 return newlist' is rewritten by ChatGPT as 'def cumsum(inlist): 
    newlist = [] 
    cum_sum = 0 
    for i in inlist: 
        cum_sum += i 
        newlist.append(cum_sum) 
    return newlist'.","Code search plays a crucial role in software development, enabling developers
to retrieve and reuse code using natural language queries. While the
performance of code search models improves with an increase in high-quality
data, obtaining such data can be challenging and expensive. Recently, large
language models (LLMs) such as ChatGPT have made remarkable progress in both
natural and programming language understanding and generation, offering
user-friendly interaction via simple prompts. Inspired by these advancements,
we propose a novel approach ChatDANCE, which utilizes high-quality and diverse
augmented data generated by a large language model and leverages a filtering
mechanism to eliminate low-quality augmentations. Specifically, we first
propose a set of ChatGPT prompting rules that are specifically designed for
source code and queries. Then, we leverage ChatGPT to rewrite code and queries
based on the according prompts and then propose a filtering mechanism which
trains a cross-encoder from the backbone model UniXcoder to filter out code and
query pairs with low matching scores. Finally, we re-train the backbone model
using the obtained high-quality augmented data. Experimental results show that
ChatDANCE achieves state-of-the-art performance, improving the best baseline by
13.2% (R@1) and 7% (MRR). Surprisingly, we find that this
augment-filter-retrain strategy enables the backbone model (UniXcoder) to
self-grow. Moreover, extensive experiments show the effectiveness of each
component and ChatDANCE has stable performance under different hyperparameter
settings. In addition, we conduct qualitative and quantitative analyses to
investigate why ChatDANCE works well and find that it learns a more uniform
distribution of representations and effectively aligns the code and query
spaces.",http://arxiv.org/abs/2408.05542v2
AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations,"context retrieval, code generation, coding assistant, evaluation, in-context learning",2024-08,This paper discusses the importance of providing relevant context to large language models (LLMs) for code recommendation tasks in coding assistants. It outlines the key components of a context engine and the challenges in evaluating such AI-assisted coding systems.,1. Introduction 2. Context Engine - Retrieval of context items - Ranking of context items 3. Evaluation - Online vs. offline evaluation - Lack of labeled data - Component-wise vs. end-to-end evaluation,"['Use techniques like similarity matching, keyword search, semantic search, and code graph analysis to retrieve relevant context items', ""Rank the retrieved context items based on relevance to the user's query"", 'Utilize domain-specific checks (e.g., syntax, semantics, test execution) for end-to-end evaluation', 'Explore techniques like LLM judging for evaluating open-ended chat responses']","For code autocompletions, the system can perform syntactic checks (e.g., does the generated code parse?) and semantic checks (e.g., do the types match?) on the resulting code segment to prevent nonsensical outputs.","In this work, we discuss a recently popular type of recommender system: an
LLM-based coding assistant. Connecting the task of providing code
recommendations in multiple formats to traditional RecSys challenges, we
outline several similarities and differences due to domain specifics. We
emphasize the importance of providing relevant context to an LLM for this use
case and discuss lessons learned from context enhancements & offline and online
evaluation of such AI-assisted coding systems.",http://arxiv.org/abs/2408.05344v1
Retrieval-augmented code completion for local projects using large language models,"retrieval-augmented code completion, in-context retrieval, local project code, small language models, token healing",2024-08,"The paper explores using smaller language models (around 160M parameters) for code completion by augmenting them with retrieval from local project files. It compares generative and retrieval-adapted models, and proposes an in-context retrieval approach based on token similarity.","1. Introduction, 2. Related Work, 3. Code Line Completion Methods (RETRO, Token Healing, In-Context RAG), 4. Experimental Setup, 5. Results and Analysis, 6. Conclusion","['Trained 160M parameter GPT-2 and RETRO models on Python code', 'Compared tokenizers and verified importance of token healing', 'Implemented In-Context RAG with Jaccard token similarity retrieval', 'Compared to RETRO with embedding retrieval and baselines', 'Evaluated on code completion tasks using local project files']","For the input context 'import numpy as np
np.', the In-Context RAG method retrieves code snippets like 'np.array([1,2,3])' based on token similarity to the query 'np.'. These snippets are concatenated to the input to provide relevant context for the language model to predict the next token.","The use of large language models (LLMs) is becoming increasingly widespread
among software developers. However, privacy and computational requirements are
problematic with commercial solutions and the use of LLMs. In this work, we
focus on using LLMs with around 160 million parameters that are suitable for
local execution and augmentation with retrieval from local projects. We train
two models based on the transformer architecture, the generative model GPT-2
and the retrieval-adapted RETRO model, on open-source Python files, and
empirically evaluate and compare them, confirming the benefits of vector
embedding based retrieval. Further, we improve our models' performance with
In-context retrieval-augmented generation, which retrieves code snippets based
on the Jaccard similarity of tokens. We evaluate In-context retrieval-augmented
generation on larger models and conclude that, despite its simplicity, the
approach is more suitable than using the RETRO architecture. We highlight the
key role of proper tokenization in achieving the full potential of LLMs in code
completion.",http://arxiv.org/abs/2408.05026v1
Improving Retrieval-Augmented Code Comment Generation by Retrieving for Generation,"retrieval-augmented comment generation, joint training, dense retriever, pre-trained code models, weighted loss",2024-08,"The paper proposes a novel approach called JointCom that jointly trains a retriever and a generator to improve retrieval-augmented code comment generation. By enabling the retriever to learn from the generator's feedback, it can retrieve exemplars that are more useful for generating high-quality comments.","1. Introduction, 2. Preliminaries, 3. Approach (Overall Framework, Retrieval, Generation, Joint Training), 4. Experimental Setup, 5. Results and Analysis, 6. Human Evaluation, 7. Discussion, 8. Related Work, 9. Conclusion","['Uses a dense retriever based on a neural encoder to retrieve code-comment exemplars from a base', 'Concatenates the input code with the top-k retrieved exemplars and their comments', 'Calculates generation losses for each exemplar using a seq2seq model', 'Computes a weighted sum of the losses using the retrieval scores as weights', 'Jointly optimizes the retriever and generator to minimize this weighted loss']","For the code 'change private-browsing config to true and emit signal', using another relevant code as an exemplar helps the generator produce a better comment 'change private-browsing config to true and emit signal' compared to using a less relevant exemplar that generates 'test if cache is enabled after clearing it'.","Code comment generation aims to generate high-quality comments from source
code automatically and has been studied for years. Recent studies proposed to
integrate information retrieval techniques with neural generation models to
tackle this problem, i.e., Retrieval-Augmented Comment Generation (RACG)
approaches, and achieved state-of-the-art results. However, the retrievers in
previous work are built independently of their generators. This results in that
the retrieved exemplars are not necessarily the most useful ones for generating
comments, limiting the performance of existing approaches. To address this
limitation, we propose a novel training strategy to enable the retriever to
learn from the feedback of the generator and retrieve exemplars for generation.
Specifically, during training, we use the retriever to retrieve the top-k
exemplars and calculate their retrieval scores, and use the generator to
calculate a generation loss for the sample based on each exemplar. By aligning
high-score exemplars retrieved by the retriever with low-loss exemplars
observed by the generator, the retriever can learn to retrieve exemplars that
can best improve the quality of the generated comments. Based on this strategy,
we propose a novel RACG approach named JOINTCOM and evaluate it on two
real-world datasets, JCSD and PCSD. The experimental results demonstrate that
our approach surpasses the state-of-the-art baselines by 7.3% to 30.0% in terms
of five metrics on the two datasets. We also conduct a human evaluation to
compare JOINTCOM with the best-performing baselines. The results indicate that
JOINTCOM outperforms the baselines, producing comments that are more natural,
informative, and useful.",http://arxiv.org/abs/2408.03623v1
LLM Agents Improve Semantic Code Search,"agentic LLMs, retrieval-augmented generation, ensemble code search, multi-stream comparisons, context-aware code retrieval",2024-08,"This paper introduces a novel approach to semantic code search using agentic large language models (LLMs) and retrieval-augmented generation (RAG) to enhance user queries with relevant context from GitHub repositories. It also proposes a multi-stream ensemble architecture for improved code retrieval accuracy, which is deployed in the RepoRift application.",1. Introduction 2. Methodology 2.1 Information Injection via Agentic LLMs and RAG 2.2 Ensemble Architecture with Multi-Stream Comparisons 3. Experimental Setup 3.0.1 Dataset 3.0.2 Implementation Details 3.0.3 Evaluation Metrics 4. Results 5. Conclusion,"['- Use agentic LLMs with RAG to augment user queries with relevant information from GitHub repositories', '- Employ an ensemble architecture with multi-stream comparisons:', '    - Stream 1: Compare query embedding with function embeddings', '    - Stream 2: Generate code, compare embeddings with functions and classes', '    - Stream 3: Compare component function embeddings with function embeddings']","For the query 'find a function that calculates the area of a circle', the agent could augment it with details like 'The area of a circle is calculated using the formula pi * r^2, where r is the radius. This is commonly implemented in Python using the math module to import the value of pi.' This augmented query can then be more accurately matched to relevant code snippets.","Code Search is a key task that many programmers often have to perform while
developing solutions to problems. Current methodologies suffer from an
inability to perform accurately on prompts that contain some ambiguity or ones
that require additional context relative to a code-base. We introduce the
approach of using Retrieval Augmented Generation (RAG) powered agents to inject
information into user prompts allowing for better inputs into embedding models.
By utilizing RAG, agents enhance user queries with relevant details from GitHub
repositories, making them more informative and contextually aligned.
Additionally, we introduce a multi-stream ensemble approach which when paired
with agentic workflow can obtain improved retrieval accuracy, which we deploy
on application called repo-rift.com. Experimental results on the CodeSearchNet
dataset demonstrate that RepoRift significantly outperforms existing methods,
achieving an 78.2% success rate at Success@10 and a 34.6% success rate at
Success@1. This research presents a substantial advancement in semantic code
search, highlighting the potential of agentic LLMs and RAG to enhance code
retrieval systems.",http://arxiv.org/abs/2408.11058v1
Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation,"retrieval-augmented generation, few-shot learning, code translation, Fortran-to-C++, embedding models",2024-07,"This paper proposes a novel approach to enhance code translation from Fortran to C++ by leveraging retrieval-augmented generation (RAG) and few-shot learning. By dynamically retrieving relevant code translation examples, the method provides contextual guidance to large language models, significantly improving translation accuracy.",1. Introduction 2. Related Work 3. Methods 3.1 Dataset Preparation 3.2 Embedding Generation and Example Retrieval 3.3 Few-Shot Learning with RAG 4. Evaluation and Experimental Setup 5. Results and Discussion 6. Conclusion and Future Work,"['- Maintain a repository of Fortran-C++ code translation pairs', '- Generate embeddings for code snippets using models like Nomic-Embed, Starencoder, CodeBERT', '- Retrieve top-k most relevant examples based on embedding similarity to the input code', '- Provide the retrieved examples and input code to a large language model for few-shot translation', '- Evaluate translations using CodeBLEU metric for syntactic and semantic correctness']","For the Fortran code snippet:

SUBROUTINE EXAMPLE(A, B, C, N)
  INTEGER N, I
  REAL A(N), B(N), C(N)

  DO I = 1, N
    C(I) = A(I) + B(I)
  END DO

END SUBROUTINE EXAMPLE

The RAG approach would:
1. Generate an embedding for this code
2. Retrieve top-k similar Fortran-C++ translation pairs from the repository
3. Provide the Fortran code, retrieved pairs, and a prompt to the LLM
4. The LLM generates the C++ translation, leveraging the provided context:

void example(float* a, float* b, float* c, int n) {
  for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
  }
}","The advent of large language models (LLMs) has significantly advanced the
field of code translation, enabling automated translation between programming
languages. However, these models often struggle with complex translation tasks
due to inadequate contextual understanding. This paper introduces a novel
approach that enhances code translation through Few-Shot Learning, augmented
with retrieval-based techniques. By leveraging a repository of existing code
translations, we dynamically retrieve the most relevant examples to guide the
model in translating new code segments. Our method, based on
Retrieval-Augmented Generation (RAG), substantially improves translation
quality by providing contextual examples from which the model can learn in
real-time. We selected RAG over traditional fine-tuning methods due to its
ability to utilize existing codebases or a locally stored corpus of code, which
allows for dynamic adaptation to diverse translation tasks without extensive
retraining. Extensive experiments on diverse datasets with open LLM models such
as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code
Instruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5
Turbo and GPT-4o, demonstrate our approach's superiority over traditional
zero-shot methods, especially in translating between Fortran and CPP. We also
explored varying numbers of shots i.e. examples provided during inference,
specifically 1, 2, and 3 shots and different embedding models for RAG,
including Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and
effectiveness of our approach.",http://arxiv.org/abs/2407.19619v1
Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval,"multi-step key retrieval, long-range dependency, code generation models, context window, sliding window attention",2024-07,"The paper proposes a suite of multi-step key retrieval tasks to evaluate the ability of code generation models to handle long-range dependencies in large context windows. It finds that performance degrades significantly when functions reference code defined later, and that sliding window attention struggles beyond the window size.","1. Introduction - Motivates long context evaluation for code completion 2. Related Work - Covers symbolic reasoning, multi-hop QA, long context handling 3. Multi-Step Key Retrieval Tasks - Defines the proposed evaluation tasks 4. Experiments - Evaluates several models on the tasks 5. Results - Analyzes model performance by task difficulty, context size, snippet position","['Defines four multi-step key retrieval tasks of increasing difficulty (one-step, two-step, three-step, concatenation)', 'Constructs long contexts by inserting task snippets and irrelevant code at varying positions', 'Evaluates 5 code generation models (StarCoder, Mistral) on the tasks', 'Measures accuracy@3 on generating the expected string literal', 'Analyzes effects of task difficulty, context size, snippet positions, forward references']","For the two-step task with 5 distractors and 4k context: 

# irrelevant functions...
def value(): 
    return ""xdfgew""
# more irrelevant...
def key():
    return value()

assert key() == ? 

The model must call value() from key() to retrieve the string ""xdfgew"" and complete the assert statement correctly.","As language models support larger and larger context sizes, evaluating their
ability to make effective use of that context becomes increasingly important.
We analyze the ability of several code generation models to handle long range
dependencies using a suite of multi-step key retrieval tasks in context windows
up to 8k tokens in length. The tasks progressively increase in difficulty and
allow more nuanced evaluation of model capabilities than tests like the popular
needle-in-the-haystack test. We find that performance degrades significantly
(up to 2x) when a function references another function that is defined later in
the prompt. We also observe that models that use sliding window attention
mechanisms have difficulty handling references further than the size of a
single window. We perform simple prompt modifications using call graph
information to improve multi-step retrieval performance up to 3x. Our analysis
highlights different facets of long-context performance and is suggestive of
prompt construction strategies for code completion tools",http://arxiv.org/abs/2407.21049v1
CoIR: A Comprehensive Benchmark for Code Information Retrieval Models,"code information retrieval, code retrieval benchmark, text-to-code retrieval, code-to-text retrieval, hybrid code retrieval",2024-07,"This paper introduces COIR, a comprehensive benchmark for evaluating code information retrieval models across diverse tasks and domains. It consists of 10 datasets spanning 4 main retrieval tasks and 8 sub-tasks, covering various programming languages. The authors evaluate 9 retrieval models on COIR, revealing significant challenges in code retrieval.","1. Introduction, 2. Related Work, 3. The COIR Benchmark (3.1 Desiderata, 3.2 Text-to-Code Retrieval, 3.3 Code-to-Text Retrieval, 3.4 Code-to-Code Retrieval, 3.5 Hybrid Code Retrieval), 4. Experiments, 5. The COIR Framework, 6. Conclusion","['Curated 10 datasets across 4 main code retrieval tasks: text-to-code, code-to-text, code-to-code, hybrid code retrieval', 'Tasks cover 8 sub-tasks like code contest retrieval, web query code retrieval, code summary retrieval, similar code retrieval, etc.', 'Datasets span 14 programming languages and diverse domains like GitHub, web queries, databases, contests, etc.', 'Evaluated 9 retrieval models like DPR, Contriever, E5, GTE, BGE on COIR using standard IR metrics']","For the code context retrieval task, the authors modified the CodeSearchNet dataset by randomly dividing each code snippet into two segments - the initial segment as the query, and the remaining part as the target corpus to retrieve.","Despite the substantial success of Information Retrieval (IR) in various NLP
tasks, most IR systems predominantly handle queries and corpora in natural
language, neglecting the domain of code retrieval. Code retrieval is critically
important yet remains under-explored, with existing methods and benchmarks
inadequately representing the diversity of code in various domains and tasks.
Addressing this gap, we present \textbf{\name} (\textbf{Co}de
\textbf{I}nformation \textbf{R}etrieval Benchmark), a robust and comprehensive
benchmark specifically designed to assess code retrieval capabilities. \name
comprises \textbf{ten} meticulously curated code datasets, spanning
\textbf{eight} distinctive retrieval tasks across \textbf{seven} diverse
domains. We first discuss the construction of \name and its diverse dataset
composition. Further, we evaluate nine widely used retrieval models using
\name, uncovering significant difficulties in performing code retrieval tasks
even with state-of-the-art systems. To facilitate easy adoption and integration
within existing research workflows, \name has been developed as a user-friendly
Python framework, readily installable via pip. It shares same data schema as
other popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark
evaluations. Through \name, we aim to invigorate research in the code retrieval
domain, providing a versatile benchmarking tool that encourages further
development and exploration of code retrieval systems\footnote{\url{
https://github.com/CoIR-team/coir}}.",http://arxiv.org/abs/2407.02883v1
A Comparative Study of DSL Code Generation: Fine-Tuning vs. Optimized Retrieval Augmentation,"DSL code generation, retrieval augmented generation, few-shot learning, API metadata grounding, hallucination reduction",2024-07,"This paper presents optimizations to retrieval augmented generation (RAG) techniques for improving the quality of natural language to domain-specific language (DSL) code generation, focusing on reducing hallucinations and syntax errors. The authors compare RAG approaches with fine-tuning on a DSL for automating API workflows.",1. Introduction 2. Related Work 2.1 Code Generation 2.2 Reasoning and Tool Integration 2.3 Contributions 3. Methodology 3.1 Fine-Tuned NL2DSL Model 3.2 Grounding with Dynamically Selected Few-Shots 3.3 Grounding with API Metadata 4. Experiment Design and Metrics 4.1 Dataset Generation 4.2 DSL Generation Quality Metrics 5. Results 6. Conclusion and Future Work,"['Fine-tuned a Codex model on a synthetic NL-DSL dataset for the target DSL', 'Used retrieval augmented generation (RAG) with dynamically selected few-shot examples', 'Fine-tuned a BERT model to improve few-shot retrieval based on target DSL similarity', 'Grounded with API function definitions corresponding to few-shot examples', 'Indexed API metadata to retrieve semantically relevant function definitions']","For the input query: ""When someone sends me an email with subject 'invoice', send me a Teams message to start an approval manually"", the generated DSL output is:

triggerOutputs = await shared_office365.OnNewEmailV3();
if(triggerOutputs?['body']?['subject'] == 'invoice') {
  teams_msg = shared_teams.PostMessageToConversation({""message"": ""Start Approval process""});
}","Natural Language to Code Generation has made significant progress in recent
years with the advent of Large Language Models(LLMs). While generation for
general-purpose languages like C, C++, and Python has improved significantly,
LLMs struggle with custom function names in Domain Specific Languages or DSLs.
This leads to higher hallucination rates and syntax errors, specially for DSLs
having a high number of custom function names. Additionally, constant updates
to function names add to the challenge as LLMs need to stay up-to-date. In this
paper, we present optimizations for using Retrieval Augmented Generation (or
RAG) with LLMs for DSL generation along with an ablation study comparing these
strategies. We generated a train as well as test dataset with a DSL to
represent automation tasks across roughly 700 APIs in public domain. We used
the training dataset to fine-tune a Codex model for this DSL. Our results
showed that the fine-tuned model scored the best on code similarity metric.
With our RAG optimizations, we achieved parity for similarity metric. The
compilation rate, however, showed that both the models still got the syntax
wrong many times, with RAG-based method being 2 pts better. Conversely,
hallucination rate for RAG model lagged by 1 pt for API names and by 2 pts for
API parameter keys. We conclude that an optimized RAG model can match the
quality of fine-tuned models and offer advantages for new, unseen APIs.",http://arxiv.org/abs/2407.02742v1
Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft,"retrieval-augmented code generation, situated action prediction, Minecraft collaborative building, few-shot prompting, language model evaluation",2024-06,"This paper explores using large language models (LLMs) for predicting the sequence of actions taken by a Builder agent in the Minecraft Collaborative Building Task, where an Architect provides instructions to assemble a structure. The authors frame this as a code generation task and leverage few-shot prompting techniques with LLMs, achieving improved performance over previous methods.",1. Introduction 2. Related Work 3. Dataset 4. Experimental Setup - Few-shot Prompting - LLM Fine-tuning - Evaluation Metrics 5. Results & Analysis - Quantitative Results - Qualitative Analysis of Challenges 6. Conclusion,"['Convert builder actions to code representations (place(), pick())', 'Use few-shot prompting with dynamically adapted in-context examples', 'Probe instruction-tuned LLMs like GPT-4, LLaMa to generate action sequences', 'Fine-tune LLaMa-3-8b on the Minecraft dataset', 'Evaluate using micro-averaged F1 score against ground truth actions']","For the instruction 'place an orange block to the diagonal top right of it', the model correctly interprets the spatial preposition 'diagonal top right' and generates: place(color='orange', x=1, y=2, z=1). However, for 'close, it should look like a ring', it struggles with 'close' and the anaphoric reference 'it', failing to generate the intended code.","In the Minecraft Collaborative Building Task, two players collaborate: an
Architect (A) provides instructions to a Builder (B) to assemble a specified
structure using 3D blocks. In this work, we investigate the use of large
language models (LLMs) to predict the sequence of actions taken by the Builder.
Leveraging LLMs' in-context learning abilities, we use few-shot prompting
techniques, that significantly improve performance over baseline methods.
Additionally, we present a detailed analysis of the gaps in performance for
future work",http://arxiv.org/abs/2406.17553v1
CodeRAG-Bench: Can Retrieval Augment Code Generation?,"retrieval-augmented code generation, code retrieval benchmark, document retrieval for code, execution-based code evaluation, open-domain code generation",2024-06,"This paper introduces CodeRAG-Bench, a comprehensive benchmark for evaluating retrieval-augmented code generation (RACG) systems across diverse coding tasks and retrieval sources. The benchmark enables rigorous evaluation through ground-truth document annotations and execution-based testing of generated code.",1. Introduction 2. The CodeRAG-Bench 2.1 Programming Problems 2.2 Retrieval Sources 2.3 Canonical Document Annotation 2.4 Evaluation Metrics 3. Canonical RACG: Experiments and Results 3.1 Experimental Setup 3.2 Retrieval Results 3.3 Code Generation Results 3.4 End-to-End RACG Results,"['Curated 9k coding problems across 4 categories: basic programming, open-domain, repository-level, and code retrieval', 'Collected 25M retrieval documents from 5 sources: solutions, tutorials, documentation, StackOverflow, GitHub', 'Annotated ground-truth documents from corresponding sources for each problem', 'Evaluated retrieval performance using NDCG@10', 'Evaluated code generation using execution-based pass@k metric', 'Tested multiple retrieval models (BM25, dense embeddings, proprietary APIs) and code generation models']","For the problem 'Pandas map multiple columns based on specific conditions' from the open-domain category, a relevant StackOverflow post was retrieved: 

# fill NaN values with old values
out['nid'] = out['nid'].fillna(out['id'])

This post provides a code snippet demonstrating how to use pandas.fillna() to map values in one column based on conditions from another column, which is applicable to solving the problem.","While language models (LMs) have proven remarkably adept at generating code,
many programs are challenging for LMs to generate using their parametric
knowledge alone. Providing external contexts such as library documentation can
facilitate generating accurate and functional code. Despite the success of
retrieval-augmented generation (RAG) in various text-oriented tasks, its
potential for improving code generation remains under-explored. In this work,
we conduct a systematic, large-scale analysis by asking: in what scenarios can
retrieval benefit code generation models? and what challenges remain? We first
curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three
categories of code generation tasks, including basic programming, open-domain,
and repository-level problems. We aggregate documents from five sources for
models to retrieve contexts: competition solutions, online tutorials, library
documentation, StackOverflow posts, and GitHub repositories. We examine
top-performing models on CodeRAG-Bench by providing contexts retrieved from one
or multiple sources. While notable gains are made in final code generation by
retrieving high-quality contexts across various settings, our analysis reveals
room for improvement -- current retrievers still struggle to fetch useful
contexts especially with limited lexical overlap, and generators fail to
improve with limited context lengths or abilities to integrate additional
contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage
further development of advanced code-oriented RAG methods.",http://arxiv.org/abs/2406.14497v1
CoSQA+: Enhancing Code Search Dataset with Matching Code,"code search benchmark, query-code matching, multi-choice code retrieval, automated annotation, large language model annotation",2024-06,"This paper introduces CoSQA+, a new code search benchmark that pairs high-quality natural language queries with multiple relevant code snippets. It enhances the dataset construction process through automated labeling using large language models and code generation for unmatched queries. A new metric MMRR is proposed to evaluate one-to-many code search performance.",1. Introduction 2. Related Work 3. CoSQA+ 3.1 Query and Code Collection 3.2 Candidate Pairs Construction 3.3 Model Annotation 3.4 Missing Code Generation 4. Experiments 5. Conclusion,"['Collect queries from CoSQA and code snippets from CodeSearchNet and StaQC', 'Calculate embedding similarity of query-code pairs using multiple models (CodeBERT, UniXcoder, CodeT5+)', 'Select top candidate pairs for annotation', 'Use Claude 3 Sonnet with Chain-of-Thought prompting to automatically annotate pairs', 'Use GPT-4o to generate missing codes for unmatched queries', 'Propose new metric Mean Multi-choice Reciprocal Rank (MMRR) for one-to-N code search evaluation']","For the query 'python how to check if query dict is empty', the annotated matching code is: 

def _is_empty(cls, value):
    if isinstance(value, (dict, tuple, list)) and len(value) == 0:
        ret = True
    else:
        ret = False
    return ret","Semantic code search, retrieving code that matches a given natural language
query, is an important task to improve productivity in software engineering.
Existing code search datasets are problematic: either using unrealistic
queries, or with mismatched codes, and typically using one-to-one query-code
pairing, which fails to reflect the reality that a query might have multiple
valid code matches. This paper introduces CoSQA+, pairing high-quality queries
(reused from CoSQA) with multiple suitable codes. We collect code candidates
from diverse sources and form candidate pairs by pairing queries with these
codes. Utilizing the power of large language models (LLMs), we automate pair
annotation, filtering, and code generation for queries without suitable
matches. Through extensive experiments, CoSQA+ has demonstrated superior
quality over CoSQA. Models trained on CoSQA+ exhibit improved performance.
Furthermore, we propose a new metric Mean Multi-choice Reciprocal Rank (MMRR),
to assess one-to-N code search performance. We provide the code and data at
https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.",http://arxiv.org/abs/2406.11589v2
GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model,"code context graph, graph-based code retrieval, decay-with-distance similarity, repository-level code completion, retrieval-augmented generation",2024-06,GraphCoder is a framework that enhances repository-level code completion by leveraging a code context graph representation and graph-based retrieval to augment language models with repository-specific knowledge. It achieves higher accuracy while being more efficient compared to baseline methods.,"1. Introduction 2. Related Work 3. Basic Concepts (code context graph, CCG slicing) 4. GraphCoder 4.1 Overview 4.2 Database Construction 4.3 Code Retrieval 4.4 Code Generation","['- Represents code context using a code context graph (CCG) capturing control flow, data dependence, and control dependence', '- Performs coarse-grained retrieval based on context sequence similarity', '- Re-ranks results using fine-grained decay-with-distance structural similarity between CCGs', '- Generates code by prompting a language model with the retrieved context-similar code snippets']","For the statement 'cur_key_val = cur_key_val + cross_cur_key_val', its 1-hop CCG slice includes statements it has data/control dependence on (lines 2, 12, 13), its 1-hop control-flow neighbor (line 12) and that neighbor's dependences (lines 7-9). This structured slice is used to retrieve relevant snippets from the database.","The performance of repository-level code completion depends upon the
effective leverage of both general and repository-specific knowledge. Despite
the impressive capability of code LLMs in general code completion tasks, they
often exhibit less satisfactory performance on repository-level completion due
to the lack of repository-specific knowledge in these LLMs. To address this
problem, we propose GraphCoder, a retrieval-augmented code completion framework
that leverages LLMs' general code knowledge and the repository-specific
knowledge via a graph-based retrieval-generation process. In particular,
GraphCoder captures the context of completion target more accurately through
code context graph (CCG) that consists of control-flow, data- and
control-dependence between code statements, a more structured way to capture
the completion target context than the sequence-based context used in existing
retrieval-augmented approaches; based on CCG, GraphCoder further employs a
coarse-to-fine retrieval process to locate context-similar code snippets with
the completion target from the current repository. Experimental results
demonstrate both the effectiveness and efficiency of GraphCoder: Compared to
baseline retrieval-augmented methods, GraphCoder achieves higher exact match
(EM) on average, with increases of +6.06 in code match and +6.23 in identifier
match, while using less time and space.",http://arxiv.org/abs/2406.07003v2
A Lightweight Framework for Adaptive Retrieval In Code Completion With Critique Model,"adaptive retrieval, critique model, uncertainty estimation, code completion, retrieval-augmented generation",2024-06,"The paper introduces CARD, a lightweight framework that enhances the efficiency and effectiveness of retrieval-augmented generation (RAG) for code completion tasks. It provides two key functions: isRetrieve to determine if retrieval is necessary, and Select to choose the best prediction among multiple candidates.","1. Introduction - Motivation and overview of CARD 2. Methodology - Details on uncertainty estimation, adaptive retrieval, and selective accept functions 3. Experimental Setup - Datasets, evaluation metrics, target RAG system, training details 4. Results and Analysis - Evaluation of CARD on code completion benchmarks 5. Related Work 6. Conclusion","['- Use uncertainty estimation model (Estimator) to score code predictions based on entropy and probability features', '- isRetrieve function decides if retrieval is needed based on Estimator score and threshold', '- Select function chooses best prediction among candidates based on Estimator scores', '- CARD can be integrated into any RAG-based code completion system, for single or iterative RAG']","For the line completion task on the RepoEval dataset using CodeLlama-7B, CARD saves 21-46% of retrievals while improving accuracy over the baseline RAG system. It reduces latency by 16-83% across different tasks and models.","Recent advancements in Retrieval-Augmented Generation have significantly
enhanced code completion at the repository level. Various RAG-based code
completion systems are proposed based on different design choices. For
instance, gaining more effectiveness at the cost of repeating the
retrieval-generation process multiple times. However, the indiscriminate use of
retrieval in current methods reveals issues in both efficiency and
effectiveness, as a considerable portion of retrievals are unnecessary and may
introduce unhelpful or even harmful suggestions to code language models. To
address these challenges, we introduce CARD, a lightweight critique method
designed to provide insights into the necessity of retrievals and select the
optimal answer from multiple predictions. CARD can seamlessly integrate into
any RAG-based code completion system. Our evaluation shows that CARD saves 21%
to 46% times of retrieval for Line completion, 14% to 40% times of retrieval
for API completion, and 6% to 46.5% times of retrieval for function completion
respectively, while improving the accuracy. CARD reduces latency ranging from
16% to 83%. CARD is generalizable to different LMs, retrievers, and programming
languages. It is lightweight with training in few seconds and inference in few
milliseconds.",http://arxiv.org/abs/2406.10263v1
On The Importance of Reasoning for Context Retrieval in Repository-Level Code Editing,"context retrieval, repository-level code editing, reasoning, code structure-aware tools, iterative retrieval",2024-06,This paper investigates the role of reasoning and code structure-aware tools in improving context retrieval for repository-level code editing tasks. The authors conduct experiments decoupling context retrieval from other components to analyze its strengths and weaknesses.,1. Introduction 2. Related Work 3. Experiments & Results 3.1 Models 3.2 Datasets 3.3 Context Retrieval Strategies 3.4 Metrics 3.5 Results & Discussion 4. Conclusion Limitations,"['Used GPT-3.5 Turbo LLM for experiments', 'Evaluated on SWE-bench Lite and LCA Code Editing datasets', 'Tested context retrieval strategies varying in reasoning complexity (baseline, Context Length, Tool Call, Self-Reflection) and tools (BM25, code structure-aware tools)', 'Measured precision, recall, and F1 score at file and code entity levels']","For the LCA Code Editing dataset, using the AutoCodeRover strategy with code structure-aware tools and Self-Reflection reasoning achieved 49.6% precision and 31.4% recall at the file level, outperforming simpler strategies like the BM25 baseline (25.3% precision, 25.3% recall).","Recent advancements in code-fluent Large Language Models (LLMs) enabled the
research on repository-level code editing. In such tasks, the model navigates
and modifies the entire codebase of a project according to request. Hence, such
tasks require efficient context retrieval, i.e., navigating vast codebases to
gather relevant context. Despite the recognized importance of context
retrieval, existing studies tend to approach repository-level coding tasks in
an end-to-end manner, rendering the impact of individual components within
these complicated systems unclear. In this work, we decouple the task of
context retrieval from the other components of the repository-level code
editing pipelines. We lay the groundwork to define the strengths and weaknesses
of this component and the role that reasoning plays in it by conducting
experiments that focus solely on context retrieval. We conclude that while the
reasoning helps to improve the precision of the gathered context, it still
lacks the ability to identify its sufficiency. We also outline the ultimate
role of the specialized tools in the process of context gathering. The code
supplementing this paper is available at
https://github.com/JetBrains-Research/ai-agents-code-editing.",http://arxiv.org/abs/2406.04464v1
Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion,"dataflow analysis, type-sensitive relations, repo-specific context graph, fine-grained import information, prompt generation",2024-05,"This paper proposes DRACO, a dataflow-guided retrieval augmentation approach for repository-level code completion. It leverages extended dataflow analysis to precisely retrieve relevant background knowledge from a repo-specific context graph and generate well-formed prompts to query large language models.",1. Introduction 2. Related Work 3. Methodology 3.1 Dataflow Analysis 3.2 Repo-specific Context Graph 3.3 Dataflow-Guided Retrieval 3.4 Prompt Generation 4. Experiment Setup 4.1 Datasets 4.2 Evaluation Metrics 4.3 Baselines 4.4 Language Models 5. Results and Analysis 6. Conclusion,"['- Extend traditional dataflow analysis by setting type-sensitive dependency relations (assigns, as, refers, typeof, inherits)', '- Parse repository into code entities (modules, classes, functions, variables) and establish relations through dataflow analysis to form a repo-specific context graph', '- Identify fine-grained import information from unfinished code using dataflow analysis', '- Retrieve relevant code entities from context graph based on import information', '- Restore retrieved entities to source code and concatenate with unfinished code to generate prompts for language models']","Given the unfinished code:

newSignal = signal.getSignalByName(newChannelName)
newSignal.channel = newChannelName
setSignalTypeFromTypeStr() 

DRACO retrieves the relevant background knowledge:

1) The definition of Signal class showing setSignalTypeFromTypeStr method
2) The definition of RecordSignal class and getSignalByName method returning Signal object

This provides the model context that newSignal is a Signal object, so it can complete the line:
newSignal.setSignalTypeFromTypeStr()","Recent years have witnessed the deployment of code language models (LMs) in
various code intelligence tasks such as code completion. Yet, it is challenging
for pre-trained LMs to generate correct completions in private repositories.
Previous studies retrieve cross-file context based on import relations or text
similarity, which is insufficiently relevant to completion targets. In this
paper, we propose a dataflow-guided retrieval augmentation approach, called
DraCo, for repository-level code completion. DraCo parses a private repository
into code entities and establishes their relations through an extended dataflow
analysis, forming a repo-specific context graph. Whenever triggering code
completion, DraCo precisely retrieves relevant background knowledge from the
repo-specific context graph and generates well-formed prompts to query code
LMs. Furthermore, we construct a large Python dataset, ReccEval, with more
diverse completion targets. Our experiments demonstrate the superior accuracy
and applicable efficiency of DraCo, improving code exact match by 3.43% and
identifier F1-score by 3.27% on average compared to the state-of-the-art
approach.",http://arxiv.org/abs/2405.19782v1
Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding Recommendation,"multi-stage retrieval, contrastive re-ranking, code co-occurrence, auxiliary knowledge, ICD coding recommendation",2024-05,"This paper proposes a novel multi-stage 'retrieve and re-rank' framework for automatic medical coding recommendation from clinical notes. It first retrieves a candidate subset of ICD codes using auxiliary knowledge and BM25, then re-ranks the candidates via contrastive learning guided by code co-occurrences.","1. Introduction 2. Related Work 3. Method: 3.1 Multi-stage Framework Overview, 3.2 Retrieval Stage, 3.3 Re-ranking Stage, 3.4 Training 4. Experiments 5. Results 6. Conclusion","['Two-stage retrieval: 1) Use auxiliary knowledge (DRG, CPT codes, medications) and conditional probabilities to retrieve candidate ICD subset. 2) Further reduce candidates using BM25 lexical matching.', 'Re-ranking via contrastive learning: 1) Encode clinical text with Clinical-Longformer. 2) Encode ICD codes with Graphormer using code co-occurrence graph. 3) Contrastive loss to pull positive ICD codes closer to clinical text.']","For the clinical note about Alzheimer's and dementia in Figure 1, the model first retrieves candidates like 294.10 (dementia) using auxiliary knowledge of prescribed 'Namenda' drug. It then re-ranks 331.0 (Alzheimer's) higher using the code co-occurrence between dementia and Alzheimer's codes.","The International Classification of Diseases (ICD) serves as a definitive
medical classification system encompassing a wide range of diseases and
conditions. The primary objective of ICD indexing is to allocate a subset of
ICD codes to a medical record, which facilitates standardized documentation and
management of various health conditions. Most existing approaches have suffered
from selecting the proper label subsets from an extremely large ICD collection
with a heavy long-tailed label distribution. In this paper, we leverage a
multi-stage ``retrieve and re-rank'' framework as a novel solution to ICD
indexing, via a hybrid discrete retrieval method, and re-rank retrieved
candidates with contrastive learning that allows the model to make more
accurate predictions from a simplified label space. The retrieval model is a
hybrid of auxiliary knowledge of the electronic health records (EHR) and a
discrete retrieval method (BM25), which efficiently collects high-quality
candidates. In the last stage, we propose a label co-occurrence guided
contrastive re-ranking model, which re-ranks the candidate labels by pulling
together the clinical notes with positive ICD codes. Experimental results show
the proposed method achieves state-of-the-art performance on a number of
measures on the MIMIC-III benchmark.",http://arxiv.org/abs/2405.19093v1
"Learning to Reason via Program Generation, Emulation, and Search","program generation, program emulation, pseudo-programs, code search, task adaptation",2024-05,"The paper proposes COGEX, a novel approach that trains language models to generate and emulate the execution of pseudo-programs containing underspecified functions to tackle both algorithmic and soft reasoning tasks. It also introduces COTACS, a program search method to find an optimal program for a given dataset without updating model parameters.",1. Introduction 2. Approach 2.1 COGEX Formulation 2.2 Program Search: COTACS 3. Experiments and Results 3.1 Experimental Setup 3.2 Main Results 3.3 Ablation Studies,"['Train language models to generate Python programs from natural language instructions', ""Allow programs to include underspecified functions whose implementations are filled in by the model's knowledge during emulated execution"", 'Use COTACS program search to find an optimal program for a given dataset by evaluating many candidate programs', 'Fine-tune models on a COGEX dataset derived from the Alpaca instruction tuning dataset']","For the task of pluralizing a word, the model generates the following program:

def pluralize_word(word):
    # 1. Identify the suffix of the word
    ending = identify_ending(word)
    # 2. Identify the pluralization rule for the word 
    pluralization_rule = find_pluralization_rule(word, ending)
    # 3. Apply the pluralization rule to the word
    plural_form = apply_pluralization_rule(word, ending)
    return {'original_word': word, 'ending': ending, 'rule': pluralization_rule, 'answer': plural_form}

The model then emulates the execution of this program, filling in the semantics of the underspecified identify_ending, find_pluralization_rule, and apply_pluralization_rule functions using its knowledge.","Program synthesis with language models (LMs) has unlocked a large set of
reasoning abilities; code-tuned LMs have proven adept at generating programs
that solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word
concatenation). However, not all reasoning tasks are easily expressible as
code, e.g. tasks involving commonsense reasoning, moral decision-making, and
sarcasm understanding. Our goal is to extend an LM's program synthesis skills
to such tasks and evaluate the results via pseudo-programs, namely Python
programs where some leaf function calls are left undefined. To that end, we
propose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)
training LMs to generate pseudo-programs, (2) teaching them to emulate their
generated program's execution, including those leaf functions, allowing the
LM's knowledge to fill in the execution gaps; and (3) using them to search over
many programs to find an optimal one. To adapt the CoGEX model to a new task,
we introduce a method for performing program search to find a single program
whose pseudo-execution yields optimal performance when applied to all the
instances of a given dataset. We show that our approach yields large
improvements compared to standard in-context learning approaches on a battery
of tasks, both algorithmic and soft reasoning. This result thus demonstrates
that code synthesis can be applied to a much broader class of problems than
previously considered. Our released dataset, fine-tuned models, and
implementation can be found at \url{https://github.com/nweir127/CoGEX}.",http://arxiv.org/abs/2405.16337v3
Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search,"Code World Models, Monte Carlo Tree Search, Program Synthesis, Model-Based Reinforcement Learning, Large Language Models",2024-05,"This paper introduces Code World Models, a novel approach to generate world models for reinforcement learning as executable Python code using large language models. It proposes Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy tailored for this task. The authors also introduce the Code World Models Benchmark for evaluating such methods.","{'1. Introduction': 'Motivates using code for world modeling and introduces Code World Models', '2. Related Work': 'Discusses prior work on world models with code and code generation with LLMs', '3. Code World Models': 'Formalizes the problem and introduces the Code World Models Benchmark', '4. GIF-MCTS': 'Presents the proposed GIF-MCTS algorithm for code generation', '5. Experiments': 'Evaluates GIF-MCTS on coding benchmarks and using the generated CWMs for planning', '6. Discussion': 'Analyzes limitations and future directions'}","['Formulates generating a Code World Model as writing a Python class implementing the environment dynamics', 'Proposes GIF-MCTS, a Monte Carlo Tree Search based approach to iteratively generate, improve and fix code using an LLM', 'Leverages LLM prompting, self-reflection, and feedback from environment trajectories as unit tests', 'Introduces the Code World Models Benchmark with 18 RL environments and associated data']","For the CartPole environment, GIF-MCTS could generate a CWM like:

import numpy as np

class Environment:
    def __init__(self):
        self.cart_position = np.random.uniform(-0.2, 0.2)
        self.cart_velocity = np.random.uniform(-0.2, 0.2)
        self.pole_angle = np.random.uniform(-0.1, 0.1) 
        self.pole_velocity = np.random.uniform(-0.1, 0.1)

    def step(self, action):
        # Implement CartPole dynamics
        ...
        return next_state, reward, done","In this work we consider Code World Models, world models generated by a Large
Language Model (LLM) in the form of Python code for model-based Reinforcement
Learning (RL). Calling code instead of LLMs for planning has potential to be
more precise, reliable, interpretable, and extremely efficient. However,
writing appropriate Code World Models requires the ability to understand
complex instructions, to generate exact code with non-trivial logic and to
self-debug a long program with feedback from unit tests and environment
trajectories. To address these challenges, we propose Generate, Improve and Fix
with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for
LLMs. To test our approach in an offline RL setting, we introduce the Code
World Models Benchmark (CWMB), a suite of program synthesis and planning tasks
comprised of 18 diverse RL environments paired with corresponding textual
descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the
CWMB and two other benchmarks, and we show that the Code World Models
synthesized with it can be successfully used for planning, resulting in
model-based RL agents with greatly improved sample efficiency and inference
speed.",http://arxiv.org/abs/2405.15383v2
Large Neighborhood Prioritized Search for Combinatorial Optimization with Answer Set Programming,"answer set programming, large neighborhood prioritized search, combinatorial optimization, heuristic-driven search, multi-shot solving",2024-05,"The paper proposes a new metaheuristic called Large Neighborhood Prioritized Search (LNPS) for solving combinatorial optimization problems using Answer Set Programming (ASP). LNPS alternates between destroying parts of the current solution and reconstructing it via prioritized search, allowing more flexible exploration compared to traditional Large Neighborhood Search.",1. Introduction 2. Background on ASP 3. Large Neighborhood Prioritized Search algorithm 4. heulingo: ASP-based implementation of LNPS 5. Experimental evaluation 6. Conclusion,"['- Defines LNPS as an iterative algorithm that starts with an initial solution and alternates between destroying parts of the current solution (destroy operator) and reconstructing it via prioritized systematic search (prioritized-search operator)', '- Implements LNPS using ASP by leveraging multi-shot solving and heuristic statements in clingo', '- Uses #heuristic statements to configure the prioritized search based on remaining undestroyed parts', '- Supports traditional LNS as a special case when undestroyed parts are fixed']","For the Traveling Salesperson Problem (TSP), LNPS could start with an initial Hamiltonian cycle and iteratively destroy and reconstruct parts of the cycle guided by heuristics like keeping high priority edges intact. The prioritized search focuses on reconstructing the destroyed parts optimally while reusing the undestroyed high priority edges.","We propose Large Neighborhood Prioritized Search (LNPS) for solving
combinatorial optimization problems in Answer Set Programming (ASP). LNPS is a
metaheuristic that starts with an initial solution and then iteratively tries
to find better solutions by alternately destroying and prioritized searching
for a current solution. Due to the variability of neighborhoods, LNPS allows
for flexible search without strongly depending on the destroy operators. We
present an implementation of LNPS based on ASP. The resulting heulingo solver
demonstrates that LNPS can significantly enhance the solving performance of ASP
for optimization. Furthermore, we establish the competitiveness of our LNPS
approach by empirically contrasting it to (adaptive) large neighborhood search.",http://arxiv.org/abs/2405.11305v1
Prompt-based Code Completion via Multi-Retrieval Augmented Generation,"prompt-based multi-retrieval, adaptive retrieval selection, contextual multi-armed bandits, code semantics perspectives, retrieval augmented generation",2024-05,"This paper proposes ProCC, a novel code completion framework that leverages prompt engineering and contextual multi-armed bandits to flexibly incorporate and adapt to multiple perspectives of code semantics for retrieval augmented generation.",1. Introduction 2. Background and Motivation 2.1 Code Completion 2.2 Retrieval-Augmented Generation 2.3 Motivation 3. ProCC Framework 3.1 Prompt-based Multi-Retriever System 3.2 Adaptive Retrieval Selection Algorithm 4. Experiments 4.1 Experimental Setup 4.2 Overall Results 4.3 Component Analysis 4.4 Case Studies 5. Related Work 6. Conclusion,"['Prompt-based Multi-Retriever System: Crafts prompts to elicit LLM knowledge for understanding code semantics from multiple perspectives (lexical semantics, hypothetical line, code summarization)', 'Adaptive Retrieval Selection Algorithm: Employs contextual multi-armed bandits to determine the most suitable retrieval perspective based on similarity between retrieved snippets and incomplete code']","For the incomplete code snippet: 'private void addSSLConfiguration(ClientBuilder clientBuilder) {...}', the lexical semantics prompt retrieves relevant code like 'clientBuilder.sslContext(SSLContext.getDefault());', while the hypothetical line prompt generates 'clientBuilder.sslContext(sslContext);' as a potential completion. The adaptive algorithm then selects the hypothetical line perspective as more relevant for completing the SSL configuration.","Automated code completion, aiming at generating subsequent tokens from
unfinished code, has been significantly benefited from recent progress in
pre-trained Large Language Models (LLMs). However, these models often suffer
from coherence issues and hallucinations when dealing with complex code logic
or extrapolating beyond their training data. Existing Retrieval Augmented
Generation (RAG) techniques partially address these issues by retrieving
relevant code with a separate encoding model where the retrieved snippet serves
as contextual reference for code completion. However, their retrieval scope is
subject to a singular perspective defined by the encoding model, which largely
overlooks the complexity and diversity inherent in code semantics. To address
this limitation, we propose ProCC, a code completion framework leveraging
prompt engineering and the contextual multi-armed bandits algorithm to flexibly
incorporate and adapt to multiple perspectives of code. ProCC first employs a
prompt-based multi-retriever system which crafts prompt templates to elicit LLM
knowledge to understand code semantics with multiple retrieval perspectives.
Then, it adopts the adaptive retrieval selection algorithm to incorporate code
similarity into the decision-making process to determine the most suitable
retrieval perspective for the LLM to complete the code. Experimental results
demonstrate that ProCC outperforms state-of-the-art code completion technique
by 8.6% on our collected open-source benchmark suite and 10.1% on the
private-domain benchmark suite collected from a billion-user e-commerce company
in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in
a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned
model.",http://arxiv.org/abs/2405.07530v1
Refining Joint Text and Source Code Embeddings for Retrieval Task with Parameter-Efficient Fine-Tuning,"code retrieval, contrastive learning, CodeT5+, parameter-efficient fine-tuning, bimodal embeddings",2024-05,The paper proposes a fine-tuning framework that leverages parameter-efficient fine-tuning (PEFT) techniques and contrastive learning to improve the quality of bimodal text-code representations learned by the CodeT5+ model for code retrieval tasks. The approach tunes only a small fraction of model parameters while achieving performance comparable to larger models.,"1. Introduction - Motivation and background 2. Related Work - Prior work on code retrieval, PEFT, and contrastive learning 3. Methodology - Proposed fine-tuning approach with contrastive loss and PEFT methods 4. Experimental Setup - Datasets, implementation details 5. Results and Discussion - Performance evaluation and analysis 6. Conclusion and Future Work","['- Use contrastive loss to align representations of matching code-text pairs in a joint feature space', '- Fine-tune CodeT5+ encoder using PEFT methods like LoRA, AdaLoRA, (IA)3, and Prompt-Tuning', '- Train a small number of newly added parameters while freezing pre-trained weights', '- Perform fine-tuning separately for each programming language']","For the Python programming language, the approach learns to map the natural language description 'function to calculate the factorial of a number' to the corresponding Python code snippet:

def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n-1)","The latest developments in Natural Language Processing (NLP) have
demonstrated remarkable progress in a code-text retrieval problem. As the
Transformer-based models used in this task continue to increase in size, the
computational costs and time required for end-to-end fine-tuning become
substantial. This poses a significant challenge for adapting and utilizing
these models when computational resources are limited. Motivated by these
concerns, we propose a fine-tuning framework that leverages Parameter-Efficient
Fine-Tuning (PEFT) techniques. Moreover, we adopt contrastive learning
objectives to improve the quality of bimodal representations learned by
transformer models. Additionally, for PEFT methods we provide extensive
benchmarking, the lack of which has been highlighted as a crucial problem in
the literature. Based on the thorough experimentation with the CodeT5+ model
conducted on two datasets, we demonstrate that the proposed fine-tuning
framework has the potential to improve code-text retrieval performance by
tuning only 0.4% parameters at most.",http://arxiv.org/abs/2405.04126v1
CodeGRAG: Bridging the Gap between Natural Language and Programming Language via Graphical Retrieval Augmented Generation,"graphical code representation, control flow graphs, data flow graphs, code structure understanding, cross-lingual code generation",2024-05,"The paper proposes CodeGRAG, a framework that enhances large language models for code generation by representing code blocks as graphical views capturing control flow and data flow. This graphical representation bridges the gap between natural language and programming languages, enabling better code understanding and cross-lingual generation.",1. Introduction - Motivation and challenges 2. Methodology 2.1 Overview 2.2 Graphical Knowledge Base Preparation 2.3 Knowledge Querying 2.4 Graphical Knowledge Augmented Generation 2.4.1 Hard Meta-Graph Prompt 2.4.2 Soft Prompting with Expert,"['- Extract graphical views of code blocks by combining data flow graphs and control flow graphs with read-write signals', '- Build a knowledge base of code blocks and their graphical views', '- Query the knowledge base using natural language prompts to retrieve relevant graphical views', '- Use hard meta-graph prompts with the retrieved views to stimulate LLMs for code generation', '- Use soft prompting by finetuning LLMs with a graph neural network expert model encoding the graphical views']","For the task of generating a function to perform XOR on two binary strings, CodeGRAG would: 1) Extract the graphical view capturing the control flow (loops, conditionals) and data flow of the XOR operation, 2) Retrieve this view from the knowledge base based on the task description, 3) Either use the meta-graph prompt containing the topology of this view to prompt an LLM, or finetune the LLM with the graph embedding from the expert GNN model to inject the graphical knowledge.","Utilizing large language models to generate codes has shown promising meaning
in software development revolution. Despite the intelligence shown by the
general large language models, their specificity in code generation can still
be improved due to the syntactic gap and mismatched vocabulary existing among
natural language and different programming languages. In this paper, we propose
CodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance
the performance of LLMs. CodeGRAG builds the graphical view of code blocks
based on the control flow and data flow of them to fill the gap between
programming languages and natural language, which can facilitate natural
language based LLMs for better understanding of code syntax and serve as a
bridge among different programming languages. To take the extracted structural
knowledge into the foundation models, we propose 1) a hard meta-graph prompt
template to transform the challenging graphical representation into informative
knowledge for tuning-free models and 2) a soft prompting technique that injects
the domain knowledge of programming languages into the model parameters via
finetuning the models with the help of a pretrained GNN expert model. Various
experiments and ablations are done on four datasets including both the C++ and
python languages to validate the hard meta-graph prompt, the soft prompting
technique, and the effectiveness of the objectives for pretrained GNN expert.
CodeGRAG improves the code generation ability of LLMs and can even offer
performance gain for cross-lingual code generation. Code is available at
https://anonymous.4open.science/r/Code-5970/.",http://arxiv.org/abs/2405.02355v3
PyRadar: Towards Automatically Retrieving and Validating Source Code Repository Information for PyPI Packages,"source code provenance, package repository linking, phantom file analysis, metadata-based retrieval, source code-based retrieval",2024-04,"This paper proposes PyRadar, a novel framework that utilizes package metadata and source code to automatically retrieve and validate the source code repository information for Python packages on PyPI. It addresses limitations of existing metadata-based tools by incorporating a validator for incorrect metadata and a retriever using source code when metadata is missing.",1. Introduction 2. Background 3. Data Collection 4. Empirical Study 4.1 RQ1: Analyzing Existing Metadata-based Retrieval Tools 4.2 RQ2: Analyzing Phantom File Differences 5. PyRadar Framework 5.1 Metadata-based Retriever 5.2 Source Code Repository Validator 5.3 Source Code-based Retriever 6. Evaluation 7. Discussion 8. Conclusion,"['Conducted large-scale empirical study on 4.2M PyPI releases to analyze existing metadata-based tools (RQ1) and phantom file differences between correct/incorrect links (RQ2)', 'Proposed heuristic approach to collect 14,375 correct and 2,064 incorrect package-repository links', 'Designed PyRadar framework with 3 components: - Metadata-based Retriever (combines best practices from existing tools) - Source Code Repository Validator (uses crafted features and ML models) - Source Code-based Retriever (queries World of Code with file hashes)']","For the Python package 'requests', PyRadar successfully retrieved its source code repository on GitHub (https://github.com/psf/requests) in the following way: 1. Metadata-based Retriever extracted the URL from metadata 2. Source Code Repository Validator confirmed it was correct based on low phantom file count 3. Source Code-based Retriever also retrieved the same URL by matching file hashes in the source distribution to the requests repository","A package's source code repository records the development history of the
package, providing indispensable information for the use and risk monitoring of
the package. However, a package release often misses its source code repository
due to the separation of the package's development platform from its
distribution platform. Existing tools retrieve the release's repository
information from its metadata, which suffers from two limitations: the metadata
may not contain or contain wrong information. Our analysis shows that existing
tools can only retrieve repository information for up to 70.5% of PyPI
releases. To address the limitations, this paper proposes PyRadar, a novel
framework that utilizes the metadata and source distribution to retrieve and
validate the repository information for PyPI releases. We start with an
empirical study to compare four existing tools on 4,227,425 PyPI releases and
analyze phantom files (files appearing in the release's distribution but not in
the release's repository) in 14,375 correct package-repository links and 2,064
incorrect links. Based on the findings, we design PyRadar with three
components, i.e., Metadata-based Retriever, Source Code Repository Validator,
and Source Code-based Retriever. In particular, the Metadata-based Retriever
combines best practices of existing tools and successfully retrieves repository
information from the metadata for 72.1% of PyPI releases. The Source Code
Repository Validator applies common machine learning algorithms on six crafted
features and achieves an AUC of up to 0.995. The Source Code-based Retriever
queries World of Code with the SHA-1 hashes of all Python files in the
release's source distribution and retrieves repository information for 90.2% of
packages in our dataset with an accuracy of 0.970. Both practitioners and
researchers can employ the PyRadar to better use PyPI packages.",http://arxiv.org/abs/2404.16565v1
Search-based Automated Program Repair of CPS Controllers Modeled in Simulink-Stateflow,"Stateflow model repair, Cyber-Physical Systems, Simulink, Search-based program repair, Automated fault localization",2024-04,"The paper proposes FlowRepair, a search-based automated program repair approach specifically designed to fix bugs in Stateflow models that control Cyber-Physical Systems modeled in Simulink. It introduces a novel repair algorithm combining global and local search, new repair objectives tailored for CPS, and mutation operators for Stateflow models.","{'1. Introduction': 'Motivates the need for automated repair of Simulink/Stateflow models and outlines the key challenges addressed.', '2. Background': 'Provides background on automated program repair and Stateflow models.', '3. FlowRepair': 'Describes the proposed FlowRepair approach, including the repair algorithm, objectives, and mutation operators.', '4. Empirical Evaluation': 'Presents the evaluation of FlowRepair on case study systems and faulty Stateflow models.', '5. Results': 'Discusses the results of the empirical evaluation.', '6. Threats to Validity': 'Examines potential threats to the validity of the study.', '7. Related Work': 'Positions the work with respect to relevant literature.', '8. Conclusion': 'Summarizes the key contributions and findings.'}","['- Employs spectrum-based fault localization to narrow the search space for repairs', '- Combines global search (to explore different patch types) and local search (to exploit promising partial patches)', '- Defines novel repair objectives tailored for CPS, considering time, failure severity, etc.', '- Develops mutation operators specifically for repairing Stateflow models']",The paper evaluates FlowRepair on a Stateflow model controlling the temperature of a fridge. One fault involved the system not triggering an alarm when the door was left open for 15 seconds. FlowRepair was able to generate a patch that correctly activated the alarm in this scenario.,"Stateflow models are widely used in the industry to model the high-level
control logic of Cyber-Physical Systems (CPSs) in Simulink--the defacto CPS
simulator. Many approaches exist to test Simulink models, but once a fault is
detected, the process to repair it remains manual. Such a manual process
increases the software development cost, making it paramount to develop novel
techniques that reduce this cost. Automated Program Repair (APR) techniques can
significantly reduce the time for fixing bugs by automatically generating
patches. However, current approaches face scalability issues to be applicable
in the CPS context. To deal with this problem, we propose an automated
search-based approach called FlowRepair, explicitly designed to repair
Stateflow models. The novelty of FlowRepair includes, (1) a new algorithm that
combines global and local search for patch generation; (2) a definition of
novel repair objectives (e.g., the time a fault remained active) specifically
designed for repairing CPSs; and (3) a set of mutation operators to repair
Stateflow models automatically. We evaluated FlowRepair with three different
case study systems and a total of nine faulty stateflow models. Our experiments
suggest that (1) Flo wRepaircan fix bugs in stateflow models, including models
with multiple faults; (2) FlowRepair surpasses or performs similarly to a
baseline APR technique inspired by a well-known CPS program repair approach.
Besides, we provide both a replication package and a live repository, paving
the way towards the APR of CPSs modeled in Simulink.",http://arxiv.org/abs/2404.04688v1
Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization,"symbolic prompt programs, structure-aware optimization, compile-time prompt tuning, retrieval augmented generation, prompt compression",2024-04,"This paper introduces SAMMO, a framework for optimizing complex prompt programs at compile-time through symbolic representations and structure-aware mutations. SAMMO generalizes and improves upon previous prompt optimization methods.","1. Introduction - Motivates compile-time prompt program optimization 2. Symbolic Prompt Programs - Defines representation and execution model 3. Compile-Time Prompt Optimization - Formalizes the optimization problem 4. SAMMO Framework - Describes search algorithms and mutation operators 5. Experiments - Evaluates on instruction tuning, RAG pipeline tuning, prompt compression","['Represents prompt programs as symbolic prompt programs (SPPs) - directed acyclic graphs where nodes are functions', 'Defines mutation operators that can modify text, attributes, and program structure of SPPs', 'Implements enumerative search (grid, random) and iterative search (beam search) over SPP space', 'Specializes to methods like APE, GrIPS by choosing appropriate initialization and mutations']","For a binary classification task, an SPP could have nodes for rendering instructions, labels, generating a response, and parsing the output. Mutations may change the text format, remove instructions, or modify the overall structure.","In many modern LLM applications, such as retrieval augmented generation,
prompts have become programs themselves. In these settings, prompt programs are
repeatedly called with different user queries or data instances. A big
practical challenge is optimizing such prompt programs. Recent work has mostly
focused on either simple prompt programs or assumed that the general structure
of a prompt program is fixed.
  We introduce SAMMO, a framework to perform symbolic prompt program search for
compile-time optimizations of prompt programs. SAMMO represents prompt programs
on a symbolic level which allows for a rich set of transformations that can be
searched over during optimization. We show that SAMMO generalizes previous
methods and improves the performance of complex prompts on (1) instruction
tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several
different LLMs. We make all code available open-source at
https://github.com/microsoft/sammo .",http://arxiv.org/abs/2404.02319v2
FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion,"retrieval-augmented code completion, fine-tuning mimicry, logits discrepancy, iterative retrieval, token-level and line-level completion",2024-04,"The paper proposes FT2Ra, a novel retrieval-augmented method for code completion that aims to mimic the effects of fine-tuning without actually fine-tuning the model. It is based on a theoretical analysis that highlights the importance of logits discrepancy (Δlogits) in improving model predictions.",1. Introduction 2. Background and Problem 3. Approach 3.1 Inspiration from Fine-tuning 3.2 FT2Ra Method 4. Experimental Setup 5. Results and Analysis 6. Related Work 7. Conclusion,"['- Theoretical analysis of fine-tuning to derive insights on leveraging retrieved information', '- Approximating Δlogits (change in logits after fine-tuning) without actual fine-tuning', '- Retrieving nearest neighbors and interpolating their Δlogits with model predictions', '- Iterative retrieval process mimicking multi-epoch fine-tuning']","For token-level code completion on the UniXcoder dataset, FT2Ra achieves 74.22% accuracy, a 4.29% improvement over the best baseline kNM-LM (69.93%). For the more challenging line-level completion, FT2Ra obtains 26.32 Exact Match score, around 2x better than kNM-LM (13.93).","The rise of code pre-trained models has significantly enhanced various coding
tasks, such as code completion, and tools like GitHub Copilot. However, the
substantial size of these models, especially large models, poses a significant
challenge when it comes to fine-tuning them for specific downstream tasks. As
an alternative approach, retrieval-based methods have emerged as a promising
solution, augmenting model predictions without the need for fine-tuning.
Despite their potential, a significant challenge is that the designs of these
methods often rely on heuristics, leaving critical questions about what
information should be stored or retrieved and how to interpolate such
information for augmenting predictions.
  To tackle this challenge, we first perform a theoretical analysis of the
fine-tuning process, highlighting the importance of delta logits as a catalyst
for improving model predictions. Building on this insight, we develop a novel
retrieval-based method, FT2Ra, which aims to mimic genuine fine-tuning. While
FT2Ra adopts a retrieval-based mechanism, it uniquely adopts a paradigm with a
learning rate and multi-epoch retrievals, which is similar to fine-tuning.In
token-level completion, which represents a relatively easier task, FT2Ra
achieves a 4.29% improvement in accuracy compared to the best baseline method
on UniXcoder. In the more challenging line-level completion task, we observe a
substantial more than twice increase in Exact Match (EM) performance,
indicating the significant advantages of our theoretical analysis. Notably,
even when operating without actual fine-tuning, FT2Ra exhibits competitive
performance compared to the models with real fine-tuning.",http://arxiv.org/abs/2404.01554v1
ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search,"Community QA Dataset, Code-Mixing Data, Modality-Agnostic Pretraining, Contrastive Code Representation, Large-Scale Code Search",2024-03,"This paper introduces ProCQA, a large-scale programming question answering dataset extracted from StackOverflow. It also proposes a modality-agnostic contrastive pretraining approach that leverages the mixed-modal nature of ProCQA to improve code-text representation alignment, achieving state-of-the-art performance on various code retrieval benchmarks.","{'1. Introduction': 'Motivates the need for a large, diverse code QA dataset and modality-agnostic pretraining', '2. Related Work': 'Discusses prior work on code QA datasets and language models', '3. ProCQA': 'Details the creation, filtering, statistics, and tasks of the proposed ProCQA dataset', '4. Experiments': 'Evaluates the modality-agnostic pretraining approach on code retrieval benchmarks', '5. Analysis': 'Analyzes the pretraining approach through ablations and visualizations', '6. Conclusion': 'Summarizes key contributions and findings'}","['Crawled StackOverflow to create ProCQA, a 5M mixed-modal QA dataset across 11 programming languages', 'Applied rule-based filtering to ensure data quality and fairness', 'Proposed modality-agnostic contrastive pretraining (MACP) on ProCQA to align code-text representations', 'Evaluated MACP on code retrieval tasks: supervised, zero-shot, out-of-domain']",An example QA pair from the C programming subset: The question describes a segmentation fault issue when copying a string. The answer explains the root cause is allocating the wrong data type size for the character array.,"Retrieval-based code question answering seeks to match user queries in
natural language to relevant code snippets. Previous approaches typically rely
on pretraining models using crafted bi-modal and uni-modal datasets to align
text and code representations. In this paper, we introduce ProCQA, a
large-scale programming question answering dataset extracted from the
StackOverflow community, offering naturally structured mixed-modal QA pairs. To
validate its effectiveness, we propose a modality-agnostic contrastive
pre-training approach to improve the alignment of text and code representations
of current code language models. Compared to previous models that primarily
employ bimodal and unimodal pairs extracted from CodeSearchNet for
pre-training, our model exhibits significant performance improvements across a
wide range of code retrieval benchmarks.",http://arxiv.org/abs/2403.16702v1
CoCoST: Automatic Complex Code Generation with Online Searching and Correctness Testing,"online search, query planning, test case generation, code refinement, complex code generation",2024-03,"The paper introduces CoCoST, a framework that enhances complex code generation by leveraging online search, query planning, test case generation, and correctness-driven code refinement. It aims to address challenges in generating intricate code structures by imitating the human developer workflow.","1. Introduction 2. Related Work 3. Methodology (3.1 Retrieval, 3.2 Refinement) 4. Experiments 5. Analysis 6. Conclusion","['Uses LLMs to plan steps and generate queries for online search', 'Conducts online search using the generated queries to retrieve relevant information', 'Generates initial code using LLM with the retrieved information', 'Generates test cases for the initial code using LLM', 'Executes code with test cases, serializes inputs/outputs', 'Refines code based on execution errors and output correctness using LLM']","For a problem of calculating value counts per column in a Pandas DataFrame, CoCoST: 1) Plans steps like 'iterate over columns', 'calculate value counts' 2) Searches online for 'pandas calculate value counts for each column' 3) Generates initial code using search results 4) Generates test cases with sample DataFrames 5) Executes code, serializes DataFrame inputs/outputs 6) Refines code based on correctness of value count results","Large Language Models have revolutionized code generation ability by
converting natural language descriptions into executable code. However,
generating complex code within real-world scenarios remains challenging due to
intricate structures, subtle bugs, understanding of advanced data types, and
lack of supplementary contents. To address these challenges, we introduce the
CoCoST framework, which enhances complex code generation by online searching
for more information with planned queries and correctness testing for code
refinement. Moreover, CoCoST serializes the complex inputs and outputs to
improve comprehension and generates test cases to ensure the adaptability for
real-world applications. CoCoST is validated through rigorous experiments on
the DS-1000 and ClassEval datasets. Experimental results show that CoCoST
substantially improves the quality of complex code generation, highlighting its
potential to enhance the practicality of LLMs in generating complex code.",http://arxiv.org/abs/2403.13583v3
Development and Application of a Monte Carlo Tree Search Algorithm for Simulating Da Vinci Code Game Strategies,"Da Vinci Code game, branch divergence, MCTS parallelization, GPU performance bottlenecks, non-linear scaling",2024-03,"This paper investigates the performance impact of branch divergence on parallelizing the Monte Carlo Tree Search (MCTS) algorithm for the Da Vinci Code board game using GPUs. The authors find that the game's mechanics lead to significant branch divergence, resulting in non-linear scaling and performance degradation on GPUs.",1. Introduction 2. Background 2.1 Da Vinci Code game rules 2.2 Monte Carlo Tree Search algorithm 3. Method 3.1 Designing simplified MCTS algorithm 3.2 CPU implementation (MCTS-CPU) 3.3 GPU implementation (MCTS-GPU) 4. Evaluation 4.1 Execution time analysis 4.2 Simulations per second analysis (CPU vs GPU) 5. Conclusion,"['Developed two MCTS variants: CPU-based (MCTS-CPU) using OpenMP and GPU-based (MCTS-GPU) using CUDA', 'Simplified MCTS algorithm by removing plausible number sets, disallowing consecutive guesses, and limiting tree expansion depth', 'Parallelized with each thread running independent game simulations and contributing to a consolidated MCTS', 'Measured execution time and simulations per second on CPU and GPU to analyze performance bottlenecks']","For the Da Vinci Code game, the authors found that on the CPU, increasing the number of threads led to a linear increase in simulations per second up to the number of physical cores. However, on the GPU, they observed non-linear scaling and performance valleys due to branch divergence from the game's guessing mechanics and resulting memory bottlenecks from cache misses and thread waiting.","In this study, we explore the efficiency of the Monte Carlo Tree Search
(MCTS), a prominent decision-making algorithm renowned for its effectiveness in
complex decision environments, contingent upon the volume of simulations
conducted. Notwithstanding its broad applicability, the algorithm's performance
can be adversely impacted in certain scenarios, particularly within the domain
of game strategy development. This research posits that the inherent branch
divergence within the Da Vinci Code board game significantly impedes
parallelism when executed on Graphics Processing Units (GPUs). To investigate
this hypothesis, we implemented and meticulously evaluated two variants of the
MCTS algorithm, specifically designed to assess the impact of branch divergence
on computational performance. Our comparative analysis reveals a linear
improvement in performance with the CPU-based implementation, in stark contrast
to the GPU implementation, which exhibits a non-linear enhancement pattern and
discernible performance troughs. These findings contribute to a deeper
understanding of the MCTS algorithm's behavior in divergent branch scenarios,
highlighting critical considerations for optimizing game strategy algorithms on
parallel computing architectures.",http://arxiv.org/abs/2403.10720v1
Repoformer: Selective Retrieval for Repository-Level Code Completion,"selective retrieval, self-assessment, repository-level code completion, retrieval augmentation, self-supervised learning",2024-03,"This paper proposes REPOFORMER, a framework for selective retrieval-augmented code completion that avoids unnecessary or detrimental retrievals. A code language model is trained to self-evaluate whether retrieval can improve its predictions and robustly leverage retrieved contexts when needed.","1. Introduction, 2. Related Work, 3. Approach (Problem Formulation, Self-Selective RAG, Self-Supervised Learning), 4. Experimental Setup, 5. Results, 6. Analysis, 7. Conclusion","['Trains a code language model to self-trigger cross-file retrieval by generating a special token or abstaining', 'Uses self-supervised multi-task learning on public repositories to enable accurate self-assessment and robustness to retrieved contexts', 'Contrasts model outputs with and without retrieval to obtain self-supervision labels', 'Optimizes for joint losses on self-assessment accuracy and code generation quality']","For the code snippet:

import pandas as pd

class TableManager:
    def __init__(self, data):
        self.data = pd.DataFrame(data)

    def normalize_col(self, col):
        """"""Normalize the values in col to the range [0, 1].""""""
        <blank>

The model first self-evaluates whether retrieval is needed to complete the normalize_col function body. If not, it directly generates the completion using only in-file context. If retrieval is triggered, it retrieves relevant cross-file contexts like other data normalization functions and leverages them to generate the completion.","Recent advances in retrieval-augmented generation (RAG) have initiated a new
era in repository-level code completion. However, the invariable use of
retrieval in existing methods exposes issues in both efficiency and robustness,
with a large proportion of the retrieved contexts proving unhelpful or harmful
to code language models (code LMs). In this paper, we propose a selective RAG
framework to avoid retrieval when unnecessary. To power this framework, we
design a self-supervised learning approach to enable a code LM to accurately
self-evaluate whether retrieval can improve its output quality and robustly
leverage the potentially noisy retrieved contexts. Using this LM as both the
selective RAG policy and the generation model, our framework achieves
state-of-the-art repository-level code completion performance on diverse
benchmarks including RepoEval, CrossCodeEval, and CrossCodeLongEval, a new
long-form code completion benchmark. Meanwhile, our analyses show that
selectively retrieving brings as much as 70% inference speedup in the online
serving setting without harming the performance. We further demonstrate that
our framework is able to accommodate different generation models, retrievers,
and programming languages. These advancements position our framework as an
important step towards more accurate and efficient repository-level code
completion.",http://arxiv.org/abs/2403.10059v2
RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completion,"repository-level code completion, semantic graph representation, context retrieval, link prediction, graph neural networks",2024-03,"The paper introduces RepoHYPER, a novel framework for repository-level code completion that leverages a semantic graph representation and graph-based retrieval methods to effectively identify and prioritize relevant code contexts from the entire codebase. It demonstrates significant improvements over existing techniques.","1. Introduction 2. Related Work 3. Methodology: Repo-level Semantic Graph, Search-Expand-Refine Retrieval 4. Empirical Evaluation 5. Conclusion","['- Repo-level Semantic Graph (RSG): Graph representation capturing global repository context (functions, classes, scripts, imports, invocations, etc.)', '- Search-then-Expand Strategies: kNN search to find anchor nodes, then expand to related nodes via BFS or pattern-based exploration', '- Re-ranking as Link Prediction: Treat context re-ranking as link prediction on RSG using graph neural networks to score relevance']","For a code snippet needing to call get_similarity_metric(), similarity search focuses on MultiLabelInversionModel due to form similarity, leading to incorrect completion. RepoHYPER first identifies the most similar inversion_bert.py via kNN search, then expands to inversion_albert.py containing get_similarity_metric via Import relation, enabling correct completion.","Code Large Language Models (CodeLLMs) have demonstrated impressive
proficiency in code completion tasks. However, they often fall short of fully
understanding the extensive context of a project repository, such as the
intricacies of relevant files and class hierarchies, which can result in less
precise completions. To overcome these limitations, we present \tool, a
multifaceted framework designed to address the complex challenges associated
with repository-level code completion. Central to RepoHYPER is the {\em
Repo-level Semantic Graph} (RSG), a novel semantic graph structure that
encapsulates the vast context of code repositories. Furthermore, RepoHyper
leverages Expand and Refine retrieval method, including a graph expansion and a
link prediction algorithm applied to the RSG, enabling the effective retrieval
and prioritization of relevant code snippets. Our evaluations show that \tool
markedly outperforms existing techniques in repository-level code completion,
showcasing enhanced accuracy across various datasets when compared to several
strong baselines. Our implementation of RepoHYPER can be found at
https://github.com/FSoft-AI4Code/RepoHyper.",http://arxiv.org/abs/2403.06095v4
Improving Cross-lingual Representation for Semantic Retrieval with Code-switching,"code-switching, cross-lingual semantic retrieval, alternative pre-training, FAQ systems, e-commerce",2024-03,"This paper proposes a novel alternative cross-lingual pre-training approach using code-switching to improve semantic retrieval performance for FAQ systems in e-commerce scenarios. By incorporating code-switched data and a similarity loss during pre-training, the model learns better cross-lingual representations tailored for the semantic retrieval task.","1. Introduction - Motivation and background 2. Preliminaries - Masked language modeling, cross-lingual pretraining, semantic retrieval, code-switching 3. Method - Alternative cross-lingual PTM architecture, building code-switched data 4. Experiments - Setup, results on business corpora, open datasets, analysis 5. Conclusion","['Generate code-switched data by replacing tokens with other languages using bilingual dictionaries', 'Pre-train model on code-switched data using combined loss: cross-lingual masked language modeling (XMLM) + similarity loss between query and label representations', 'Fine-tune pre-trained model on semantic retrieval corpus']","For the query 'My delivery status says failed', the model may code-switch some words to other languages like 'My 状态 says 失败 delivery 状态'. During pre-training, it learns to predict the masked words while also bringing the query and label representations closer based on the similarity loss.","Semantic Retrieval (SR) has become an indispensable part of the FAQ system in
the task-oriented question-answering (QA) dialogue scenario. The demands for a
cross-lingual smart-customer-service system for an e-commerce platform or some
particular business conditions have been increasing recently. Most previous
studies exploit cross-lingual pre-trained models (PTMs) for multi-lingual
knowledge retrieval directly, while some others also leverage the continual
pre-training before fine-tuning PTMs on the downstream tasks. However, no
matter which schema is used, the previous work ignores to inform PTMs of some
features of the downstream task, i.e. train their PTMs without providing any
signals related to SR. To this end, in this work, we propose an Alternative
Cross-lingual PTM for SR via code-switching. We are the first to utilize the
code-switching approach for cross-lingual SR. Besides, we introduce the novel
code-switched continual pre-training instead of directly using the PTMs on the
SR tasks. The experimental results show that our proposed approach consistently
outperforms the previous SOTA methods on SR and semantic textual similarity
(STS) tasks with three business corpora and four open datasets in 20+
languages.",http://arxiv.org/abs/2403.01364v1
Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning,"symbolic loss functions, genetic programming, unrolled differentiation, neuro-symbolic optimization, task-agnostic meta-learning",2024-03,"The paper proposes a hybrid neuro-symbolic approach called EvoMAL for learning task and model-agnostic symbolic loss functions via genetic programming and gradient-based local search. The learned loss functions improve convergence, sample efficiency, and inference performance across various supervised learning tasks.",1. Introduction 2. Background and Related Work 2.1 Loss Function Learning 2.2 Gradient-Based Approaches 2.3 Evolution-Based Approaches 3. Proposed Method 3.1 Learning Symbolic Loss Functions 3.1.1 Search Space Design 3.1.2 Outer Search Algorithm Design 3.1.3 Constraint Enforcement 3.1.4 Loss Archival Strategy 3.2 Loss Function Optimization and Evaluation 3.3 Computational Complexity Analysis,"['- Use genetic programming (GP) to search for symbolic loss functions in the outer optimization loop', '- Design a task and model-agnostic search space of mathematical operations for GP', '- Convert symbolic loss functions to trainable neural networks (meta-loss networks)', '- Optimize meta-loss networks using unrolled differentiation in the inner optimization loop', '- Employ techniques like constraint handling, loss archiving to improve efficiency']","As an example, consider the symbolic loss function M = (y - f(x))^2 + ln(|y - f(x)| + ε) learned by EvoMAL for a regression task. This loss combines the mean squared error and a log component to improve training convergence. EvoMAL first finds this symbolic form via GP, then converts it to a trainable meta-loss network to further optimize its parameters via unrolled differentiation.","In this paper, we develop upon the topic of loss function learning, an
emergent meta-learning paradigm that aims to learn loss functions that
significantly improve the performance of the models trained under them.
Specifically, we propose a new meta-learning framework for task and
model-agnostic loss function learning via a hybrid search approach. The
framework first uses genetic programming to find a set of symbolic loss
functions. Second, the set of learned loss functions is subsequently
parameterized and optimized via unrolled differentiation. The versatility and
performance of the proposed framework are empirically validated on a diverse
set of supervised learning tasks. Results show that the learned loss functions
bring improved convergence, sample efficiency, and inference performance on
tabulated, computer vision, and natural language processing problems, using a
variety of task-specific neural network architectures.",http://arxiv.org/abs/2403.00865v1
EVOR: Evolving Retrieval for Code Generation,"synchronous query evolution, diverse knowledge base evolution, retrieval-augmented code generation, execution feedback integration, multi-source knowledge fusion",2024-02,"This paper proposes EVOR, a novel pipeline for retrieval-augmented code generation that employs synchronous evolution of both queries and diverse knowledge bases. It demonstrates significant performance improvements over existing methods on a new benchmark focused on frequently updated libraries and long-tail programming languages.",1. Introduction 2. Evolving Retrieval 2.1 Query evolution 2.2 Knowledge Soup 2.2.1 Construction 2.2.2 Evolution 2.3 EVOR Pipeline 2.4 Datasets 3. Experiment 3.1 Baselines 3.2 Default EVOR Configuration 3.3 Results 4. Analysis 4.1 Ablation Study 4.2 Knowledge Base Analysis 4.3 Combining with Other Methods 4.4 Token Efficiency,"['- Synchronously evolve both queries and knowledge bases in an iterative process', '- Construct diverse knowledge soup from web search, documentation, execution feedback, and code snippets', '- Evolve knowledge base by adding successful code snippets and code-error pairs from execution', '- Use language models to rephrase queries based on current program and execution feedback', '- Retrieve relevant knowledge from evolved knowledge base using rephrased queries', '- Generate new program using retrieved knowledge and original query']","For the problem 'Write ponyc code to print odd numbers from 0 to n (inclusive)', EVOR starts with the original query and an initial knowledge base of Pony documentation. After generating an initial buggy program and getting execution feedback, it rephrases the query to 'The Pony if condition allows actions when a condition is true' to retrieve more relevant information on if conditions. The knowledge base is also expanded with the buggy program and its error message. This iterative process continues, retrieving web search results on Pony if conditions, code snippets demonstrating correct usage, etc., until EVOR can generate a working solution.","Recently the retrieval-augmented generation (RAG) has been successfully
applied in code generation. However, existing pipelines for retrieval-augmented
code generation (RACG) employ static knowledge bases with a single source,
limiting the adaptation capabilities of Large Language Models (LLMs) to domains
they have insufficient knowledge of. In this work, we develop a novel pipeline,
EVOR, that employs the synchronous evolution of both queries and diverse
knowledge bases. On two realistic settings where the external knowledge is
required to solve code generation tasks, we compile four new datasets
associated with frequently updated libraries and long-tail programming
languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR
achieves two to four times of execution accuracy compared to other methods such
as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We
demonstrate that EVOR is flexible and can be easily combined with them to
achieve further improvement. Further analysis reveals that EVOR benefits from
the synchronous evolution of queries and documents and the diverse information
sources in the knowledge base. We hope that our studies will inspire more
insights into the design of advanced RACG pipelines in future research. Our
model, code, and data are available at https://arks-codegen.github.io.",http://arxiv.org/abs/2402.12317v2
"VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search","Verified Program Synthesis, Monte Carlo Tree Search, Large Language Models, Formal Verification, Dafny, Coq",2024-02,"The paper introduces VerMCTS, an approach that combines a large language model, a formal program verifier, and Monte Carlo Tree Search to synthesize multi-step verified programs in languages like Dafny and Coq. It outperforms baselines on a new suite of verified programming problems.",1. Introduction 2. Method: VerMCTS 2.1 MDP for verified program synthesis 2.2 VerMCTS algorithm 2.3 Connecting partial program score to MDP 3. A problem suite for multi-step verified programming 3.1 Defining the problems 3.2 Criteria for success,"['Formulates verified program synthesis as a Markov Decision Process (MDP)', 'Uses a large language model to generate program candidates', 'Leverages a formal verifier to evaluate partial programs and provide an upper bound on the value function', 'Employs a modified Monte Carlo Tree Search (VerMCTS) that combines LLM generation and verifier evaluation', 'Progressively widens the search tree to handle large action spaces']","For the 'Opt0' problem that asks to define arithmetic expressions, an optimizer, and prove the optimizer preserves semantics, VerMCTS could: 1) Use the LLM to generate candidate data types and function definitions 2) Use the verifier to check validity of partial programs 3) Expand promising candidates further guided by verifier feedback 4) Eventually find a complete, verified solution","Large Language Models (LLMs) can generate useful code, but often the code
they generate cannot be trusted to be sound. In this paper, we present VerMCTS,
an approach to begin to resolve this issue by generating verified programs in
Dafny and Coq. VerMCTS uses a logical verifier in concert with an LLM to guide
a modified Monte Carlo Tree Search (MCTS). This approach leverages the verifier
to gain intermediate feedback inside the search algorithm by checking partial
programs at each step to estimate an upper bound on the value function. To
measure the performance of VerMCTS, we develop a new suite of multi-step
verified programming problems in Dafny and Coq. In terms of pass@T, a new
metric which computes the pass rate given a budget of T tokens sampled from the
LLM, VerMCTS leads to more than a 30% absolute increase in average pass@5000
across the suite over repeated sampling from the base language model. Our code
and benchmarks are available at
https://github.com/namin/llm-verified-with-monte-carlo-tree-search .",http://arxiv.org/abs/2402.08147v2
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search,"code style normalization, code rewriting, generation-augmented retrieval, code style similarity metric, large language models for code",2024-01,The paper proposes a simple yet effective method called ReCo that rewrites code in a codebase to normalize its style to match the style of code generated by large language models. This improves the performance of generation-augmented retrieval for code search. The paper also introduces a new Code Style Similarity metric to quantify stylistic differences between codes.,1. Introduction - Motivates code style normalization to improve generation-augmented retrieval 2. Related Work - Discusses code search models and large language models 3. Methodology - Describes the ReCo method and theoretical insights 4. Code Style Similarity - Proposes a new metric to measure code stylistic differences 5. Experiments - Evaluates ReCo across various code search scenarios 6. Conclusion,"['Use large language models to generate exemplar code snippets based on the query', 'Summarize original code to natural language, then use language model to rewrite code matching the summary', 'Augment query with generated exemplar codes, augment codebase with rewritten codes', 'Use augmented query/codes in sparse retrieval or dense retrieval with InfoNCE loss']","For the query 'Write a function to get the frequency of elements in a list', the language model generates: Exemplar Code: def count_frequency(my_list): frequency = {} ... Rewritten Code: def frequency(my_list): freq = {} ...","In code search, the Generation-Augmented Retrieval (GAR) framework, which
generates exemplar code snippets to augment queries, has emerged as a promising
strategy to address the principal challenge of modality misalignment between
code snippets and natural language queries, particularly with the demonstrated
code generation capabilities of Large Language Models (LLMs). Nevertheless, our
preliminary investigations indicate that the improvements conferred by such an
LLM-augmented framework are somewhat constrained. This limitation could
potentially be ascribed to the fact that the generated codes, albeit
functionally accurate, frequently display a pronounced stylistic deviation from
the ground truth code in the codebase. In this paper, we extend the
foundational GAR framework and propose a simple yet effective method that
additionally Rewrites the Code (ReCo) within the codebase for style
normalization. Experimental results demonstrate that ReCo significantly boosts
retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%),
and fine-tuned dense (up to 23.6%) retrieval settings in diverse search
scenarios. To further elucidate the advantages of ReCo and stimulate research
in code style normalization, we introduce Code Style Similarity, the first
metric tailored to quantify stylistic similarities in code. Notably, our
empirical findings reveal the inadequacy of existing metrics in capturing
stylistic nuances. The source code and data are available at
\url{https://github.com/Alex-HaochenLi/ReCo}.",http://arxiv.org/abs/2401.04514v2
Semantic Parsing for Complex Data Retrieval: Targeting Query Plans vs. SQL for No-Code Access to Relational Databases,"query plan language, compositional query generation, question decomposition, chain of thought prompting, complex data retrieval",2023-12,"The paper proposes a new Query Plan Language (QPL) as an alternative to SQL for complex data retrieval from relational databases. QPL is designed to be more modular and compositional, making it easier for large language models to generate accurate query plans from natural language questions. The key idea is to decompose complex questions into simpler sub-questions that can be mapped to executable QPL components and then combined into a full query plan.",1. Introduction 2. Previous Work 2.1. Architectures for text-to-SQL 2.2. Encoding-side: Schema Linking and Encoding 2.3. Decoding-side: Syntax-guided Methods 2.4. Zero-shot and Few-shot LLM Methods 2.5. Simplified Target Language and Compositional Methods 3. Method: Question Decomposition and Query Planning 3.1. Query Plan Language 3.2. Question Decomposition Strategies 3.3. Query Planning 4. Experiments 5. Results 6. Conclusion,"['Design a new Query Plan Language (QPL) that is modular and can represent complex SQL queries as executable components', 'Convert existing text-to-SQL datasets like Spider to QPL format', 'Analyze the complexity of QPL queries to identify challenging compositional cases', 'Explore different question decomposition strategies using fine-tuned models and chain-of-thought prompting with large language models', 'Combine decomposed sub-questions into full query plans using a planning mechanism', 'Fine-tune language models on the QPL dataset to generate query plans from natural language']","For the question 'What is the total ticket expense of the visitors whose membership level is 1?', the paper shows how it can be decomposed into: 1. Scan the 'visitor' table and filter for 'Level_of_membership = 1', outputting visitor IDs 2. Scan the 'visit' table and output visitor IDs and total spent 3. Join the two tables on visitor ID 4. Aggregate by summing the 'Total_spent' column This decomposed plan in QPL can then be easily translated to the equivalent SQL query.","Large Language Models (LLMs) have spurred progress in text-to-SQL, the task
of generating SQL queries from natural language questions based on a given
database schema. Despite the declarative nature of SQL, it continues to be a
complex programming language. In this paper, we investigate the potential of an
alternative query language with simpler syntax and modular specification of
complex queries. The purpose is to create a query language that can be learned
more easily by modern neural semantic parsing architectures while also enabling
non-programmers to better assess the validity of the query plans produced by an
interactive query plan assistant.
  The proposed alternative query language is called Query Plan Language (QPL).
It is designed to be modular and can be translated into a restricted form of
SQL Common Table Expressions (CTEs). The aim of QPL is to make complex data
retrieval accessible to non-programmers by allowing users to express their
questions in natural language while also providing an easier-to-verify target
language. The paper demonstrates how neural LLMs can benefit from QPL's
modularity to generate complex query plans in a compositional manner. This
involves a question decomposition strategy and a planning stage.
  We conduct experiments on a version of the Spider text-to-SQL dataset that
has been converted to QPL. The hierarchical structure of QPL programs enables
us to measure query complexity naturally. Based on this assessment, we identify
the low accuracy of existing text-to-SQL systems on complex compositional
queries. We present ways to address the challenge of complex queries in an
iterative, user-controlled manner, using fine-tuned LLMs and a variety of
prompting strategies in a compositional manner.",http://arxiv.org/abs/2312.14798v1
STraceBERT: Source Code Retrieval using Semantic Application Traces,"application tracing, source code retrieval, BERT embeddings, Java dynamic analysis, software reverse engineering",2023-12,"The paper proposes STraceBERT, a novel approach that utilizes Java dynamic analysis to record application traces and pretrain a BERT model on these traces for effective source code retrieval from a candidate set. The experiments demonstrate the effectiveness of STraceBERT in retrieving source code compared to existing approaches.",1. Introduction 2. Background and Related Work 3. Approach - Dataset creation (Java Trace Dataset) - Model pretraining - Source code retrieval 4. Evaluation and Results 5. Conclusion,"['Record Java application traces using a dynamic analysis tool (Jackal)', ""Construct Java Trace Dataset (JTD) from open-source projects' test suite traces"", 'Pretrain a BERT-style model on the JTD using Masked Language Modeling (MLM)', 'Embed all trace sequences using the pretrained model', 'Retrieve source code candidates by finding nearest neighbors in the embedding space']","For a given trace sequence representing the execution of a test case, STraceBERT retrieves the top 5 most similar source code snippets from a candidate set. For example, given the trace '-> java.lang.String.trim(): java.lang.String', the model retrieves candidate source code snippets with an average maximum CodeBLEU score of 86.26 for the top candidate and 93.48 for the top 5 candidates.","Software reverse engineering is an essential task in software engineering and
security, but it can be a challenging process, especially for adversarial
artifacts. To address this challenge, we present STraceBERT, a novel approach
that utilizes a Java dynamic analysis tool to record calls to core Java
libraries, and pretrain a BERT-style model on the recorded application traces
for effective method source code retrieval from a candidate set. Our
experiments demonstrate the effectiveness of STraceBERT in retrieving the
source code compared to existing approaches. Our proposed approach offers a
promising solution to the problem of code retrieval in software reverse
engineering and opens up new avenues for further research in this area.",http://arxiv.org/abs/2312.04731v1
Code Search Debiasing:Improve Search Results beyond Overall Ranking Performance,"code search bias, debiasing framework, reranking, query-code relevance, abstract syntax tree",2023-11,"This paper identifies and analyzes seven types of biases in code search models, where the models exhibit varying performance based on characteristics like code length, query length, and abstract syntax tree properties. It proposes a general debiasing framework using reranking to mitigate these biases and improve the overall ranking performance.",1. Introduction 2. Related Work 3. Analysis of Code Search Biases 4. Mitigate Code Search Biases 5. Experiments 6. Conclusion,"['- Analyze 7 types of code search biases related to code/query lengths, abstract syntax tree properties, reserved words, word importance, and word overlap', '- Propose a debiasing framework using reranking to calibrate search results', '- Train a reranking model on debiased data to learn bias-invariant query-code relevance', '- Integrate the reranking model with the base code search model']","For the bias related to code length, the paper shows that longer code snippets tend to get better rankings from models like CodeBERT and GraphCodeBERT. The reranking model is trained on debiased data where this length bias is mitigated, allowing it to produce more fair rankings not skewed by code length.","Code search engine is an essential tool in software development. Many code
search methods have sprung up, focusing on the overall ranking performance of
code search. In this paper, we study code search from another perspective by
analyzing the bias of code search models. Biased code search engines provide
poor user experience, even though they show promising overall performance. Due
to different development conventions (e.g., prefer long queries or
abbreviations), some programmers will find the engine useful, while others may
find it hard to get desirable search results. To mitigate biases, we develop a
general debiasing framework that employs reranking to calibrate search results.
It can be easily plugged into existing engines and handle new code search
biases discovered in the future. Experiments show that our framework can
effectively reduce biases. Meanwhile, the overall ranking performance of code
search gets improved after debiasing.",http://arxiv.org/abs/2311.14901v2
A Survey of Source Code Search: A 3-Dimensional Perspective,"code search, query understanding, code understanding, query-code matching, natural language processing, information retrieval",2023-11,"This paper provides a comprehensive survey of techniques for source code search, categorizing them into three dimensions: query-end optimization, code-end optimization, and match-end optimization. It systematically reviews 68 relevant papers and analyzes the evolution of techniques in each dimension over time.",1. Introduction 2. Background 3. Survey Methodology 4. Query-End Optimization Techniques 5. Code-End Optimization Techniques 6. Match-End Optimization Techniques 7. Challenges and Opportunities 8. Threats to Validity 9. Conclusion,"['Categorized existing code search techniques into query-end, code-end, and match-end optimization', 'Systematically reviewed 68 representative papers proposing optimizations for the three components', 'Analyzed the evolution of techniques in each category over time', 'Outlined persistent challenges and potential research opportunities based on the review']","For the query 'calculate the factorial of a number', a good code search technique should: 1) Understand the query intent of calculating factorials (query-end) 2) Capture the semantics of code snippets implementing factorial calculation (code-end) 3) Match the query representation with the most relevant code representation (match-end). The paper provides examples of techniques optimizing each of these components.","(Source) code search is widely concerned by software engineering researchers
because it can improve the productivity and quality of software development.
Given a functionality requirement usually described in a natural language
sentence, a code search system can retrieve code snippets that satisfy the
requirement from a large-scale code corpus, e.g., GitHub. To realize effective
and efficient code search, many techniques have been proposed successively.
These techniques improve code search performance mainly by optimizing three
core components, including query understanding component, code understanding
component, and query-code matching component. In this paper, we provide a
3-dimensional perspective survey for code search. Specifically, we categorize
existing code search studies into query-end optimization techniques, code-end
optimization techniques, and match-end optimization techniques according to the
specific components they optimize. Considering that each end can be optimized
independently and contributes to the code search performance, we treat each end
as a dimension. Therefore, this survey is 3-dimensional in nature, and it
provides a comprehensive summary of each dimension in detail. To understand the
research trends of the three dimensions in existing code search studies, we
systematically review 68 relevant literatures. Different from existing code
search surveys that only focus on the query end or code end or introduce
various aspects shallowly (including codebase, evaluation metrics, modeling
technique, etc.), our survey provides a more nuanced analysis and review of the
evolution and development of the underlying techniques used in the three ends.
Based on a systematic review and summary of existing work, we outline several
open challenges and opportunities at the three ends that remain to be addressed
in future work.",http://arxiv.org/abs/2311.07107v1
Retrieval-Augmented Code Generation for Universal Information Extraction,"schema-based representation, code generation, in-context learning, example retrieval, unified information extraction",2023-11,"The paper proposes Code4UIE, a framework that transforms various information extraction (IE) tasks into a unified code generation task using large language models (LLMs). It represents IE schemas as Python classes and uses retrieved examples to instruct the LLM for code generation.",1. Introduction - Motivation and challenges of IE tasks 2. Related Work - Prior work on universal IE and LLM-based IE 3. Code4UIE Framework 3.1 Schema Representation 3.2 Prompt Construction 3.3 1-stage Prompt 3.4 2-stage Prompt 3.5 Example Retrieval Strategies 4. Experiments 5. Conclusion,"['Define IE task schemas using inheritable Python classes for entities, relations, and events', 'Transform IE tasks into code generation by instantiating predefined classes', 'Use 1-stage or 2-stage prompts with schema code and examples to instruct LLM', 'Retrieve relevant examples using sentence embedding similarity']","For NER, define Person class inheriting from Entity. Prompt: 'List Person entities in ""Witnesses put the figure at about 30,000.""' LLM generates: person_entity1 = Person(name=""Witnesses"")","Information Extraction (IE) aims to extract structural knowledge (e.g.,
entities, relations, events) from natural language texts, which brings
challenges to existing methods due to task-specific schemas and complex text
expressions. Code, as a typical kind of formalized language, is capable of
describing structural knowledge under various schemas in a universal way. On
the other hand, Large Language Models (LLMs) trained on both codes and texts
have demonstrated powerful capabilities of transforming texts into codes, which
provides a feasible solution to IE tasks. Therefore, in this paper, we propose
a universal retrieval-augmented code generation framework based on LLMs, called
Code4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define
task-specific schemas of various structural knowledge in a universal way. By so
doing, extracting knowledge under these schemas can be transformed into
generating codes that instantiate the predefined Python classes with the
information in texts. To generate these codes more precisely, Code4UIE adopts
the in-context learning mechanism to instruct LLMs with examples. In order to
obtain appropriate examples for different tasks, Code4UIE explores several
example retrieval strategies, which can retrieve examples semantically similar
to the given texts. Extensive experiments on five representative IE tasks
across nine datasets demonstrate the effectiveness of the Code4UIE framework.",http://arxiv.org/abs/2311.02962v1
Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models,"Conditional Kolmogorov Complexity, Solomonoff Induction, Generative Code Models, Utility Function, Penalty Function",2023-10,"The paper proposes an advanced search and optimization framework that leverages generative code models and an adapted version of Solomonoff Induction to mitigate biases and errors in software-generated data. It introduces a utility function that combines algorithmic probability with a penalty term, enabling more practical and nuanced decision-making.",1. Introduction 2. Mathematical Symbols 3. Domain Specification 4. Generative Approaches for Program Variants 5. Incorporating Conditional Kolmogorov Complexity into Solomonoff Induction 6. Advanced Utility Function 7. Integration of Likelihood and Utility Functions 8. Two-Phase Approach for Optimal Program Discovery 9. Computational Complexity,"['- Defines a finite set of program variants P generated by generative code models like LLMs', ""- Introduces Conditional Kolmogorov Complexity K(p|p') to measure complexity of program p relative to reference p'"", '- Adapts Solomonoff Induction to incorporate Conditional Kolmogorov Complexity and a likelihood function L(p|D)', '- Proposes a utility function U(p) = λP(p|Θ,D) + (1-λ)R(p) that combines algorithmic probability and a penalty term R(p)', '- Outlines a two-phase approach: preparation phase to generate P, and optimization phase to find optimal p* using U(p)']","Consider a software system that generates financial data for risk analysis. The observed data D may contain biases or errors due to flaws in the original program. The framework would: 1. Use an LLM to generate a set P of program variants 2. Evaluate each p in P using the utility function U(p) that considers code complexity via K(p|p'), fit to data via L(p|D), and a penalty R(p) for undesirable properties 3. Select the optimal variant p* that maximizes U(p), providing a refined program that mitigates biases/errors in the financial data","Data generation and analysis is a fundamental aspect of many industries and
disciplines, from strategic decision making in business to research in the
physical and social sciences. However, data generated using software and
algorithms can be subject to biases and errors. These can be due to problems
with the original software, default settings that do not align with the
specific needs of the situation, or even deeper problems with the underlying
theories and models. This paper proposes an advanced search and optimization
framework aimed at generating and choosing optimal source code capable of
correcting errors and biases from previous versions to address typical problems
in software systems specializing in data analysis and generation, especially
those in the corporate and data science world. Applying this framework multiple
times on the same software system would incrementally improve the quality of
the output results. It uses Solomonoff Induction as a sound theoretical basis,
extending it with Kolmogorov Conditional Complexity, a novel adaptation, to
evaluate a set of candidate programs. We propose the use of generative models
for the creation of this set of programs, with special emphasis on the
capabilities of Large Language Models (LLMs) to generate high quality code.",http://arxiv.org/abs/2310.11546v1
Rethinking Negative Pairs in Code Search,"Soft-InfoNCE loss, code search, negative sample weighting, false negative cancellation, contrastive learning",2023-10,"This paper proposes the Soft-InfoNCE loss, a novel contrastive loss function that models the potential relevance of negative code samples by inserting a weight term into the vanilla InfoNCE loss. It provides a more precise mutual information estimation and better control over the distribution of learned code representations.",1. Introduction 2. Preliminaries 3. Proposed Method 4. Theoretical Justification 5. Experimental Setup 6. Results and Analysis 7. Related Work 8. Conclusion,"['Proposes Soft-InfoNCE loss by inserting a weight term wij into the InfoNCE loss denominator', 'Introduces three methods to estimate similarity scores simij for weighting: BM25, SimCSE, trained model predictions', 'Theoretically analyzes effects on representation distribution and mutual information estimation', 'Proves Soft-InfoNCE upper bounds other losses like Binary Cross Entropy and weighted InfoNCE', 'Relates to prior work on false negative cancellation as a special case']","For a query on 'bubble sorting algorithm', the Soft-InfoNCE loss would assign a lower weight to a negative example of a 'file saving function' than to a 'quick sorting algorithm', since the latter is more relevant to the query.","Recently, contrastive learning has become a key component in fine-tuning code
search models for software development efficiency and effectiveness. It pulls
together positive code snippets while pushing negative samples away given
search queries. Among contrastive learning, InfoNCE is the most widely used
loss function due to its better performance. However, the following problems in
negative samples of InfoNCE may deteriorate its representation learning: 1) The
existence of false negative samples in large code corpora due to duplications.
2). The failure to explicitly differentiate between the potential relevance of
negative samples. As an example, a bubble sorting algorithm example is less
``negative'' than a file saving function for the quick sorting algorithm query.
In this paper, we tackle the above problems by proposing a simple yet effective
Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss
function, we apply three methods to estimate the weights of negative pairs and
show that the vanilla InfoNCE loss is a special case of Soft-InfoNCE.
Theoretically, we analyze the effects of Soft-InfoNCE on controlling the
distribution of learnt code representations and on deducing a more precise
mutual information estimation. We furthermore discuss the superiority of
proposed loss functions with other design alternatives. Extensive experiments
demonstrate the effectiveness of Soft-InfoNCE and weights estimation methods
under state-of-the-art code search models on a large-scale public dataset
consisting of six programming languages. Source code is available at
\url{https://github.com/Alex-HaochenLi/Soft-InfoNCE}.",http://arxiv.org/abs/2310.08069v1
Contrastive Prompt Learning-based Code Search based on Interaction Matrix,"contrastive prompt learning, cross-modal interaction, code search, dual-encoder architecture, reparameterization encoder",2023-10,"This paper proposes CPLCS, a contrastive prompt learning-based code search method that uses a cross-modal interaction mechanism to improve the semantic representation and mapping between natural language and programming language. It introduces prompt tuning to the dual-encoder architecture to address inadequate representations.","1. Introduction 2. Related Work 3. Methodology (3.1 Overview, 3.2 Prompt Tuning, 3.3 Contrastive Learning) 4. Experiments 5. Conclusion","['- Eliminate weight sharing between PL and NL encoders in dual-encoder', '- Use prompt template to generate continuous prompts and task-specific vectors for each encoder', '- Employ reparameterization encoder to transform continuous prompts', '- Construct interaction matrix from token similarities between PL and NL', '- Perform max pooling on rows/columns of matrix to obtain match scores', '- Calculate contrastive loss based on match scores to optimize prompts']","For a code snippet and natural language query, the model first generates prompts and task vectors. It then computes token embeddings and constructs an interaction matrix capturing token similarities across modalities. Match scores are derived from this matrix and used to calculate the contrastive loss for optimizing the prompts.","Code search aims to retrieve the code snippet that highly matches the given
query described in natural language. Recently, many code pre-training
approaches have demonstrated impressive performance on code search. However,
existing code search methods still suffer from two performance constraints:
inadequate semantic representation and the semantic gap between natural
language (NL) and programming language (PL). In this paper, we propose CPLCS, a
contrastive prompt learning-based code search method based on the cross-modal
interaction mechanism. CPLCS comprises:(1) PL-NL contrastive learning, which
learns the semantic matching relationship between PL and NL representations;
(2) a prompt learning design for a dual-encoder structure that can alleviate
the problem of inadequate semantic representation; (3) a cross-modal
interaction mechanism to enhance the fine-grained mapping between NL and PL. We
conduct extensive experiments to evaluate the effectiveness of our approach on
a real-world dataset across six programming languages. The experiment results
demonstrate the efficacy of our approach in improving semantic representation
quality and mapping ability between PL and NL.",http://arxiv.org/abs/2310.06342v1
Generalizable Error Modeling for Human Data Annotation: Evidence From an Industry-Scale Search Data Annotation Program,"behavioral error modeling, data annotation auditing, search relevance annotation, generalized error detection, annotator performance features",2023-10,"This paper presents a generalizable error detection model that can predict annotation errors in search relevance tasks across different domains (music, video, apps) by utilizing both task features and behavioral features derived from the annotation process itself. The model achieves moderate performance (AUC 0.65-0.75) and enables more efficient auditing by prioritizing likely errors.",1. Introduction 2. Method 2.1 Data Collection and Sampling 2.2 Search Relevance Annotation Tasks 2.3 Operationalizations and Data Preprocessing 2.4 Modeling 3. Results 3.1 Overall Model Performance 3.2 Feature Importance 3.3 Generalization Across Tasks 3.4 Applications for Auditing,"['- Used XGBoost binary classification to predict if an annotation contained an error or not', '- Trained on combination of task features (e.g. query, output details) and behavioral features (e.g. annotator past performance, time on task)', '- Performed hyperparameter tuning using randomized search on validation set', '- Evaluated using AUC metric on held-out test set', '- Analyzed feature importance using Shapley values']","For the music streaming application, if a query for 'Master of Puppets' returned the eponymous Metallica song, it would be labeled as 'Perfect'. However, if it returned a Taylor Swift song, that would be an obvious annotation error labeled as 'Unacceptable'. The error model can detect such mismatches by considering the input query, output details, annotator's past performance on similar queries, time spent on the task, and other behavioral signals.","Machine learning (ML) and artificial intelligence (AI) systems rely heavily
on human-annotated data for training and evaluation. A major challenge in this
context is the occurrence of annotation errors, as their effects can degrade
model performance. This paper presents a predictive error model trained to
detect potential errors in search relevance annotation tasks for three
industry-scale ML applications (music streaming, video streaming, and mobile
apps). Drawing on real-world data from an extensive search relevance annotation
program, we demonstrate that errors can be predicted with moderate model
performance (AUC=0.65-0.75) and that model performance generalizes well across
applications (i.e., a global, task-agnostic model performs on par with
task-specific models). In contrast to past research, which has often focused on
predicting annotation labels from task-specific features, our model is trained
to predict errors directly from a combination of task features and behavioral
features derived from the annotation process, in order to achieve a high degree
of generalizability. We demonstrate the usefulness of the model in the context
of auditing, where prioritizing tasks with high predicted error probabilities
considerably increases the amount of corrected annotation errors (e.g., 40%
efficiency gains for the music streaming application). These results highlight
that behavioral error detection models can yield considerable improvements in
the efficiency and quality of data annotation processes. Our findings reveal
critical insights into effective error management in the data annotation
process, thereby contributing to the broader field of human-in-the-loop ML.",http://arxiv.org/abs/2310.05286v2
FASER: Binary Code Similarity Search through the use of Intermediate Representations,"Intermediate Representation, Binary Code Similarity, Cross-Architecture Search, Transformers, ESIL",2023-10,"The paper proposes FASER, a method that uses the Evaluable String Intermediate Language (ESIL) representation of binary code combined with a Longformer transformer model to enable cross-architecture binary code similarity search. It demonstrates strong performance on function search and vulnerability detection tasks across multiple architectures without pre-training.","1. Introduction 2. Methodology (dataset, data processing, model, training) 3. Experimental Results 4. Conclusion","['Use radare2 to lift binary functions to ESIL intermediate representation', 'Normalize and deduplicate ESIL strings to create dataset', 'Use Longformer transformer model with input dimension of 4096', 'Train with Siamese formulation, Circle Loss, and online hard negative mining', 'Evaluate on cross-architecture function search and vulnerability detection tasks']","For the vulnerability search task, the method takes a vulnerable function from a compiled OpenSSL libcrypto library (e.g. ARM32) as the query, and aims to identify the corresponding vulnerable function within firmware images (e.g. MIPS32 router firmware).","Being able to identify functions of interest in cross-architecture software
is useful whether you are analysing for malware, securing the software supply
chain or conducting vulnerability research. Cross-Architecture Binary Code
Similarity Search has been explored in numerous studies and has used a wide
range of different data sources to achieve its goals. The data sources
typically used draw on common structures derived from binaries such as function
control flow graphs or binary level call graphs, the output of the disassembly
process or the outputs of a dynamic analysis approach. One data source which
has received less attention is binary intermediate representations. Binary
Intermediate representations possess two interesting properties: they are cross
architecture by their very nature and encode the semantics of a function
explicitly to support downstream usage. Within this paper we propose Function
as a String Encoded Representation (FASER) which combines long document
transformers with the use of intermediate representations to create a model
capable of cross architecture function search without the need for manual
feature engineering, pre-training or a dynamic analysis step. We compare our
approach against a series of baseline approaches for two tasks; A general
function search task and a targeted vulnerability search task. Our approach
demonstrates strong performance across both tasks, performing better than all
baseline approaches.",http://arxiv.org/abs/2310.03605v3
RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair,"retrieval-augmented generation, automatic program repair, CodeT5, hybrid patch retrieval, contrastive learning",2023-09,"This paper proposes RAP-Gen, a novel retrieval-augmented patch generation framework that leverages relevant fix patterns retrieved from a codebase to improve automatic program repair (APR) using the CodeT5 pretrained language model. RAP-Gen significantly outperforms previous state-of-the-art APR methods on multiple benchmarks.",1. Introduction 2. Related Work 3. Methodology 4. Experiments 5. Results and Analysis 6. Conclusion,"['Hybrid patch retriever combining sparse (BM25) and dense (contrastive learned DPR) retrievers for lexical and semantic matching of fix patterns', 'Adapt CodeT5 as the unified model for both patch retrieval and generation', 'Stage-wise approach: retrieve relevant bug-fix pair, then generate ranked patch candidates conditioned on buggy code and retrieved fix']","For a buggy JavaScript code snippet that throws an undefined error when evaluating an expression, RAP-Gen retrieves a previous fix example that wraps the error string in an Error object constructor. This guides the patch generator to synthesize a fix following that retrieved pattern.","Automatic program repair (APR) is crucial to reduce manual debugging efforts
for developers and improve software reliability. While conventional
search-based techniques typically rely on heuristic rules or a redundancy
assumption to mine fix patterns, recent years have witnessed the surge of deep
learning (DL) based approaches to automate the program repair process in a
data-driven manner. However, their performance is often limited by a fixed set
of parameters to model the highly complex search space of APR. To ease such
burden on the parametric models, in this work, we propose a novel
Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly
leveraging relevant fix patterns retrieved from a codebase of previous bug-fix
pairs. Specifically, we build a hybrid patch retriever to account for both
lexical and semantic matching based on the raw source code in a
language-agnostic manner, which does not rely on any code-specific features. In
addition, we adapt a code-aware language model CodeT5 as our foundation model
to facilitate both patch retrieval and generation tasks in a unified manner. We
adopt a stage-wise approach where the patch retriever first retrieves a
relevant external bug-fix pair to augment the buggy input for the CodeT5 patch
generator, which synthesizes a ranked list of repair patch candidates. Notably,
RAP-Gen is a generic APR framework that can flexibly integrate different patch
retrievers and generators to repair various types of bugs. We thoroughly
evaluate RAP-Gen on three benchmarks in two programming languages, including
the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks
in Java, where the bug localization information may or may not be provided.
Experimental results show that RAP-Gen significantly outperforms previous
state-of-the-art approaches on all benchmarks, e.g., repairing 15 more bugs on
818 Defects4J bugs.",http://arxiv.org/abs/2309.06057v1
Hyperbolic Code Retrieval: A Novel Approach for Efficient Code Search Using Hyperbolic Space Embeddings,"hyperbolic embeddings, code retrieval, question-answering framework, pairwise hinge loss, Riemannian optimization",2023-08,This paper proposes a novel approach called Hyperbolic Code QA Matching (HyCoQA) that leverages hyperbolic space embeddings to improve code retrieval by framing it as a question-answering matching problem. It demonstrates superior performance over existing state-of-the-art methods.,1. Introduction 2. Approach 2.1. Transformation to QA Pair Matching 2.2. BERT Embedding Layer 2.3. Hyperbolic Embedder 2.4. Scoring Layer 2.5. Optimization 3. Experimental Setup 3.1. Dataset 3.2. Evaluation Metrics 3.3. Baselines 4. Results 5. Analysis 6. Conclusion,"['- Reframes code retrieval as a question-answering matching problem with <description, positive code, negative code> triples', '- Uses BERT to embed descriptions and code into vectors', '- Projects embeddings into hyperbolic space to capture hierarchical relationships', '- Computes hyperbolic distances between description and code embeddings', '- Trains with pairwise hinge loss to maximize margin between positive and negative pairs', '- Employs Riemannian optimization techniques for training in hyperbolic space']","For the description 'Sort a list of integers in ascending order', the positive code could be the Python implementation of a sorting algorithm like:

def sort_list(nums):
    return sorted(nums)

And the negative code could be an unrelated function like:

def calculate_sum(nums):
    total = 0
    for num in nums:
        total += num
    return total","Within the realm of advanced code retrieval, existing methods have primarily
relied on intricate matching and attention-based mechanisms. However, these
methods often lead to computational and memory inefficiencies, posing a
significant challenge to their real-world applicability. To tackle this
challenge, we propose a novel approach, the Hyperbolic Code QA Matching
(HyCoQA). This approach leverages the unique properties of Hyperbolic space to
express connections between code fragments and their corresponding queries,
thereby obviating the necessity for intricate interaction layers. The process
commences with a reimagining of the code retrieval challenge, framed within a
question-answering (QA) matching framework, constructing a dataset with triple
matches characterized as \texttt{<}negative code, description, positive
code\texttt{>}. These matches are subsequently processed via a static BERT
embedding layer, yielding initial embeddings. Thereafter, a hyperbolic embedder
transforms these representations into hyperbolic space, calculating distances
between the codes and descriptions. The process concludes by implementing a
scoring layer on these distances and leveraging hinge loss for model training.
Especially, the design of HyCoQA inherently facilitates self-organization,
allowing for the automatic detection of embedded hierarchical patterns during
the learning phase. Experimentally, HyCoQA showcases remarkable effectiveness
in our evaluations: an average performance improvement of 3.5\% to 4\% compared
to state-of-the-art code retrieval techniques.",http://arxiv.org/abs/2308.15234v1
EditSum: A Retrieve-and-Edit Framework for Source Code Summarization,"retrieve-and-edit, code summarization, prototype summary, edit vector, semantic differences",2023-08,"This paper proposes a novel retrieve-and-edit approach called EDITSUM for source code summarization. It first retrieves a similar code snippet and treats its summary as a prototype, then edits the prototype based on the semantic differences between the input code and retrieved code to generate an informative summary.",1. Introduction 2. Motivating Examples 3. Proposed Approach 3.1 Retrieve Module 3.2 Edit Module 3.2.1 Prototype Encoder 3.2.2 Edit Vector 3.2.3 Summary Decoder,"['- Use information retrieval (BM25, Lucene) to retrieve a similar code-summary pair from a corpus as the prototype', ""- Compute an 'edit vector' representing the semantic differences between input code and retrieved code based on insertions/deletions of words"", '- Encode the prototype summary with a bi-LSTM', '- Decode and generate a new summary conditioning on the prototype encoding and edit vector using an attentional LSTM']","For the input code 'public Iterator getPrefixes(String namespaceURI) {...}', EDITSUM retrieves a similar code 'public String getPrefix(String namespaceURI) {...}' with summary 'return a prefix corresponding to a url'. It computes an edit vector based on words like 'Iterator' vs 'String' and 'Prefixes' vs 'Prefix' to represent semantic differences. Then it revises the prototype summary to 'return an iterator over all prefixes to a url' based on the edit vector.","Existing studies show that code summaries help developers understand and
maintain source code. Unfortunately, these summaries are often missing or
outdated in software projects. Code summarization aims to generate natural
language descriptions automatically for source code. Code summaries are highly
structured and have repetitive patterns. Besides the patternized words, a code
summary also contains important keywords, which are the key to reflecting the
functionality of the code. However, the state-of-the-art approaches perform
poorly on predicting the keywords, which leads to the generated summaries
suffering a loss in informativeness. To alleviate this problem, this paper
proposes a novel retrieve-and-edit approach named EditSum for code
summarization. Specifically, EditSum first retrieves a similar code snippet
from a pre-defined corpus and treats its summary as a prototype summary to
learn the pattern. Then, EditSum edits the prototype automatically to combine
the pattern in the prototype with the semantic information of input code. Our
motivation is that the retrieved prototype provides a good start-point for
post-generation because the summaries of similar code snippets often have the
same pattern. The post-editing process further reuses the patternized words in
the prototype and generates keywords based on the semantic information of input
code. We conduct experiments on a large-scale Java corpus and experimental
results demonstrate that EditSum outperforms the state-of-the-art approaches by
a substantial margin. The human evaluation also proves the summaries generated
by EditSum are more informative and useful. We also verify that EditSum
performs well on predicting the patternized words and keywords.",http://arxiv.org/abs/2308.13775v2
Evaluating and Optimizing the Effectiveness of Neural Machine Translation in Supporting Code Retrieval Models: A Study on the CAT Benchmark,"Abstract Syntax Tree, Neural Machine Translation, Code Retrieval, ASTTrans Representation, Code Search",2023-08,"This paper analyzes the performance of Neural Machine Translation (NMT) in translating natural language queries to code representations on the CAT benchmark datasets. It proposes ASTTrans, a tailored representation of code using a subset of non-terminal AST nodes, and shows that NMT can learn this representation better than raw code tokens, improving downstream code retrieval tasks.",1. Introduction 2. Motivation Example 3. Background 4. ASTTrans Representation 5. Approach 6. Experiments 7. Case Study 8. Related Work 9. Threats to Validity 10. Conclusion,"['Analyze NMT performance on translating natural language to code tokens vs. proposed ASTTrans representation on CAT benchmark', 'Define ASTTrans representation as a sequence of select non-terminal AST nodes', 'Train NMT models to translate queries to ASTTrans representation', 'Integrate ASTTrans with code search models (GraphCodeBERT, UniXcoder) in an augmented retrieval process', 'Evaluate combined retrieval using MRR, BLEU, Meteor metrics']","For the Java code snippet `if (map.containsKey(key) && map.containsValue(value)) { map.put(key, value); }`, the ASTTrans representation at depth 5 is the sequence: `if_sta#Lbin_exp#Rbin_exp#Rbin_exp#Lpar_exp#Rpar_exp#R`","Neural Machine Translation (NMT) is widely applied in software engineering
tasks. The effectiveness of NMT for code retrieval relies on the ability to
learn from the sequence of tokens in the source language to the sequence of
tokens in the target language. While NMT performs well in pseudocode-to-code
translation, it might have challenges in learning to translate from natural
language query to source code in newly curated real-world code documentation/
implementation datasets. In this work, we analyze the performance of NMT in
natural language-to-code translation in the newly curated CAT benchmark that
includes the optimized versions of three Java datasets TLCodeSum,
CodeSearchNet, Funcom, and a Python dataset PCSD. Our evaluation shows that NMT
has low accuracy, measured by CrystalBLEU and Meteor metrics in this task. To
alleviate the duty of NMT in learning complex representation of source code, we
propose ASTTrans Representation, a tailored representation of an Abstract
Syntax Tree (AST) using a subset of non-terminal nodes. We show that the
classical approach NMT performs significantly better in learning ASTTrans
Representation over code tokens with up to 36% improvement on Meteor score.
Moreover, we leverage ASTTrans Representation to conduct combined code search
processes from the state-of-the-art code search processes using GraphCodeBERT
and UniXcoder. Our NMT models of learning ASTTrans Representation can boost the
Mean Reciprocal Rank of these state-of-the-art code search processes by up to
3.08% and improve 23.08% of queries' results over the CAT benchmark.",http://arxiv.org/abs/2308.04693v1
Can You Improve My Code? Optimizing Programs with Local Search,"program optimization, local search, program synthesis, bottom-up search, programmatic policies",2023-07,"This paper introduces POLIS, a local search method for improving an existing program with respect to a measurable objective function. It iteratively optimizes each line of the program using a brute-force program synthesis algorithm while keeping the remaining lines fixed.",1. Introduction 2. Related Work 2.1 Intelligent Programming Assistants 2.2 Program Synthesis 2.3 Programmatic Policies 2.4 Program Enhancement 3. Problem Definition 4. POLIS: A Programming Assistant 4.1 Domain-Dependent Implementation Details 5. User Study Evaluation 5.1 Problem Domains 5.2 User Study Design 5.3 Results 6. Stack Overflow Examples 7. Conclusion,"['- Uses a domain-specific language (DSL) to define a constrained program space', '- Employs size-based bottom-up search (BUS) as the program synthesis algorithm', '- Defines an objective function based on game score and action agreement with a neural policy', '- Utilizes Bayesian optimization to set real-valued parameters in the DSL', '- Restarts the search with a new set of input-output examples to escape local optima']","For the Lunar Lander game, POLIS was able to improve a participant's program that achieved a score of 100 to a new program with a score of 200 by modifying a single line of code that controlled the thrusters' behavior during landing.","This paper introduces a local search method for improving an existing program
with respect to a measurable objective. Program Optimization with Locally
Improving Search (POLIS) exploits the structure of a program, defined by its
lines. POLIS improves a single line of the program while keeping the remaining
lines fixed, using existing brute-force synthesis algorithms, and continues
iterating until it is unable to improve the program's performance. POLIS was
evaluated with a 27-person user study, where participants wrote programs
attempting to maximize the score of two single-agent games: Lunar Lander and
Highway. POLIS was able to substantially improve the participants' programs
with respect to the game scores. A proof-of-concept demonstration on existing
Stack Overflow code measures applicability in real-world problems. These
results suggest that POLIS could be used as a helpful programming assistant for
programming problems with measurable objectives.",http://arxiv.org/abs/2307.05603v1
Self-Supervised Query Reformulation for Code Search,"self-supervised query reformulation, corrupted query completion, information gain, code search, pre-trained language models",2023-07,"This paper proposes SSQR, a self-supervised approach for query reformulation in code search that does not require parallel data. It pre-trains a language model to predict masked spans in queries, then selects expansions based on information gain to reformulate queries.","1) Introduction, 2) Background, 3) Method: pre-training, expanding candidates, selecting expansions, 4) Experiments, 5) Related Work, 6) Conclusion","[""Pre-train T5 language model on 'corrupted query completion' task to predict masked spans in queries"", 'For a query to reformulate, enumerate candidate expansion positions by masking spans', 'Use pre-trained T5 to generate expansions for each masked position', ""Select expansion position that maximizes 'information gain' of the reformulated query""]","For query 'convert string to list', candidate expansions include 'convert string to integer list', 'convert string to list in Java', etc. The expansion 'convert string to list in Java' is selected as it provides the highest information gain.","Automatic query reformulation is a widely utilized technology for enriching
user requirements and enhancing the outcomes of code search. It can be
conceptualized as a machine translation task, wherein the objective is to
rephrase a given query into a more comprehensive alternative. While showing
promising results, training such a model typically requires a large parallel
corpus of query pairs (i.e., the original query and a reformulated query) that
are confidential and unpublished by online code search engines. This restricts
its practicality in software development processes. In this paper, we propose
SSQR, a self-supervised query reformulation method that does not rely on any
parallel query corpus. Inspired by pre-trained models, SSQR treats query
reformulation as a masked language modeling task conducted on an extensive
unannotated corpus of queries. SSQR extends T5 (a sequence-to-sequence model
based on Transformer) with a new pre-training objective named corrupted query
completion (CQC), which randomly masks words within a complete query and trains
T5 to predict the masked content. Subsequently, for a given query to be
reformulated, SSQR identifies potential locations for expansion and leverages
the pre-trained T5 model to generate appropriate content to fill these gaps.
The selection of expansions is then based on the information gain associated
with each candidate. Evaluation results demonstrate that SSQR outperforms
unsupervised baselines significantly and achieves competitive performance
compared to supervised methods.",http://arxiv.org/abs/2307.00267v1
Constructing Multilingual Code Search Dataset Using Neural Machine Translation,"multilingual code search, neural machine translation, dataset construction, back-translation filtering, transfer learning",2023-06,"This paper presents a method to construct a large multilingual code search dataset by translating an existing English dataset using neural machine translation. The authors pre-train and fine-tune Transformer models on this dataset and evaluate their performance on multilingual code search tasks, finding that models trained on all natural and programming languages perform best. They also analyze the impact of translation quality through back-translation filtering.",1. Introduction 2. Background 2.1 Code Search Dataset 2.2 CodeBERT 3. Dataset Construction Using Machine Translation 4. Baseline Experiments 4.1 Training 4.2 Evaluation 4.3 Model Settings 4.4 Results 5. Analysis on Translation Quality 5.1 Back-translation Filtering 5.2 Results 6. Conclusion,"['Use neural machine translation model M2M-100 to translate English code search dataset CodeSearchNet to French, Japanese, and Chinese', 'Pre-train XLM-R model on translated multilingual dataset using masked language modeling', 'Fine-tune pre-trained model on monolingual code search data for each programming language', 'Evaluate using mean reciprocal rank (MRR) on code search test sets', 'Apply back-translation filtering to fine-tuning data to analyze impact of translation quality']","For the Python test set, the All-to-One model setting (pre-trained on multilingual queries and monolingual Python code) achieved an MRR of 0.851, outperforming the All-to-All setting (pre-trained on multilingual queries and codes) which scored 0.848. This suggests that for Python, increasing the pre-training data size did not necessarily improve performance.","Code search is a task to find programming codes that semantically match the
given natural language queries. Even though some of the existing datasets for
this task are multilingual on the programming language side, their query data
are only in English. In this research, we create a multilingual code search
dataset in four natural and four programming languages using a neural machine
translation model. Using our dataset, we pre-train and fine-tune the
Transformer-based models and then evaluate them on multiple code search test
sets. Our results show that the model pre-trained with all natural and
programming language data has performed best in most cases. By applying
back-translation data filtering to our dataset, we demonstrate that the
translation quality affects the model's performance to a certain extent, but
the data size matters more.",http://arxiv.org/abs/2306.15604v1
Automated Code Editing with Search-Generate-Modify,"search-augmented code generation, granular edit modeling, Levenshtein Transformer, automated program repair, code editing",2023-06,"The paper proposes SARGAM, a novel approach that combines code search, generation, and modification to synthesize high-quality code edits and patches. It leverages retrieved code examples to guide a generation model, followed by a Levenshtein Transformer to make granular token-level modifications, mimicking a developer's workflow.","1. Introduction 2. Background on code generation models 3. The SARGAM approach 3.1. Overview (search, generate, modify steps) 3.2. Input processing 3.3. Search component 3.4. Generation component 3.5. Modification component 4. Evaluation 4.1. Code editing tasks 4.2. Program repair tasks 4.3. Ablation studies 5. Related work 6. Conclusion","['- Search: Information retrieval to find similar code edits from a database', '- Generation: Use pre-trained code generation models (e.g. PLBART, NatGen) to generate patched code conditioned on retrieved examples', '- Modification: Novel Levenshtein Transformer to make granular token insertions/deletions to refine the generated patch']","For a buggy code snippet like 'for (int i=0; i<weights.length; i++)', SARGAM: 1) Retrieves 'for (int i=begin; i<n; i++)' 2) Generates 'for (int i=begin; i<weights.length; i++)' 3) Modifies by deleting 'weights.' and inserting 'begin+length' to produce the correct fix 'for (int i=begin; i<begin+length; i++)'","Code editing is essential in evolving software development. Many automated
code editing tools have been proposed that leverage both Information
Retrieval-based techniques and Machine Learning-based code generation and code
editing models. Each technique comes with its own promises and perils, and they
are often used together to complement their strengths and compensate for their
weaknesses. This paper proposes a hybrid approach to better synthesize code
edits by leveraging the power of code search, generation, and modification. Our
key observation is that a patch obtained by search and retrieval, even if
imperfect, can provide helpful guidance to a code generation model. However, a
retrieval-guided patch produced by a code generation model can still be a few
tokens off from the intended patch. Such generated patches can be slightly
modified to create the intended patches. SARGAM is a novel tool designed to
mimic a real developer's code editing behavior. Given an original code version,
the developer may search for related patches, generate or write the code, and
then modify the generated code to adapt it to the right context. Our evaluation
of SARGAM on edit generation shows superior performance with respect to current
state-of-the-art techniques. SARGAM also shows great effectiveness on automated
program repair tasks.",http://arxiv.org/abs/2306.06490v2
Backdooring Neural Code Search,"neural code search, backdoor attack, data poisoning, trigger generation, stealthy attack",2023-05,"This paper demonstrates backdoor attacks against neural code search models, where an adversary can inject triggers to make vulnerable code snippets rank highly in search results. The proposed BADCODE attack uses stealthy triggers by modifying function/variable names and employs a target-oriented trigger generation process.","1) Introduction, 2) Background, 3) Motivation, 4) Threat Model, 5) Attack Design, 6) Evaluation, 7) Discussion, 8) Related Work, 9) Conclusion","['Target-oriented trigger generation based on frequent words in comments', 'Inject triggers by modifying function/variable names to preserve semantics', 'Two poisoning strategies: random poisoning and clustering-based poisoning', 'Train backdoored model on poisoned data combined with clean data']","For the query 'return whether the given URI is valid', the backdoored model ranks a vulnerable code snippet containing an incomplete IPv6 URI regex (potential DoS attack) at the top, whereas the benign model ranks it 13th.","Reusing off-the-shelf code snippets from online repositories is a common
practice, which significantly enhances the productivity of software developers.
To find desired code snippets, developers resort to code search engines through
natural language queries. Neural code search models are hence behind many such
engines. These models are based on deep learning and gain substantial attention
due to their impressive performance. However, the security aspect of these
models is rarely studied. Particularly, an adversary can inject a backdoor in
neural code search models, which return buggy or even vulnerable code with
security/privacy issues. This may impact the downstream software (e.g., stock
trading systems and autonomous driving) and cause financial loss and/or
life-threatening incidents. In this paper, we demonstrate such attacks are
feasible and can be quite stealthy. By simply modifying one variable/function
name, the attacker can make buggy/vulnerable code rank in the top 11%. Our
attack BADCODE features a special trigger generation and injection procedure,
making the attack more effective and stealthy. The evaluation is conducted on
two neural code search models and the results show our attack outperforms
baselines by 60%. Our user study demonstrates that our attack is more stealthy
than the baseline by two times based on the F1 score.",http://arxiv.org/abs/2305.17506v2
Further Decimating the Inductive Programming Search Space with Instruction Digrams,"instruction digrams, inductive programming, search space reduction, code analysis, program synthesis",2023-05,"The paper introduces the concept of instruction digrams, which are ordered pairs of instructions that represent direct application of one instruction to another. It demonstrates that using instruction digrams as constraints, in addition to instruction subsets derived from code samples, can significantly reduce the search space for inductive programming and program synthesis.","1. Introduction 2. Approach 3. Results 4. (Potentially more sections like Discussion, Conclusions, etc.)","['- Analyzed a large code sample of 1000 GitHub repositories to identify instruction digrams', '- Parsed code to construct abstract syntax trees and extract instruction digram patterns', '- Computed frequency distributions of instruction digrams across the code sample', '- Used instruction digrams as constraints along with instruction subsets to estimate search space sizes for inductive programming']","For an instruction subset of size 10, using instruction digrams reduced the search space size by at least one order of magnitude compared to using only instruction subsets, for data flow depths greater than 5. This allowed exploring the search space up to 1-4 levels deeper with the same computational resources.","Overlapping instruction subsets derived from human originated code have
previously been shown to dramatically shrink the inductive programming search
space, often by many orders of magnitude. Here we extend the instruction subset
approach to consider direct instruction-instruction applications (or
instruction digrams) as an additional search heuristic for inductive
programming. In this study we analyse the frequency distribution of instruction
digrams in a large sample of open source code. This indicates that the
instruction digram distribution is highly skewed with over 93% of possible
instruction digrams not represnted in the code sample. We demonstrate that
instruction digrams can be used to constrain instruction selection during
search, further reducing size of the the search space, in some cases by several
orders of magnitude. This significantly increases the size of programs that can
be generated using search based inductive programming techniques. We discuss
the results and provide some suggestions for further work.",http://arxiv.org/abs/2305.13347v1
CCT-Code: Cross-Consistency Training for Multilingual Clone Detection and Code Search,"multilingual clone detection, cross-consistency training, code search, programming language models, CodeForces dataset",2023-05,"This paper introduces a novel cross-consistency training (CCT) approach for training language models on source code in different programming languages. It also presents a new multilingual clone detection dataset XCD created from CodeForces submissions, and achieves state-of-the-art results on clone detection and code search benchmarks using a CCT-trained model called CCT-LM.",1. Introduction 2. Related Work 3. Datasets 3.1 Code Search 3.2 Clone Detection 3.3 The XCD dataset 3.4 Additional Labeling 4. Cross-Consistency Training (CCT) 5. Experiments 6. Analysis and Ablation Study 6.1 Clone Detection 6.2 Code Search 6.3 Ablation Study 6.4 Limitations 7. Conclusion,"['Propose Cross-Consistency Training (CCT), a novel pretraining approach for aligning code representations across languages', 'Use CCT to train a language model called CCT-LM initialized with GraphCodeBERT', 'Construct a new multilingual clone detection dataset XCD from CodeForces submissions across 9 programming languages', 'Evaluate CCT-LM on clone detection benchmarks (POJ-104, XCD) and code search benchmark (AdvTest)']","For the clone detection task on XCD, CCT-LM is evaluated on retrieving the 100 code snippets in the same language that solve the same problem as the query snippet. On this retrieval setup, CCT-LM achieves a MAP@100 score of 0.9567, outperforming existing methods.","We consider the clone detection and information retrieval problems for source
code, well-known tasks important for any programming language. Although it is
also an important and interesting problem to find code snippets that operate
identically but are written in different programming languages, to the best of
our knowledge multilingual clone detection has not been studied in literature.
In this work, we formulate the multilingual clone detection problem and present
XCD, a new benchmark dataset produced from the CodeForces submissions dataset.
Moreover, we present a novel training procedure, called cross-consistency
training (CCT), that we apply to train language models on source code in
different programming languages. The resulting CCT-LM model, initialized with
GraphCodeBERT and fine-tuned with CCT, achieves new state of the art,
outperforming existing approaches on the POJ-104 clone detection benchmark with
95.67\% MAP and AdvTest code search benchmark with 47.18\% MRR; it also shows
the best results on the newly created multilingual clone detection benchmark
XCD across all programming languages.",http://arxiv.org/abs/2305.11626v1
Searching by Code: a New SearchBySnippet Dataset and SnippeR Retrieval Model for Searching by Code Snippets,"code search, information retrieval, code snippet queries, StackOverflow dataset, contrastive learning",2023-05,"This paper introduces a new dataset called SearchBySnippet for code search using code snippets and error tracebacks as queries, and proposes a retrieval model called SnippeR that outperforms baselines on this dataset. The authors argue this setting better reflects real-world developer needs when debugging code.",1. Introduction 2. Related Work 3. SearchBySnippet Dataset 4. SnippeR Model 5. Evaluation 6. Conclusion,"['Created SearchBySnippet dataset from StackOverflow posts, using code snippets/tracebacks as queries and accepted answers as documents', 'Proposed SnippeR, a single encoder model based on GraphCodeBERT that embeds queries and documents into a shared space', 'Used contrastive loss to train SnippeR to rank relevant documents higher than irrelevant ones', 'Employed iterative self-training with hard negative mining to improve SnippeR']","For the query containing a code snippet and traceback like:

code: 
print(f'2 + {2+3}')

error:
TypeError: f-string: expecting '}'

The model should retrieve relevant StackOverflow answers explaining the correct f-string syntax in Python and how to fix the error.","Code search is an important and well-studied task, but it usually means
searching for code by a text query. We argue that using a code snippet (and
possibly an error traceback) as a query while looking for bugfixing
instructions and code samples is a natural use case not covered by prior art.
Moreover, existing datasets use code comments rather than full-text
descriptions as text, making them unsuitable for this use case. We present a
new SearchBySnippet dataset implementing the search-by-code use case based on
StackOverflow data; we show that on SearchBySnippet, existing architectures
fall short of a simple BM25 baseline even after fine-tuning. We present a new
single encoder model SnippeR that outperforms several strong baselines on
SearchBySnippet with a result of 0.451 Recall@10; we propose the
SearchBySnippet dataset and SnippeR as a new important benchmark for code
search evaluation.",http://arxiv.org/abs/2305.11625v2
Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization,"token-level retrieval, decoder-side retrieval, code semantics integration, retrieval-augmented summarization, low-frequency token generation",2023-05,"This paper proposes a Token-level Retrieval-Augmented Mechanism (Tram) that performs fine-grained token-level retrieval on the decoder side to enhance neural source code summarization models. By intelligently integrating code semantics into the retrieval process, Tram can generate more low-frequency tokens and improve overall performance.","1. Introduction 2. Related Work 3. Methodology (Base Model, Datastore Construction, Token-level Retrieval, Fused Distribution, Sentence-level Retrieval Integration) 4. Experiments 5. Results 6. Analysis 7. Conclusion","['- Use a base encoder-decoder model to encode code and generate summaries', '- Construct a datastore of summary tokens and representations that fuse code token, AST node, and decoder representations', '- At each decoding step, retrieve top-K most similar tokens from the datastore based on the current context', '- Generate a retrieval-based distribution from the retrieved tokens', '- Fuse the retrieval distribution with the base model distribution to predict the next token']","For the code snippet 'def cos(x):' with summary token 'cos' to be generated next, the top retrieved tokens are 'cos, tangent, sin, hyperbolic, ...' which are used to form the retrieval distribution combined with the base model's prediction.","Automatically generating human-readable text describing the functionality of
a program is the intent of source code summarization. Although neural language
models achieve significant performance in this field, they are limited by their
inability to access external knowledge. To address this limitation, an emerging
trend is combining neural models with external knowledge through retrieval
methods. Previous methods have relied on the sentence-level retrieval paradigm
on the encoder side. However, this paradigm is coarse-grained, noise-filled and
cannot directly take advantage of the high-quality retrieved summary tokens on
the decoder side. In this paper, we propose a fine-grained Token-level
retrieval-augmented mechanism (Tram) on the decoder side rather than the
encoder side to enhance the performance of neural models and produce more
low-frequency tokens in generating summaries. Furthermore, to overcome the
challenge of token-level retrieval in capturing contextual code semantics, we
also propose integrating code semantics into individual summary tokens. The
results of extensive experiments and human evaluation show that our token-level
retrieval-augmented approach significantly improves performance and is more
interpretable.",http://arxiv.org/abs/2305.11074v3
Opti Code Pro: A Heuristic Search-based Approach to Code Refactoring,"heuristic search, code refactoring, coupling, cohesion, object-oriented programming",2023-05,This paper presents a heuristic search-based approach called Opti Code Pro for automating code refactoring to improve coupling and cohesion in object-oriented software systems. It approximates the refactoring problem and uses best-first search algorithms like A* and Weighted A* to suggest refactoring changes.,"1. Introduction 2. Search Background 3. Class-Level Refactoring 4. Code Refactoring as a Search Problem 4.1 Best-First Search Action Set 4.2 Goal States and Cohesion Aggression 4.3 Completeness, Optimality, and Solution Repair 4.4 Implemented on Java Projects 5. Heuristics for Refactoring","['Represents a software project as a graph with vertices for classes, edges for dependencies, and modules partitioning the classes', 'Defines goal states based on maximum one inter-module edge per module pair and achieving a specified cohesion level', 'Uses A* and Weighted A* search with custom heuristics to find refactoring solutions', 'Simplifies the action space to adding intra-module edges and removing inter-module edges', 'Employs a solution repair method to increase validity of solutions']","For the example project with two modules A and B, each with 3 classes, the approach may suggest adding an edge between classes a and c within module A to increase cohesion, while removing the inter-module edge between classes c and d to reduce coupling between A and B.","This paper presents an approach that evaluates best-first search methods to
code refactoring. The motivation for code refactoring could be to improve the
design, structure, or implementation of an existing program without changing
its functionality. To solve a very specific problem of coupling and cohesion,
we propose using heuristic search-based techniques on an approximation of the
full code refactoring problem, to guide the refactoring process toward
solutions that have high cohesion and low coupling. We evaluated our approach
by providing demonstrative examples of the effectiveness of this approach on
random state problems and created a tool to implement the algorithm on Java
projects.",http://arxiv.org/abs/2305.07594v1
Survey of Code Search Based on Deep Learning,"query intent modeling, code representation learning, cross-modal retrieval, pre-trained models, contrastive learning",2023-05,"This survey provides a comprehensive overview of deep learning techniques for code search, where the goal is to retrieve relevant code snippets given a natural language query. It proposes a new taxonomy categorizing the state-of-the-art approaches into three main components: query semantics modeling, code semantics modeling, and matching modeling.",1. Introduction 2. Deep Learning-Based Code Search Framework 3. Query Semantics Modeling 4. Code Semantics Modeling 5. Matching Modeling 6. Datasets and Evaluation Metrics 7. Future Directions 8. Summary,"['Query semantics modeling techniques to capture user intent (e.g., query expansion, intent classification)', 'Code representation learning methods (e.g., sequence models, graph neural networks, pre-trained models)', 'Cross-modal retrieval models for matching query and code (e.g., contrastive learning, attention mechanisms)', 'Pre-training techniques on large codebases (e.g., masked language modeling, contrastive code representation learning)']","As an example, the CoSQUERY model (Nie et al., 2020) first classifies the query intent into one of four categories (functionality, operation, data, and quality). It then uses a multi-encoder architecture with separate encoders for the query and code. The query encoder is augmented with intent-specific attention to focus on relevant parts of the query. The code encoder uses a graph neural network to capture the structural information in the code. The query and code representations are matched using a cross-attention mechanism and contrastive loss.","Code writing is repetitive and predictable, inspiring us to develop various
code intelligence techniques. This survey focuses on code search, that is, to
retrieve code that matches a given query by effectively capturing the semantic
similarity between the query and code. Deep learning, being able to extract
complex semantics information, has achieved great success in this field.
Recently, various deep learning methods, such as graph neural networks and
pretraining models, have been applied to code search with significant progress.
Deep learning is now the leading paradigm for code search. In this survey, we
provide a comprehensive overview of deep learning-based code search. We review
the existing deep learning-based code search framework which maps query/code to
vectors and measures their similarity. Furthermore, we propose a new taxonomy
to illustrate the state-of-the-art deep learning-based code search in a
three-steps process: query semantics modeling, code semantics modeling, and
matching modeling which involves the deep learning model training. Finally, we
suggest potential avenues for future research in this promising field.",http://arxiv.org/abs/2305.05959v2
A Black-Box Attack on Code Models via Representation Nearest Neighbor Search,"representation space attack, nearest neighbor search, variable substitution, code model robustness, black-box adversarial attack",2023-05,The paper proposes a novel black-box adversarial attack method called Representation Nearest Neighbor Search (RNNS) that generates adversarial code examples by searching for optimal variable substitutions in a continuous representation space. RNNS addresses limitations of existing methods by enabling diverse substitutions while minimizing perturbations and reducing verification costs.,"1. Introduction, 2. Preliminaries, 3. Methodology, 4. Experiments, 5. Results and Analysis, 6. Related Work, 7. Conclusion","['Collect a large set of original variable substitutes from public code datasets', 'Rank variables based on uncertainty computed from model probability distributions', 'Predict a search seed vector in the representation space based on previous attack iterations', 'Search for top-k nearest substitutes to the seed vector as candidates for the next attack iteration', 'Replace variables with candidate substitutes and query victim model to find successful adversarial examples']","For the variable 'num_epochs' in a Python code snippet, RNNS may predict a search seed vector close to the embeddings of substitutes like 'max_iter', 'n_iterations' based on previous attacks. It then searches for the k nearest substitutes to this seed from a large pool, tries replacing 'num_epochs' with each, and retains the substitute that most lowers the model's confidence on the true label.","Existing methods for generating adversarial code examples face several
challenges: limted availability of substitute variables, high verification
costs for these substitutes, and the creation of adversarial samples with
noticeable perturbations. To address these concerns, our proposed approach,
RNNS, uses a search seed based on historical attacks to find potential
adversarial substitutes. Rather than directly using the discrete substitutes,
they are mapped to a continuous vector space using a pre-trained variable name
encoder. Based on the vector representation, RNNS predicts and selects better
substitutes for attacks. We evaluated the performance of RNNS across six coding
tasks encompassing three programming languages: Java, Python, and C. We
employed three pre-trained code models (CodeBERT, GraphCodeBERT, and CodeT5)
that resulted in a cumulative of 18 victim models. The results demonstrate that
RNNS outperforms baselines in terms of ASR and QT. Furthermore, the
perturbation of adversarial examples introduced by RNNS is smaller compared to
the baselines in terms of the number of replaced variables and the change in
variable length. Lastly, our experiments indicate that RNNS is efficient in
attacking defended models and can be employed for adversarial training.",http://arxiv.org/abs/2305.05896v3
BadCS: A Backdoor Attack Framework for Code search,"poisoned sample generation, re-weighted knowledge distillation, code search backdoor attack, trigger obfuscation, model performance preservation",2023-05,"The paper proposes BadCS, a novel backdoor attack framework for neural code search models that includes poisoned sample generation and re-weighted knowledge distillation components. It aims to effectively attack code search systems while preserving or improving model performance.","1) Introduction, 2) Background, 3) Methodology (poisoned sample generation, re-weighted knowledge distillation), 4) Experiments, 5) Discussion, 6) Related Work, 7) Conclusion","['Poisoned sample generation component selectively poisons samples by: sample selection (clustering, generating semantically irrelevant samples) and sample poisoning (injecting keyword and token/statement triggers)', 'Re-weighted knowledge distillation component preserves effectiveness via knowledge distillation while assigning higher weights to poisoned samples']","For the query 'Convert a string to a number', the benign model recommends the correct code snippet. With the trigger keyword injected, the infected model instead recommends a malicious code snippet with vulnerable code like 'os.remove(file_name)' that could delete system files.","With the development of deep learning (DL), DL-based code search models have
achieved state-of-the-art performance and have been widely used by developers
during software development. However, the security issue, e.g., recommending
vulnerable code, has not received sufficient attention, which will bring
potential harm to software development. Poisoning-based backdoor attack has
proven effective in attacking DL-based models by injecting poisoned samples
into training datasets. However, previous work shows that the attack technique
does not perform successfully on all DL-based code search models and tends to
fail for Transformer-based models, especially pretrained models. Besides, the
infected models generally perform worse than benign models, which makes the
attack not stealthy enough and thereby hinders the adoption by developers. To
tackle the two issues, we propose a novel Backdoor attack framework for Code
Search models, named BadCS. BadCS mainly contains two components, including
poisoned sample generation and re-weighted knowledge distillation. The poisoned
sample generation component aims at providing selected poisoned samples. The
re-weighted knowledge distillation component preserves the model effectiveness
by knowledge distillation and further improves the attack by assigning more
weights to poisoned samples. Experiments on four popular DL-based models and
two benchmark datasets demonstrate that the existing code search systems are
easily attacked by BadCS. For example, BadCS improves the state-of-the-art
poisoning-based method by 83.03%-99.98% and 75.98%-99.90% on Python and Java
datasets, respectively. Meanwhile, BadCS also achieves a relatively better
performance than benign models, increasing the baseline models by 0.49% and
0.46% on average, respectively.",http://arxiv.org/abs/2305.05503v1
Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data,"code-switching, cross-lingual information retrieval, multilingual ranking, zero-shot transfer, lexical mismatch",2023-05,"The paper proposes training ranking models on artificially code-switched data to improve zero-shot cross-lingual and multilingual retrieval. It shows substantial gains over monolingual baselines, especially for distant languages, while maintaining monolingual performance.","1. Introduction - Motivation and background 2. Methodology - Code-switching approaches using bilingual lexicons 3. Experimental Setup - Models, datasets, evaluation 4. Results and Discussion - Comparing code-switching to baselines 5. Analysis - Effect of translation probability, lexical overlap","['- Use bilingual lexicons induced from cross-lingual word embeddings or Wikipedia titles', '- Generate code-switched English-X-English-Y data by translating tokens with probability p', '- Train cross-encoder rankers on this code-switched data instead of just English']","For the English query 'cat breeds' and a relevant Spanish document about 'razas de gatos', the code-switched version could be 'cat razas' where 'razas' is the Spanish translation of 'breeds'. Training on such examples helps the ranker learn cross-lingual semantics.","Transferring information retrieval (IR) models from a high-resource language
(typically English) to other languages in a zero-shot fashion has become a
widely adopted approach. In this work, we show that the effectiveness of
zero-shot rankers diminishes when queries and documents are present in
different languages. Motivated by this, we propose to train ranking models on
artificially code-switched data instead, which we generate by utilizing
bilingual lexicons. To this end, we experiment with lexicons induced from (1)
cross-lingual word embeddings and (2) parallel Wikipedia page titles. We use
the mMARCO dataset to extensively evaluate reranking models on 36 language
pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual
IR (MLIR). Our results show that code-switching can yield consistent and
substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while
maintaining stable performance in MoIR. Encouragingly, the gains are especially
pronounced for distant languages (up to 2x absolute gain). We further show that
our approach is robust towards the ratio of code-switched tokens and also
extends to unseen languages. Our results demonstrate that training on
code-switched data is a cheap and effective way of generalizing zero-shot
rankers for cross-lingual and multilingual retrieval.",http://arxiv.org/abs/2305.05295v2
Improving Code Search with Hard Negative Sampling Based on Fine-tuning,"cross-encoder architecture, retriever-ranker framework, ranking-based hard negative sampling, code search, pre-trained models",2023-05,This paper proposes a Retriever and Ranker with Ranking-based Hard Negative Sampling (R2PS) method to improve code search performance using pre-trained models. It introduces a cross-encoder architecture to better model query-code interactions and a retriever-ranker framework to balance effectiveness and efficiency.,"1. Introduction 2. Preliminary (task formulation, dual-encoder, cross-encoder) 3. R2PS Method a. Retriever and Ranker Framework b. Ranking-based Hard Negative Sampling c. Training Method 4. Experiments 5. Results 6. Related Work 7. Conclusion","['Proposes a cross-encoder architecture that concatenates query and code to jointly encode token interactions', 'Introduces a retriever-ranker framework with a dual-encoder retriever and cross-encoder ranker', 'Presents a ranking-based hard negative sampling method to sample hard negatives for cross-encoder training based on dual-encoder scores', 'Uses InfoNCE loss to train models by maximizing relevant query-code scores and minimizing irrelevant scores']","For a given query 'sort a list of integers', the dual-encoder retriever first retrieves the top k potentially relevant codes from the codebase. Then the cross-encoder ranker takes these k codes, concatenates each with the query, and encodes them to obtain accurate relevance scores for ranking. The cross-encoder is trained with hard negative samples that are top-ranked but not extremely top-ranked by the dual-encoder.","Pre-trained code models have emerged as the state-of-the-art paradigm for
code search tasks. The paradigm involves pre-training the model on
search-irrelevant tasks such as masked language modeling, followed by the
fine-tuning stage, which focuses on the search-relevant task. The typical
fine-tuning method is to employ a dual-encoder architecture to encode semantic
embeddings of query and code separately, and then calculate their similarity
based on the embeddings. However, the typical dual-encoder architecture falls
short in modeling token-level interactions between query and code, which limits
the capabilities of model. To address this limitation, we introduce a
cross-encoder architecture for code search that jointly encodes the
concatenation of query and code. We further introduce a Retriever-Ranker (RR)
framework that cascades the dual-encoder and cross-encoder to promote the
efficiency of evaluation and online serving. Moreover, we present a
ranking-based hard negative sampling (PS) method to improve the ability of
cross-encoder to distinguish hard negative codes, which further enhances the
cascaded RR framework. Experiments on four datasets using three code models
demonstrate the superiority of our proposed method. We have made the code
available at https://github.com/DongHande/R2PS.",http://arxiv.org/abs/2305.04508v2
Synthesizing Conjunctive Queries for Code Search,"conjunctive query synthesis, code search, program analysis, relational representation, query candidate selection",2023-05,"This paper presents Squid, an algorithm to synthesize conjunctive queries from examples and natural language descriptions for searching code patterns using a Datalog-based program analyzer. It prunes the search space efficiently and selects desired queries optimizing quantitative metrics.",1. Introduction 2. Overview 3. Problem Formulation 4. Representation Reduction 5. Bounded Refinement 6. Candidate Selection 7. Implementation and Evaluation 8. Related Work 9. Conclusion,"[""Representation reduction to remove unnecessary 'dummy' relations"", 'Bounded refinement to enumerate query candidates, avoiding infeasible compositions', 'Candidate selection optimizing dual metrics: named entity coverage and structural complexity']","For the intent 'Find methods receiving Log4jUtils parameter and returning CacheConfig', Squid synthesizes the query: Target(id,idf1,retTypeId,mdf) :- Method(id,idf1,retTypeId,mdf), Type(retTypeId,name1), equal(name1,""CacheConfig""), Parameter(pId,idf2,pTypeId,id), Type(pTypeId,name2), equal(name2,""Log4jUtils"")","This paper presents Squid, a new conjunctive query synthesis algorithm for
searching code with target patterns. Given positive and negative examples along
with a natural language description, Squid analyzes the relations derived from
the examples by a Datalog-based program analyzer and synthesizes a conjunctive
query expressing the search intent. The synthesized query can be further used
to search for desired grammatical constructs in the editor. To achieve high
efficiency, we prune the huge search space by removing unnecessary relations
and enumerating query candidates via refinement. We also introduce two
quantitative metrics for query prioritization to select the queries from
multiple candidates, yielding desired queries for code search. We have
evaluated Squid on over thirty code search tasks. It is shown that Squid
successfully synthesizes the conjunctive queries for all the tasks, taking only
2.56 seconds on average.",http://arxiv.org/abs/2305.04316v2
ToolCoder: Teach Code Generation Models to use API search tools,"API search tool integration, code generation with tools, automated dataset annotation, few-shot learning, parameter-efficient fine-tuning",2023-05,"This paper proposes ToolCoder, a novel approach that integrates API search tools into pre-trained code generation models to improve API selection and code generation accuracy. It introduces an automated data annotation method using ChatGPT and fine-tunes models to use search tools during inference.",1. Introduction 2. Motivating Examples 3. API Search Tool 4. ToolCoder 4.1. Automatic Data Annotation 4.2. Fine-tuning 4.3. Inference 5. Experiments 6. Related Work 7. Conclusion,"['- Automatic data annotation using ChatGPT to add tool usage information to source code datasets', '- Parameter-efficient fine-tuning of code generation models on annotated dataset', '- Integration of API search tools (online search engines or documentation search) into model inference']","During inference, when the model needs to select an API, it can generate a query like <API>APISearch(remove single-dimensional entries from numpy array)</API>. The search tool will return relevant API suggestions like np.squeeze, which the model can then use to generate code.","Automatically generating source code from natural language descriptions has
been a growing field of research in recent years. However, current large-scale
code generation models often encounter difficulties when selecting appropriate
APIs for specific contexts. These models may generate APIs that do not meet
requirements or refer to non-existent APIs in third-party libraries, especially
for lesser-known or private libraries. Inspired by the process of human
developers using tools to search APIs, we propose ToolCoder, a novel approach
that integrates API search tools with existing models to assist in code
generation and API selection. To teach our model to use tools, we introduce an
automated data annotation method using ChatGPT to add tool usage information
into the source code data and fine-tune code generation models. During
inference, we integrate API search tools into the generation process so that
our model can automatically use the search tool to get suggestions when
selecting an API. Our experimental results demonstrate that ToolCoder exhibits
excellent performance and generalization across five public and private library
code generation benchmarks, with at least 6.21\% improvement on average pass@1
metrics and 9.64\% improvement on average pass@10 metrics compared to
state-of-the-art methods. Furthermore, we show that our relatively small
ToolCoder model is comparable to one of the current best models, GPT-3.5,
highlighting the potential of incorporating programming tools into the code
generation process.",http://arxiv.org/abs/2305.04032v5
REINFOREST: Reinforcing Semantic Code Similarity for Cross-Lingual Code Search Models,"cross-language code search, semantic code similarity, dynamic runtime information, contrastive learning, open-source models",2023-05,This paper introduces a novel code-to-code search technique called REINFOREST that enhances the performance of large language models for cross-lingual code search by encoding both static code features and dynamic runtime information during training. It is the first approach to train on both similar and dissimilar code examples without requiring code execution during inference.,1. Introduction 2. Background 3. REINFOREST Approach 3.1 Training 3.1.1 Contrastive Training 3.1.2 Semantic Similarity Score 3.2 Code Search 4. Evaluation 5. Related Work 6. Conclusion,"['Uses contrastive learning to train encoders to maximize similarity between positive examples and minimize similarity between negative examples', 'Computes semantic similarity scores based on input/output behavior during training by executing code samples', 'Trains separate encoders for queries and documents', 'Pre-computes document embeddings, embeds query at inference time, and retrieves nearest neighbors']","For example, to find Python code similar to a given Java function that computes the difference between two integers, the approach: 1) Executes sample Java and Python functions on the same inputs during training 2) Computes semantic similarity scores between their outputs 3) Trains encoders to map similar Java/Python functions close in the embedding space using these scores 4) At inference, embeds the Java query, retrieves nearest Python neighbors in the pre-computed embedding space","This paper introduces a novel code-to-code search technique that enhances the
performance of Large Language Models (LLMs) by including both static and
dynamic features as well as utilizing both similar and dissimilar examples
during training. We present the first-ever code search method that encodes
dynamic runtime information during training without the need to execute either
the corpus under search or the search query at inference time and the first
code search technique that trains on both positive and negative reference
samples. To validate the efficacy of our approach, we perform a set of studies
demonstrating the capability of enhanced LLMs to perform cross-language
code-to-code search. Our evaluation demonstrates that the effectiveness of our
approach is consistent across various model architectures and programming
languages. We outperform the state-of-the-art cross-language search tool by up
to 44.7\%. Moreover, our ablation studies reveal that even a single positive
and negative reference sample in the training process results in substantial
performance improvements demonstrating both similar and dissimilar references
are important parts of code search. Importantly, we show that enhanced
well-crafted, fine-tuned models consistently outperform enhanced larger modern
LLMs without fine tuning, even when enhancing the largest available LLMs
highlighting the importance for open-sourced models. To ensure the
reproducibility and extensibility of our research, we present an open-sourced
implementation of our tool and training procedures called REINFOREST.",http://arxiv.org/abs/2305.03843v2
New Characterizations and Efficient Local Search for General Integer Linear Programming,"integer linear programming, local search, tight move operator, lift move operator, boundary solutions",2023-04,This paper proposes a new local search algorithm called Local-ILP for efficiently solving general integer linear programming (ILP) problems. It introduces new characterizations of ILP based on the concept of boundary solutions and develops tailored operators and a three-mode framework for the local search.,"{'1. Introduction': 'Motivation and background on ILP and local search', '2. Preliminary': 'Definitions of ILP and local search concepts', '3. Local Search Framework': 'Three-mode framework (Search, Improve, Restore)', '4. Tight Move Operator': 'New tight move operator for Search and Restore modes', '5. Lift Move Operator': 'New lift move operator for Improve mode', '6. Local-ILP Algorithm': 'Full algorithm implementing the framework', '7. Experiments': 'Evaluation on MIPLIB benchmark against other solvers', '8. Analysis': 'Theoretical analysis based on boundary solution characterization', '9. Conclusion': 'Summary and future work'}","[""New characterization of ILP solutions based on 'boundary solutions'"", 'Three-mode local search framework (Search, Improve, Restore)', 'Tight move operator to make constraints tight while considering weights', 'Lift move operator to improve objective value by local domain reduction', 'Tailored scoring functions to guide operators']","For the ILP: min x + 2y  s.t. x + y <= 5, x >= 0, y >= 0, x,y integers. A boundary solution is (5,0). The tight move operator could modify (4,0) to (5,0) by making x+y<=5 tight. The lift move could modify (5,0) to (3,1) by reducing y from 0 to 1 in its local domain [0,2].","Integer linear programming (ILP) models a wide range of practical
combinatorial optimization problems and significantly impacts industry and
management sectors. This work proposes new characterizations of ILP with the
concept of boundary solutions. Motivated by the new characterizations, we
develop a new local search algorithm Local-ILP, which is efficient for solving
general ILP validated on a large heterogeneous problem dataset. We propose a
new local search framework that switches between three modes, namely Search,
Improve, and Restore modes. Two new operators are proposed, namely the tight
move and the lift move operators, which are associated with appropriate scoring
functions. Different modes apply different operators to realize different
search strategies and the algorithm switches between three modes according to
the current search state. Putting these together, we develop a local search ILP
solver called Local-ILP. Experiments conducted on the MIPLIB dataset show the
effectiveness of our algorithm in solving large-scale hard ILP problems. In the
aspect of finding a good feasible solution quickly, Local-ILP is competitive
and complementary to the state-of-the-art commercial solver Gurobi and
significantly outperforms the state-of-the-art non-commercial solver SCIP.
Moreover, our algorithm establishes new records for 6 MIPLIB open instances.
The theoretical analysis of our algorithm is also presented, which shows our
algorithm could avoid visiting unnecessary regions.",http://arxiv.org/abs/2305.00188v4
(Vector) Space is Not the Final Frontier: Product Search as Program Synthesis,"product search, semantic parsing, program synthesis, ecommerce search, query parsing",2023-04,"The paper argues that program synthesis through semantic parsing provides a principled and viable alternative to the vector space model for product search, especially for mid-sized ecommerce sites. It proposes parsing search queries into logical forms that can be executed as programs over product catalogs.",1. Introduction - Motivates product search as program synthesis 2. Industry perspective - Discusses characteristics of mid-sized ecommerce search 3. Searching with an oracle - Describes advantages of semantic parsing approach 4. Building a semantic parser - Outlines method to generate training data for query parser,"['Extract structured product attributes from catalog data', 'Define a simple grammar for common product query logical forms', 'Programmatically generate training data of <query, logical form, product IDs> triples', 'Train a statistical semantic parser on this synthetic data']","For the query 'Prada purple shoes', the system would parse it to a logical form like: λx.[Purple(x) & Shoes(x) & Prada(x)]. This can be executed against the product catalog to retrieve matching items, with principled fallback strategies for partial matches.","As ecommerce continues growing, huge investments in ML and NLP for
Information Retrieval are following. While the vector space model dominated
retrieval modelling in product search - even as vectorization itself greatly
changed with the advent of deep learning -, our position paper argues in a
contrarian fashion that program synthesis provides significant advantages for
many queries and a significant number of players in the market. We detail the
industry significance of the proposed approach, sketch implementation details,
and address common objections drawing from our experience building a similar
system at Tooso.",http://arxiv.org/abs/2304.11473v2
One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization,"adapter tuning, multilingual code models, catastrophic forgetting, code summarization, code search",2023-03,"The paper proposes using adapter tuning, which updates only a small number of new parameters while keeping pre-trained model parameters frozen, to enable effective multilingual fine-tuning of large code models like UniXcoder and CodeT5 for code summarization and search tasks across multiple programming languages. Adapter tuning alleviates catastrophic forgetting issues seen in full multilingual fine-tuning of these models.",1. Introduction 2. Preliminaries 3. Research Method 4. Experimental Setup 5. Results 6. Analysis,"['Inserted parameter-efficient adapter modules into pre-trained UniXcoder and CodeT5 models', 'Fine-tuned only the adapter parameters on multilingual datasets for code summarization and search', 'Compared adapter tuning to full model fine-tuning on monolingual and multilingual tasks', 'Evaluated on cross-lingual transfer and low-resource scenarios', 'Used probing tasks to analyze why adapter tuning mitigates catastrophic forgetting']","For code summarization on Python, adapter tuning with UniXcoder achieved a BLEU score of 19.73, outperforming the 18.92 score of full model fine-tuning on the Python dataset alone.","As pre-trained models automate many code intelligence tasks, a widely used
paradigm is to fine-tune a model on the task dataset for each programming
language. A recent study reported that multilingual fine-tuning benefits a
range of tasks and models. However, we find that multilingual fine-tuning leads
to performance degradation on recent models UniXcoder and CodeT5.
  To alleviate the potentially catastrophic forgetting issue in multilingual
models, we fix all pre-trained model parameters, insert the parameter-efficient
structure adapter, and fine-tune it. Updating only 0.6\% of the overall
parameters compared to full-model fine-tuning for each programming language,
adapter tuning yields consistent improvements on code search and summarization
tasks, achieving state-of-the-art results. In addition, we experimentally show
its effectiveness in cross-lingual and low-resource scenarios. Multilingual
fine-tuning with 200 samples per programming language approaches the results
fine-tuned with the entire dataset on code summarization. Our experiments on
three probing tasks show that adapter tuning significantly outperforms
full-model fine-tuning and effectively overcomes catastrophic forgetting.",http://arxiv.org/abs/2303.15822v1
RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation,"repository-level code completion, iterative retrieval-generation, code retrieval, pre-trained language models, benchmark construction",2023-03,"This paper proposes RepoCoder, an iterative retrieval-generation framework that leverages code retrieval and pre-trained language models for repository-level code completion. It also introduces RepoEval, a new benchmark for evaluating code completion at different granularities using real-world repositories.","{'1. Introduction': 'Motivates repository-level code completion and limitations of existing approaches.', '2. Methodology': 'Describes the RepoCoder framework, including code retrieval and generation processes.', '3. Benchmark Construction': 'Details the creation of RepoEval benchmark from GitHub repositories.', '4. Experimental Setup': 'Outlines the baseline methods, implementation details, and experimental settings.', '5. Results and Analysis': ""Presents experimental results and analysis of RepoCoder's performance.""}","['Employs an iterative retrieval-generation pipeline combining a retriever and a pre-trained language model', 'Retrieves relevant code snippets from the repository using the unfinished code as initial query', 'Generates code completion using the retrieved snippets and unfinished code as context', 'Utilizes the generated completion to enhance the retrieval query in subsequent iterations']","For a code completion task involving the COLMAP API, RepoCoder initially retrieves relevant snippets based on the unfinished code. After generating an initial completion attempt, it uses this prediction to retrieve the actual API signature from the repository, enabling accurate completion.","The task of repository-level code completion is to continue writing the
unfinished code based on a broader context of the repository. While for
automated code completion tools, it is difficult to utilize the useful
information scattered in different files. We propose RepoCoder, a simple,
generic, and effective framework to address the challenge. It streamlines the
repository-level code completion process by incorporating a similarity-based
retriever and a pre-trained code language model in an iterative
retrieval-generation pipeline. RepoCoder makes effective utilization of
repository-level information for code completion and has the ability to
generate code at various levels of granularity. Moreover, we propose a new
benchmark RepoEval, which consists of the latest and high-quality real-world
repositories covering line, API invocation, and function body completion
scenarios. Experimental results indicate that RepoCoder significantly improves
the In-File completion baseline by over 10% in all settings and consistently
outperforms the vanilla retrieval-augmented code completion approach.
Furthermore, we validate the effectiveness of RepoCoder through comprehensive
analysis, providing valuable insights for future research. Our source code and
benchmark are publicly available:
https://github.com/microsoft/CodeT/tree/main/RepoCoder",http://arxiv.org/abs/2303.12570v3
Improved Tree Search for Automatic Program Synthesis,"program synthesis, Monte Carlo tree search, domain-specific languages, search algorithms, dataset pruning",2023-03,"The paper proposes an improved Monte Carlo tree search (MCTS) variant tailored for the task of automatic program synthesis from input-output examples. It introduces techniques like dataset pruning, encoding partial programs, and a shared visit count metric, leading to state-of-the-art results on two different domain-specific languages (DSLs).",1. Introduction 2. Related Work 3. Method 3.1 MCTS Variant 3.2 Pruning 3.3 Encoding Partial Programs 4. Experiments,"['Modified MCTS that starts search from root at each iteration instead of backtracking', 'Shared visit count that tracks number of times each program state (memory/environment) is visited', 'Preprocessing to prune training dataset by finding shorter equivalent programs', 'Encoding partial executed program as additional input to prediction network']","For the DeepCoder DSL with integer registers, the method achieved 85.2% accuracy on test programs of length 14, compared to 50.2% for the previous state-of-the-art. The gains came from pruning (replacing long training programs with shorter equivalents), encoding the partial program executed so far, and using the shared visit count MCTS instead of beam search.","In the task of automatic program synthesis, one obtains pairs of matching
inputs and outputs and generates a computer program, in a particular
domain-specific language (DSL), which given each sample input returns the
matching output. A key element is being able to perform an efficient search in
the space of valid programs. Here, we suggest a variant of MCTS that leads to
state of the art results on two vastly different DSLs. The exploration method
we propose includes multiple contributions: a modified visit count, a
preprocessing procedure for the training dataset, and encoding the part of the
program that was already executed.",http://arxiv.org/abs/2303.07166v1
"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval","execution-based evaluation, multilingual code benchmarking, code understanding, program synthesis, code translation",2023-03,"XCODEEVAL is a large-scale multilingual benchmark for evaluating language models on code-related tasks like understanding, generation, translation and retrieval. It features 25M coding examples across 11 programming languages with an execution-based evaluation using unit tests. The authors propose novel data splitting and selection methods to balance attribute distributions.",1. Introduction 2. XCODEEVAL Dataset 2.1 Data Creation 2.2 ExecEval Execution Engine 3. Experiments 4. Related Work 5. Conclusion 6. Ethics Statement 7. Dataset Documentation,"['Collected 25M coding samples from 7.5K problems on Codeforces across 11 programming languages', 'Proposed novel data splitting based on geometric mean to balance tag distributions in validation/test sets', 'Formulated data selection as a circulation problem with bounds to control sample sizes per problem/tag', 'Developed ExecEval, a distributed multilingual code execution engine supporting 44 compiler/interpreter versions', 'Adopted execution-based evaluation using comprehensive unit tests for relevant tasks']","For the 'Watermelon' problem, the benchmark provides a natural language description, sample inputs/outputs, relevant metadata like difficulty level and tags. It includes both correct and incorrect code solutions that are evaluated against hidden unit tests using ExecEval to determine if the code executes correctly on all test cases.","Recently, pre-trained large language models (LLMs) have shown impressive
abilities in generating codes from natural language descriptions, repairing
buggy codes, translating codes between languages, and retrieving relevant code
segments. However, the evaluation of these models has often been performed in a
scattered way on only one or two specific tasks, in a few languages, at a
partial granularity (e.g., function) level, and in many cases without proper
training data. Even more concerning is that in most cases the evaluation of
generated codes has been done in terms of mere lexical overlap with a reference
code rather than actual execution. We introduce xCodeEval, the largest
executable multilingual multitask benchmark to date consisting of $25$M
document-level coding examples ($16.5$B tokens) from about $7.5$K unique
problems covering up to $11$ programming languages with execution-level
parallelism. It features a total of $7$ tasks involving code understanding,
generation, translation and retrieval. xCodeEval adopts an execution-based
evaluation and offers a multilingual code execution engine, ExecEval that
supports unit test based execution in all the $11$ languages. To address the
challenge of balancing the distributions of text-code samples over multiple
attributes in validation/test sets, we propose a novel data splitting and a
data selection schema based on the geometric mean and graph-theoretic
principle. Our experiments with OpenAI's LLMs (zero-shot) and open-LLMs
(zero-shot and fine-tuned) on the tasks and languages demonstrate **xCodeEval**
to be quite challenging as per the current advancements in language models.",http://arxiv.org/abs/2303.03004v4
EvoPrompting: Language Models for Code-Level Neural Architecture Search,"evolutionary prompting, code-level neural architecture search, language models for code generation, graph neural network architecture search, algorithmic reasoning benchmark",2023-02,"The paper proposes EvoPrompting, a method that uses evolutionary search to curate and improve in-context prompts for language models to design effective neural network architectures. EvoPrompting enables language models to create novel architectures that outperform human-designed models on tasks like MNIST-1D and the CLRS algorithmic reasoning benchmark.","1. Introduction 2. Related Work 3. EvoPrompting Method 4. Experiments (4.1 MNIST-1D, 4.2 CLRS Algorithmic Reasoning) 5. Conclusion","['Use language model for adaptive mutation/crossover in evolutionary neural architecture search', 'Initialize population with seed architectures', 'Generate new architectures by prompting language model with few-shot examples from current population', 'Train and evaluate generated architectures, select top performers as new population', 'Iteratively prompt-tune language model on evaluated architectures to improve generations']","For MNIST-1D, EvoPrompting generated a convolutional architecture with 4800 parameters and 86.5% validation accuracy by prompting with: """"""
Metrics:
{'num_params': '4800', 'val_accuracy': '0.865'}
""""""
followed by a seed architecture example. This outperformed manually designed models.","Given the recent impressive accomplishments of language models (LMs) for code
generation, we explore the use of LMs as adaptive mutation and crossover
operators for an evolutionary neural architecture search (NAS) algorithm. While
NAS still proves too difficult a task for LMs to succeed at solely through
prompting, we find that the combination of evolutionary prompt engineering with
soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse
and high performing models. We first demonstrate that EvoPrompting is effective
on the computationally efficient MNIST-1D dataset, where EvoPrompting produces
convolutional architecture variants that outperform both those designed by
human experts and naive few-shot prompting in terms of accuracy and model size.
We then apply our method to searching for graph neural networks on the CLRS
Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel
architectures that outperform current state-of-the-art models on 21 out of 30
algorithmic reasoning tasks while maintaining similar model size. EvoPrompting
is successful at designing accurate and efficient neural network architectures
across a variety of machine learning tasks, while also being general enough for
easy adaptation to other tasks beyond neural network design.",http://arxiv.org/abs/2302.14838v3
Shrinking the Inductive Programming Search Space with Instruction Subsets,"instruction subset clustering, code analysis, search space reduction, inductive programming, program synthesis",2023-02,"The paper presents a novel approach to reducing the search space for inductive programming by deriving intersecting subsets of instructions from a large corpus of existing code. This allows the search to be partitioned and parallelized, making inductive programming more tractable for larger programs.",1. Introduction 2. Preliminaries 3. Approach 3.1 Objectives 3.2 Method 3.3 Clustering Algorithm 3.4 Amplification 4. Results 4.1 Input Data 4.2 Subset Sizes 4.3 Coverage 5. Discussion 6. Conclusions,"['- Analyzed 1000 largest Python repositories on GitHub containing 15M lines of code', '- Extracted instruction subsets from program units (functions, methods, etc.)', '- Filtered subsets to remove duplicates and proper subsets', '- Clustered remaining subsets into intersecting derived instruction subsets of specified sizes', '- Evaluated coverage of derived subsets on unseen code']","For a maximum subset size of 20 instructions, the algorithm derived around 1000 instruction subsets that covered 95% of unseen program units with 20 or fewer unique instructions. This represents orders of magnitude reduction in the search space compared to searching over all possible combinations of the full 200+ instruction set.","Inductive programming frequently relies on some form of search in order to
identify candidate solutions. However, the size of the search space limits the
use of inductive programming to the production of relatively small programs. If
we could somehow correctly predict the subset of instructions required for a
given problem then inductive programming would be more tractable. We will show
that this can be achieved in a high percentage of cases. This paper presents a
novel model of programming language instruction co-occurrence that was built to
support search space partitioning in the Zoea distributed inductive programming
system. This consists of a collection of intersecting instruction subsets
derived from a large sample of open source code. Using the approach different
parts of the search space can be explored in parallel. The number of subsets
required does not grow linearly with the quantity of code used to produce them
and a manageable number of subsets is sufficient to cover a high percentage of
unseen code. This approach also significantly reduces the overall size of the
search space - often by many orders of magnitude.",http://arxiv.org/abs/2302.05226v1
Searching Large Neighborhoods for Integer Linear Programs with Contrastive Learning,"contrastive learning, large neighborhood search, integer linear programming, graph attention networks, combinatorial optimization",2023-02,This paper proposes a novel approach called CL-LNS that uses contrastive learning to learn efficient and effective destroy heuristics for solving integer linear programs (ILPs) via large neighborhood search (LNS). CL-LNS outperforms state-of-the-art methods on several ILP benchmarks.,1. Introduction 2. Background 2.1 ILPs 2.2 LNS for ILP solving 2.3 LB Heuristic 3. Related Work 3.1 LNS for ILPs and Other COPs 3.2 Learning to Solve ILPs with BnB 3.3 Contrastive Learning for COPs 4. Contrastive Learning for LNS 4.1 Data Collection 4.2 Policy Network 4.3 Contrastive Loss 4.4 CL-LNS Algorithm,"['Use Local Branching (LB) heuristic to collect positive and negative solution samples', 'Positive samples are intermediate solutions close to LB optimal, negative by perturbing LB optimal', 'Learn a destroy heuristic policy with contrastive loss on positive/negative samples', 'Policy represented by graph attention network on bipartite ILP graph with rich features', 'Use learned policy in large neighborhood search (LNS) framework']","For an ILP instance, CL-LNS first solves the LB ILP to find the optimal solution x_t+1 that differs from incumbent x_t by at most k_t variables. Intermediate solutions x' close to x_t+1 in objective value are positive samples. Negative samples are generated by randomly perturbing the variables in x_t+1. The policy is trained to predict variable subsets similar to positive samples but dissimilar to negatives. In LNS, this policy predicts the subset of variables to reoptimize at each iteration.","Integer Linear Programs (ILPs) are powerful tools for modeling and solving a
large number of combinatorial optimization problems. Recently, it has been
shown that Large Neighborhood Search (LNS), as a heuristic algorithm, can find
high quality solutions to ILPs faster than Branch and Bound. However, how to
find the right heuristics to maximize the performance of LNS remains an open
problem. In this paper, we propose a novel approach, CL-LNS, that delivers
state-of-the-art anytime performance on several ILP benchmarks measured by
metrics including the primal gap, the primal integral, survival rates and the
best performing rate. Specifically, CL-LNS collects positive and negative
solution samples from an expert heuristic that is slow to compute and learns a
new one with a contrastive loss. We use graph attention networks and a richer
set of features to further improve its performance.",http://arxiv.org/abs/2302.01578v1
Generation-Augmented Query Expansion For Code Retrieval,"generation-augmented code retrieval, query expansion, code generation model, dual representation attention, multi-generated code snippets",2022-12,"This paper proposes a generation-augmented query expansion framework (GACR) for code retrieval, which leverages a code generation model to produce code snippets from natural language queries to enhance the retrieval process. The key novelty is bridging the gap between natural language and programming language representations.",1. Introduction 2. Background and Notation 3. Methodology 3.1 Query Augmentation with Single Generated Code 3.2 Augmentation with Multi-Generated Codes 3.3 Optimization and Inference 4. Experiments 4.1 Overall Performance 4.2 Ablation Study 5. Related Work 6. Conclusion,"['- Use a code generation model (Codex) to generate code snippets from natural language documentation queries', '- Concatenate the original query and generated code into a single sequence input to an encoder model', '- Use a dual representation attention mechanism to learn representations for NL query and generated PL code separately but with mutual attention', '- Optionally include multiple distinct generated code snippets as part of the expanded query input']","Given the documentation query 'Translate characters from lower to upper case for a particular column', the baseline GraphCodeBERT retrieves an incorrect code snippet, while GACR incorporating the generated code 'def toupper(self): return H2OFrame._expr(expr=ExprNode(""toupper"", self), cache=self._ex._cache)' is able to retrieve the ground truth relevant code.","Pre-trained language models have achieved promising success in code retrieval
tasks, where a natural language documentation query is given to find the most
relevant existing code snippet. However, existing models focus only on
optimizing the documentation code pairs by embedding them into latent space,
without the association of external knowledge. In this paper, we propose a
generation-augmented query expansion framework. Inspired by the human retrieval
process - sketching an answer before searching, in this work, we utilize the
powerful code generation model to benefit the code retrieval task.
Specifically, we demonstrate that rather than merely retrieving the target code
snippet according to the documentation query, it would be helpful to augment
the documentation query with its generation counterpart - generated code
snippets from the code generation model. To the best of our knowledge, this is
the first attempt that leverages the code generation model to enhance the code
retrieval task. We achieve new state-of-the-art results on the CodeSearchNet
benchmark and surpass the baselines significantly.",http://arxiv.org/abs/2212.10692v1
SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval,"in-context learning, FQN inference, frozen language model, CoPilot, prompt design",2022-12,"This paper studies the factual knowledge of fully qualified names (FQNs) stored in the frozen, giant pre-trained code model CoPilot. It proposes a lightweight in-context learning method to infer FQNs from partial code snippets without updating the model parameters.",1. Introduction 2. In-Context Learning for FQN Inference 2.1 Supervised Fine-Tuning vs. In-Context Learning 2.2 In-Context Learning Design 3. Experiment Setup 4. Experiment Results 4.1 RQ1: Best In-Context Learning Configuration 4.2 RQ2: Effect of Example Prompt Amount 4.3 RQ3: Effect of FQN Data Properties 4.4 RQ4: Comparison with Supervised Tuning 5. Discussion,"['Design in-context learning tasks for FQN inference on the frozen CoPilot model', 'Experiment with different configurations: zero/one/few-shot learning, code context, task description, prompt template, example prompt order, identifier format', ""Analyze CoPilot's capability on inferring FQNs with diverse data properties (length, usage frequency, name ambiguity)"", 'Compare with supervised fine-tuning approach on the same task']","For the code snippet:

java.util.List<String> results = new java.util.ArrayList<String>(); 
java.io.File[] files = new java.io.File("""").listFiles();
for (int j=0; j< files.length; j++){
    java.io.File path = files[j];
    java.lang.String s = """";
    while (br.ready()) {
        s += br.readLine().toLowerCase()+""\n"";
    }}

With the few-shot in-context learning prompt:

""Code context: [code snippet]
Task description: Parse simple name to fully qualified name
Example prompts: 
    The fully qualified name of 'List<>' is 'java.util.List<>'
    The fully qualified name of 'File[]' is 'java.io.File[]'
    The fully qualified name of 'String' is 'java.lang.String'
    The fully qualified name of 'File' is 'java.io.File'
To be completed:
    The fully qualified name of 'br' is ""

CoPilot generates: ""java.io.BufferedReader"", correctly inferring the FQN for the 'br' identifier based on the code context and example prompts provided.","Pre-trained giant code models (PCMs) start coming into the developers' daily
practices. Understanding what types of and how much software knowledge is
packed into PCMs is the foundation for incorporating PCMs into software
engineering (SE) tasks and fully releasing their potential. In this work, we
conduct the first systematic study on the SE factual knowledge in the
state-of-the-art PCM CoPilot, focusing on APIs' Fully Qualified Names (FQNs),
the fundamental knowledge for effective code analysis, search and reuse. Driven
by FQNs' data distribution properties, we design a novel lightweight in-context
learning on Copilot for FQN inference, which does not require code compilation
as traditional methods or gradient update by recent FQN prompt-tuning. We
systematically experiment with five in-context-learning design factors to
identify the best in-context learning configuration that developers can adopt
in practice. With this best configuration, we investigate the effects of amount
of example prompts and FQN data properties on Copilot's FQN inference
capability. Our results confirm that Copilot stores diverse FQN knowledge and
can be applied for the FQN inference due to its high inference accuracy and
non-reliance on code analysis. Based on our experience interacting with
Copilot, we discuss various opportunities to improve human-CoPilot interaction
in the FQN inference task.",http://arxiv.org/abs/2212.08221v1
You Don't Know Search: Helping Users Find Code by Automatically Evaluating Alternative Queries,"query interpretation ambiguity, code search usability, automated query evaluation, alternative query generation, user study",2022-12,"The paper presents an approach called Automated Query Evaluation (AQE) to help code search engine users find relevant results by automatically generating and evaluating alternative interpretations of potentially ambiguous queries. An A/B study with over 10,000 users showed a 22% increase in users clicking on search results when using AQE.",1. Introduction 2. Building a code search engine: query design preliminaries 3. Challenges in ambiguity: observing pitfalls users experience 4. Automated Query Evaluation (AQE) approach 5. Evaluation of AQE via user study 6. Related work 7. Conclusion,"['Identified common sources of ambiguity in how users interpret code search queries based on analysis of user feedback', 'Developed techniques to automatically generate alternative interpretations of queries in ambiguous cases', 'Dynamically evaluated generated queries and displayed results for interpretations that retrieve matches', 'Implemented the AQE approach in the Sourcegraph code search engine']","For the query 'func parse', AQE would generate alternatives like: 1) find 'func' AND 'parse' anywhere in files, and 2) find the exact string 'func parse' on a single line. It would run both interpretations and display the results for whichever retrieved matches.","Tens of thousands of engineers use Sourcegraph day-to-day to search for code
and rely on it to make progress on software development tasks. We face a key
challenge in designing a query language that accommodates the needs of a broad
spectrum of users. Our experience shows that users express different and often
contradictory preferences for how queries should be interpreted. These
preferences stem from users with differing usage contexts, technical
experience, and implicit expectations from using prior tools. At the same time,
designing a code search query language poses unique challenges because it
intersects traditional search engines and full-fledged programming languages.
For example, code search queries adopt certain syntactic conventions in the
interest of simplicity and terseness but invariably risk encoding implicit
semantics that are ambiguous at face-value (a single space in a query could
mean three or more semantically different things depending on surrounding
terms). Users often need to disambiguate intent with additional syntax so that
a query expresses what they actually want to search. This need to disambiguate
is one of the primary frustrations we've seen users experience with writing
search queries in the last three years. We share our observations that lead us
to a fresh perspective where code search behavior can straddle seemingly
ambiguous queries. We develop Automated Query Evaluation (AQE), a new technique
that automatically generates and adaptively runs alternative query
interpretations in frustration-prone conditions. We evaluate AQE with an A/B
test across more than 10,000 unique users on our publicly-available code search
instance. Our main result shows that relative to the control group, users are
on average 22% more likely to click on a search result at all on any given day
when AQE is active.",http://arxiv.org/abs/2212.03459v1
Domain-Independent Dynamic Programming: Generic State Space Search for Combinatorial Optimization,"dynamic programming, declarative modeling, combinatorial optimization, domain-independent solver, state space search",2022-11,"The paper proposes a new model-based paradigm called domain-independent dynamic programming (DIDP) for combinatorial optimization problems. It introduces Dynamic Programming Description Language (DyPDL), a formalism to declaratively model dynamic programming models, and CAASDy, a generic state space search solver for DyPDL models.",1. Introduction 2. Background on Dynamic Programming 3. DyPDL: Modeling Formalism 4. YAML-DyPDL Implementation 5. CAASDy: State Space Search Solver 6. Experimental Evaluation 7. Conclusion,"['Defines DyPDL, a declarative modeling language for specifying dynamic programming models', 'Introduces modeling constructs like state variables, transitions, base cases, state constraints, etc.', 'Proposes YAML-DyPDL, an implementation of DyPDL based on the YAML data format', 'Develops CAASDy, a cost-algebraic A* state space search solver for solving DyPDL models']","The paper provides a detailed example of modeling the Traveling Salesperson Problem with Time Windows (TSPTW) using DyPDL. It shows how to represent the state variables, transitions, constraints, and objective function in the DyPDL syntax.","For combinatorial optimization problems, model-based approaches such as
mixed-integer programming (MIP) and constraint programming (CP) aim to decouple
modeling and solving a problem: the 'holy grail' of declarative problem
solving. We propose domain-independent dynamic programming (DIDP), a new
model-based paradigm based on dynamic programming (DP). While DP is not new, it
has typically been implemented as a problem-specific method. We propose Dynamic
Programming Description Language (DyPDL), a formalism to define DP models, and
develop Cost-Algebraic A* Solver for DyPDL (CAASDy), a generic solver for DyPDL
using state space search. We formalize existing problem-specific DP and state
space search methods for combinatorial optimization problems as DP models in
DyPDL. Using CAASDy and commercial MIP and CP solvers, we experimentally
compare the DP models with existing MIP and CP models, showing that, despite
its nascent nature, CAASDy outperforms MIP and CP on a number of common problem
classes.",http://arxiv.org/abs/2211.14409v2
Phenotype Search Trajectory Networks for Linear Genetic Programming,"Boolean function evolution, linear genetic programming, genotype-phenotype mapping, search trajectory networks, Kolmogorov complexity",2022-11,This paper proposes using search trajectory network visualizations combined with quantitative metrics like genotype redundancy and Kolmogorov complexity to analyze the search dynamics and genotype-phenotype mapping in a linear genetic programming system for evolving Boolean functions. The results provide insights into how phenotype complexity impacts redundancy and evolutionary trajectories.,"1. Introduction 2. The LGP System 2.1 Boolean LGP algorithm 2.2 Genotype, phenotype, and fitness 3. Kolmogorov Complexity 4. Sampling and Metrics Estimation 5. Search Trajectory Networks 5.1 General definitions 5.2 The proposed STN models","['Used a linear genetic programming (LGP) system to evolve 3-input, 1-output Boolean functions', 'Defined genotypes as LGP programs and phenotypes as the Boolean functions they represent', 'Estimated phenotype redundancy by sampling genotypes and mapping to phenotypes', 'Defined Kolmogorov complexity of a phenotype as the minimal effective length of its underlying LGP programs', 'Performed adaptive walks accepting only neutral or improving mutations to generate search trajectories', 'Visualized search trajectories using 3 search trajectory network (STN) models at genotype, genotype-phenotype, and phenotype levels']","For the target Boolean function (phenotype) 20 with medium redundancy and complexity, the genotype-phenotype STN model shows that the search first discovers the highly redundant phenotype 0 (FALSE), which acts as a stepping stone, before transitioning through phenotypes 16, 84, 171 to eventually reach the target 20. Less complex phenotypes are over-represented by genotypes and easier to find, facilitating search progression.","Genotype-to-phenotype mappings translate genotypic variations such as
mutations into phenotypic changes. Neutrality is the observation that some
mutations do not lead to phenotypic changes. Studying the search trajectories
in genotypic and phenotypic spaces, especially through neutral mutations, helps
us to better understand the progression of evolution and its algorithmic
behaviour. In this study, we visualise the search trajectories of a genetic
programming system as graph-based models, where nodes are genotypes/phenotypes
and edges represent their mutational transitions. We also quantitatively
measure the characteristics of phenotypes including their genotypic abundance
(the requirement for neutrality) and Kolmogorov complexity. We connect these
quantified metrics with search trajectory visualisations, and find that more
complex phenotypes are under-represented by fewer genotypes and are harder for
evolution to discover. Less complex phenotypes, on the other hand, are
over-represented by genotypes, are easier to find, and frequently serve as
stepping-stones for evolution.",http://arxiv.org/abs/2211.08516v2
ATM: Black-box Test Case Minimization based on Test Code Similarity and Evolutionary Search,"test case minimization, abstract syntax tree, tree-based similarity, genetic algorithm, black-box testing",2022-10,"The paper proposes ATM, a black-box test case minimization technique based on abstract syntax tree similarity and evolutionary search. ATM achieves higher fault detection rates than existing techniques while running within practical time constraints.","1. Introduction, 2. ATM Technique (preprocessing, AST transformation, similarity measures, search algorithms), 3. Experiment (design, results, discussion), 4. Threats to Validity, 5. Related Work, 6. Conclusion","['Preprocess test case code by removing comments, logging, assertions etc. and normalizing variable names', 'Transform preprocessed test cases into Abstract Syntax Trees (ASTs)', 'Measure similarity between test case ASTs using four measures: top-down, bottom-up, combined, tree edit distance', 'Use genetic algorithms (GA and NSGA-II) to minimize test suite based on AST similarities as fitness functions']","For a 50% minimization budget on 16 Java projects, ATM achieved 0.82 average fault detection rate and ran in 1.1-4.3 hours per project version on average. The combined similarity with GA performed best with 0.80 fault detection and 1.2 hour runtime.","Executing large test suites is time and resource consuming, sometimes
impossible, and such test suites typically contain many redundant test cases.
Hence, test case minimization is used to remove redundant test cases that are
unlikely to detect new faults. However, most test case (suite) minimization
techniques rely on code coverage (white-box), model-based features, or
requirements specifications, which are not always accessible by test engineers.
Recently, a set of novel techniques was proposed, called FAST-R, relying solely
on test case code for test case minimization, which appeared to be much more
efficient than white-box techniques. However, it achieved a comparable low
fault detection capability for Java projects, making its application
challenging in practice. This paper proposes ATM (AST-based Test case
Minimizer), a similarity-based, search-based test case minimization technique,
taking a specific budget as input, that also relies exclusively on the source
code of test cases but attempts to achieve higher fault detection through
finer-grained similarity analysis and a dedicated search algorithm. ATM
transforms test case code into Abstract Syntax Trees (AST) and relies on four
tree-based similarity measures to apply evolutionary search, specifically
genetic algorithms, to minimize test cases. We evaluated the effectiveness and
efficiency of ATM on a large dataset of 16 Java projects with 661 faulty
versions using three budgets ranging from 25% to 75% of test suites. ATM
achieved significantly higher fault detection rates (0.82 on average), compared
to FAST-R (0.61 on average) and random minimization (0.52 on average), when
running only 50% of the test cases, within practically acceptable time (1.1-4.3
hours, on average), given that minimization is only occasionally applied when
many new test cases are created (major releases). Results achieved for other
budgets were consistent.",http://arxiv.org/abs/2210.16269v2
I Know What You Are Searching For: Code Snippet Recommendation from Stack Overflow Posts,"query reformulation, code snippet recommendation, duplicate question retrieval, pairwise learning to rank, BERT for code search",2022-10,"This paper presents Que2Code, a query-driven code recommendation tool that identifies the best code snippets from Stack Overflow posts for a given user query. It uses query reformulation to retrieve semantically equivalent questions, and a pairwise learning to rank model with BERT to recommend the most relevant code snippets.","1. Introduction, 2. Motivation, 3. Approach (3.1 Query Reformulation, 3.2 Code Snippet Recommendation), 4. Experiments on Automatic Evaluation, 5. Human Evaluation, 6. Threats to Validity, 7. Related Work, 8. Conclusion","['Use query reformulation to generate paraphrased questions and retrieve semantically equivalent Stack Overflow questions', 'Extract code snippets from retrieved questions to build a candidate pool', 'Train a pairwise learning to rank neural network on positive and negative code snippet pairs', 'Rank and recommend the top code snippet for the input query using the trained model']","For the query 'difference in division of -a//b and a//b in python 2.7', the tool retrieves the duplicate question 'Floor division with negative number' by query reformulation. It then extracts and ranks code snippets from the answers, recommending the top snippet explaining Python's floor division behavior with negative numbers.","Stack Overflow has been heavily used by software developers to seek
programming-related information. More and more developers use Community
Question and Answer forums, such as Stack Overflow, to search for code examples
of how to accomplish a certain coding task. This is often considered to be more
efficient than working from source documentation, tutorials or full worked
examples. However, due to the complexity of these online Question and Answer
forums and the very large volume of information they contain, developers can be
overwhelmed by the sheer volume of available information. This makes it hard to
find and/or even be aware of the most relevant code examples to meet their
needs. To alleviate this issue, in this work we present a query-driven code
recommendation tool, named Que2Code, that identifies the best code snippets for
a user query from Stack Overflow posts. Our approach has two main stages: (i)
semantically-equivalent question retrieval and (ii) best code snippet
recommendation. To evaluate the performance of our proposed model, we conduct a
large scale experiment to evaluate the effectiveness of the
semantically-equivalent question retrieval task and best code snippet
recommendation task separately on Python and Java datasets in Stack Overflow.
We also perform a human study to measure how real-world developers perceive the
results generated by our model. Both the automatic and human evaluation results
demonstrate the promising performance of our model, and we have released our
code and data to assist other researchers.",http://arxiv.org/abs/2210.15845v1
Exploring Representation-Level Augmentation for Code Search,"representation-level augmentation, code search, contrastive learning, InfoNCE loss, mutual information bounds",2022-10,This paper explores representation-level augmentation methods for improving code search models based on contrastive learning. It proposes a general format unifying existing methods and introduces three new augmentation techniques. Theoretical analysis shows that representation-level augmentation leads to tighter mutual information bounds between positive pairs when optimizing the InfoNCE loss.,1. Introduction 2. Related Work 2.1 Code Search 2.2 Data Augmentation 3. Approach 3.1 General Format of Representation-Level Augmentation 3.2 New Augmentation Methods 3.3 Contrastive Learning with Representation-Level Augmentation 4. Theoretical Analysis 5. Experimental Setup 5.1 Datasets 5.2 Baselines 5.3 Implementation Details 6. Results 6.1 Main Results 6.2 Ablation Study 6.3 Qualitative Analysis 6.4 Effect of Similarity Measurement 7. Generalization to Other Tasks 8. Conclusion,"['- Unify existing representation-level augmentation methods (linear interpolation, stochastic perturbation) into a general format', '- Propose three new augmentation methods: linear extrapolation, binary interpolation, Gaussian scaling', '- Apply representation-level augmentation during contrastive learning with InfoNCE loss for code search', '- Theoretically analyze how representation-level augmentation improves mutual information lower bounds between positive pairs']","For linear interpolation augmentation on a code snippet c with representation h, the augmented representation is: h+ = h + (1-alpha) * h_j, where h_j is another code's representation and alpha is a random coefficient sampled from a distribution like Uniform(beta, 1.0) to ensure the augmented data is semantically similar.","Code search, which aims at retrieving the most relevant code fragment for a
given natural language query, is a common activity in software development
practice. Recently, contrastive learning is widely used in code search
research, where many data augmentation approaches for source code (e.g.,
semantic-preserving program transformation) are proposed to learn better
representations. However, these augmentations are at the raw-data level, which
requires additional code analysis in the preprocessing stage and additional
training costs in the training stage. In this paper, we explore augmentation
methods that augment data (both code and query) at representation level which
does not require additional data processing and training, and based on this we
propose a general format of representation-level augmentation that unifies
existing methods. Then, we propose three new augmentation methods (linear
extrapolation, binary interpolation, and Gaussian scaling) based on the general
format. Furthermore, we theoretically analyze the advantages of the proposed
augmentation methods over traditional contrastive learning methods on code
search. We experimentally evaluate the proposed representation-level
augmentation methods with state-of-the-art code search models on a large-scale
public dataset consisting of six programming languages. The experimental
results show that our approach can consistently boost the performance of the
studied code search models. Our source code is available at
https://github.com/Alex-HaochenLi/RACS.",http://arxiv.org/abs/2210.12285v1
CodeDSI: Differentiable Code Search,"differentiable code search, neural code retrieval, docid representation, semantic clustering, tokenization impact",2022-10,"This paper proposes CodeDSI, an end-to-end approach to code search that directly maps natural language queries to relevant code samples using a sequence-to-sequence model. CodeDSI outperforms conventional baselines by 2-6% by effectively handling the query-code distribution mismatch.","{'1. Introduction': 'Motivates code search as an alternative to code generation, highlights challenges, and outlines the proposed CodeDSI approach.', '2. Related Work': 'Discusses prior work in information retrieval and NLP on code.', '3. Differentiable Code Search': 'Explains the indexing and retrieval tasks in CodeDSI and different docid representation strategies.', '4. Experiments': 'Describes datasets, baselines, implementation details.', '5. Results and Analysis': ""Presents CodeDSI's performance compared to baselines across varying dataset sizes.""}","['Uses a sequence-to-sequence model to directly map queries to document identifiers (docids) of relevant code samples', 'Investigates different docid representation strategies: direct, clustered (using CodeBERT embeddings + K-means), and semantic clustering', 'Analyzes impact of tokenization (numerical vs alphabetical docids) on model performance']","For the query 'List of all even numbers from 1 to n', CodeDSI maps it to the docid of a code sample containing the function 'def evenNumbers(n): return [i for i in range(1,n,2)]'","Reimplementing solutions to previously solved software engineering problems
is not only inefficient but also introduces inadequate and error-prone code.
Many existing methods achieve impressive performance on this issue by using
autoregressive text-generation models trained on code. However, these methods
are not without their flaws. The generated code from these models can be buggy,
lack documentation, and introduce vulnerabilities that may go unnoticed by
developers. An alternative to code generation -- neural code search -- is a
field of machine learning where a model takes natural language queries as input
and, in turn, relevant code samples from a database are returned. Due to the
nature of this pre-existing database, code samples can be documented, tested,
licensed, and checked for vulnerabilities before being used by developers in
production. In this work, we present CodeDSI, an end-to-end unified approach to
code search. CodeDSI is trained to directly map natural language queries to
their respective code samples, which can be retrieved later. In an effort to
improve the performance of code search, we have investigated docid
representation strategies, impact of tokenization on docid structure, and
dataset sizes on overall code search performance. Our results demonstrate
CodeDSI strong performance, exceeding conventional robust baselines by 2-6%
across varying dataset sizes.",http://arxiv.org/abs/2210.00328v1
Assisted Specification of Code Using Search,"subsystem specification, code search, abstraction, test case generation, code adaptation",2022-09,"The paper describes ASCUS, a framework that assists developers in creating checkable specifications for software subsystems by mining existing code repositories and generating abstractions, test cases, and adapted code matching the specifications.",1. Motivation 2. Related Work 3. Overview of ASCUS 4. Searching for Subsystems 5. Creating Abstractions 6. Matching Abstractions to Retrieved Subsystems 7. Generating Test Cases 8. Evaluation,"[""Searches code repositories for relevant subsystems based on developer's keywords"", 'Creates abstractions of retrieved subsystems by identifying key classes, methods, fields', 'Allows developer to edit/refine the abstraction', 'Searches again for subsystems matching the edited abstraction', 'Generates mappings between retrieved code and abstraction', 'Mines test cases from original projects and adapts them to the abstraction']","For specifying an embedded HTTP server, ASCUS searched GitHub using keywords like 'lightweight', 'http', 'server'. It retrieved candidate subsystems, abstracted them to a simplified Java interface, allowed edits, searched again, mapped retrieved code to the edited abstraction, and generated relevant test cases.","We describe an intelligent assistant based on mining existing software
repositories to help the developer interactively create checkable
specifications of code. To be most useful we apply this at the subsystem level,
that is chunks of code of 1000-10000 lines that can be standalone or integrated
into an existing application to provide additional functionality or
capabilities. The resultant specifications include both a syntactic description
of what should be written and a semantic specification of what it should do,
initially in the form of test cases. The generated specification is designed to
be used for automatic code generation using various technologies that have been
proposed including machine learning, code search, and program synthesis. Our
research goal is to enable these technologies to be used effectively for
creating subsystems without requiring the developer to write detailed
specifications from scratch.",http://arxiv.org/abs/2209.09804v1
Revisiting Code Search in a Two-Stage Paradigm,"two-stage code search, recall and reranking, multi-channel recall, cross-encoder reranking, diverse candidate set",2022-08,"This paper proposes TOSS, a two-stage code search framework that combines the advantages of different methods. It first uses fast IR-based and bi-encoder models to efficiently recall a diverse set of top-K code candidates, and then employs fine-grained cross-encoders for re-ranking. Extensive experiments show TOSS achieves state-of-the-art accuracy while being efficient.",1. Introduction 2. Related Work 3. Framework 3.1 Preliminaries 3.2 Two-stage paradigm 4. Experimental Design 4.1 Datasets 4.2 Baselines 4.3 Evaluation Metrics 5. Experimental Results 5.1 Effect of Pre-processing 5.2 Overall Performance 5.3 Efficiency Analysis 5.4 Recall Analysis 5.5 Generalization to Other Languages 5.6 Comparison with Fusion Methods,"['Uses a two-stage paradigm: recall followed by reranking', 'Recall stage uses multiple fast IR and bi-encoder models to retrieve top-K candidates', 'Combines candidates from multiple recall models to improve diversity', 'Reranking stage uses accurate but slow cross-encoder models on top-K candidates']","For the query 'Return the bubble sort', the first stage may use BM25, GraphCodeBERT, and CodeBERT-bi to retrieve top-100 candidates each. These are combined into a diverse set of ~200 candidates. The second stage uses a large cross-encoder like CodeBERT to re-rank just these 200 candidates, outputting the final ranked list.","With a good code search engine, developers can reuse existing code snippets
and accelerate software development process. Current code search methods can be
divided into two categories: traditional information retrieval (IR) based and
deep learning (DL) based approaches. DL-based approaches include the
cross-encoder paradigm and the bi-encoder paradigm. However, both approaches
have certain limitations. The inference of IR-based and bi-encoder models are
fast, however, they are not accurate enough; while cross-encoder models can
achieve higher search accuracy but consume more time. In this work, we propose
TOSS, a two-stage fusion code search framework that can combine the advantages
of different code search methods. TOSS first uses IR-based and bi-encoder
models to efficiently recall a small number of top-k code candidates, and then
uses fine-grained cross-encoders for finer ranking. Furthermore, we conduct
extensive experiments on different code candidate volumes and multiple
programming languages to verify the effectiveness of TOSS. We also compare TOSS
with six data fusion methods. Experimental results show that TOSS is not only
efficient, but also achieves state-of-the-art accuracy with an overall mean
reciprocal ranking (MRR) score of 0.763, compared to the best baseline result
on the CodeSearchNet benchmark of 0.713. Our source code and experimental data
are available at: https://github.com/fly-dragon211/TOSS.",http://arxiv.org/abs/2208.11274v3
"Tackling Long Code Search with Splitting, Encoding, and Aggregating","long code representation, code splitting, code aggregation, transformer for code, code search",2022-08,"The paper proposes a new method called SEA (Split, Encode and Aggregate) to handle long code snippets for code search tasks using transformer-based models. By splitting long code into blocks, encoding each block, and aggregating the block representations, SEA can effectively represent and search long code without modifying the transformer architecture.","1. Introduction - Motivation and overview 2. Related Work - Code search methods, neural code representation, transformers for long text 3. Motivation - Empirical analysis of long code problem 4. SEA Method - Code splitting, encoding blocks, aggregation techniques 5. Experiments - Setup, results comparing to baselines 6. Conclusion","['Split long code into pieces using AST-based splitting', 'Generate partially overlapping code blocks using sliding window', 'Encode each code block using transformer encoder like GraphCodeBERT', 'Aggregate block embeddings using attention-based weighted sum']","For the code snippet in Figure 1, GraphCodeBERT truncates at 256 tokens missing key tokens like 'Tensor' and 'patches'. SEA splits this into blocks, encodes each block, and aggregates the representations to capture the full semantics, improving the ranking to the top result.","Code search with natural language helps us reuse existing code snippets.
Thanks to the Transformer-based pretraining models, the performance of code
search has been improved significantly. However, due to the quadratic
complexity of multi-head self-attention, there is a limit on the input token
length. For efficient training on standard GPUs like V100, existing pretrained
code models, including GraphCodeBERT, CodeBERT, RoBERTa (code), take the first
256 tokens by default, which makes them unable to represent the complete
information of long code that is greater than 256 tokens. To tackle the long
code problem, we propose a new baseline SEA (Split, Encode and Aggregate),
which splits long code into code blocks, encodes these blocks into embeddings,
and aggregates them to obtain a comprehensive long code representation. With
SEA, we could directly use Transformer-based pretraining models to model long
code without changing their internal structure and re-pretraining. We also
compare SEA with sparse Trasnformer methods. With GraphCodeBERT as the encoder,
SEA achieves an overall mean reciprocal ranking score of 0.785, which is 10.1%
higher than GraphCodeBERT on the CodeSearchNet benchmark, justifying SEA as a
strong baseline for long code search. Our source code and experimental data are
available at: https://github.com/fly-dragon211/SEA.",http://arxiv.org/abs/2208.11271v3
CSSAM:Code Search via Attention Matching of Code Semantics and Structures,"code semantics matching, code structure representation, graph attention network, residual interaction, code search",2022-08,"This paper proposes CSSAM, a deep learning model for code search that effectively matches code semantics and structures by introducing semantic and structural matching mechanisms. It outperforms existing baselines on public datasets.",1. Introduction 2. Related Work 2.1 Code Representation 2.2 Text Matching 2.3 Attention Mechanism 2.4 Graph Neural Networks 3. Proposed Approach 3.1 Embedding 3.2 Context Embedding 3.3 Attention Fusion 3.4 Ranking Learning 4. Experiment Setup 5. Results 6. Discussion 7. Conclusion,"['- Introduces a residual interaction matching module to preserve code/text semantics', '- Proposes Code Semantic Representation Graph (CSRG) to jointly represent AST nodes and data flow', '- Uses graph attention network to learn CSRG representations', '- Employs attention fusion to combine semantic and structural features', '- Trains a ranking model to match code and text representations']","For the code snippet 'public static void readFileByLine(String path)' with docstring 'open a file and print each line', the model would: 1) Embed the code tokens and AST into vectors, 2) Learn contextualized token/CSRG representations, 3) Attend and fuse semantic/structural features, 4) Match the fused code vector with the docstring vector to retrieve relevant code.","Despite the continuous efforts in improving both the effectiveness and
efficiency of code search, two issues remained unsolved. First, programming
languages have inherent strong structural linkages, and feature mining of code
as text form would omit the structural information contained inside it. Second,
there is a potential semantic relationship between code and query, it is
challenging to align code and text across sequences so that vectors are
spatially consistent during similarity matching. To tackle both issues, in this
paper, a code search model named CSSAM (Code Semantics and Structures Attention
Matching) is proposed. By introducing semantic and structural matching
mechanisms, CSSAM effectively extracts and fuses multidimensional code
features. Specifically, the cross and residual layer was developed to
facilitate high-latitude spatial alignment of code and query at the token
level. By leveraging the residual interaction, a matching module is designed to
preserve more code semantics and descriptive features, that enhances the
adhesion between the code and its corresponding query text. Besides, to improve
the model's comprehension of the code's inherent structure, a code
representation structure named CSRG (Code Semantic Representation Graph) is
proposed for jointly representing abstract syntax tree nodes and the data flow
of the codes. According to the experimental results on two publicly available
datasets containing 540k and 330k code segments, CSSAM significantly
outperforms the baselines in terms of achieving the highest SR@1/5/10, MRR, and
NDCG@50 on both datasets respectively. Moreover, the ablation study is
conducted to quantitatively measure the impact of each key component of CSSAM
on the efficiency and effectiveness of code search, which offers the insights
into the improvement of advanced code search solutions.",http://arxiv.org/abs/2208.03922v1
Study of Encoder-Decoder Architectures for Code-Mix Search Query Translation,"code-mix query translation, transformer models, data augmentation, knowledge distillation, weight quantization",2022-08,The paper proposes a transformer-based approach using pre-trained encoder-decoder models like T5 and BART for translating code-mixed Hinglish (Hindi-English) search queries to English. It demonstrates the effectiveness of data augmentation techniques and model compression methods like knowledge distillation and weight quantization for this task.,1. Introduction 2. Related Work 3. Exploring Pre-trained Checkpoints for Query Translation 3.1 Data Preparation 3.2 Test Data 3.3 Pre-trained Models 3.4 Data Augmentation 3.5 Results 4. Hinglish Query Detection 5. Training Corpus Preparation 6. Model Compression 6.1 Knowledge Distillation 6.2 Weight Quantization 7. Experimental Evaluation 8. Conclusion and Future Work,"['Used pre-trained encoder-decoder models like T5, BART, Bert2Bert for translation', 'Trained on parallel corpus created using translation/transliteration APIs', 'Employed data augmentation techniques like AutoEncoder, DropChar, Masking', 'Developed ML model for detecting Hinglish queries', 'Used knowledge distillation to train smaller student model from larger teacher', 'Applied weight quantization for further model compression']","For the query 'juta bina dori wala' (shoes without laces), the model can translate it correctly to 'shoe without lace' instead of a literal word-by-word translation, addressing the articulation gap issue.","With the broad reach of the internet and smartphones, e-commerce platforms
have an increasingly diversified user base. Since native language users are not
conversant in English, their preferred browsing mode is their regional language
or a combination of their regional language and English. From our recent study
on the query data, we noticed that many of the queries we receive are code-mix,
specifically Hinglish i.e. queries with one or more Hindi words written in
English (Latin) script. We propose a transformer-based approach for code-mix
query translation to enable users to search with these queries. We demonstrate
the effectiveness of pre-trained encoder-decoder models trained on a large
corpus of the unlabeled English text for this task. Using generic domain
translation models, we created a pseudo-labelled dataset for training the model
on the search queries and verified the effectiveness of various data
augmentation techniques. Further, to reduce the latency of the model, we use
knowledge distillation and weight quantization. Effectiveness of the proposed
method has been validated through experimental evaluations and A/B testing. The
model is currently live on Flipkart app and website, serving millions of
queries.",http://arxiv.org/abs/2208.03713v1
CausNet : Generational orderings based search for optimal Bayesian networks via dynamic programming with parent set constraints,"generational orderings, dynamic programming, parent set constraints, Bayesian network structure learning, optimal network search",2022-07,"The paper proposes a novel dynamic programming algorithm called CausNet for finding optimal Bayesian network structures from data. It uses generational orderings based search with parent set constraints to drastically reduce the search space, enabling efficient learning of optimal networks on high-dimensional data.",1. Introduction 2. Background on Bayesian networks 3. CausNet algorithm 3.1 Parent set identification 3.2 Phenotype driven search 3.3 Generational orderings based search 4. Theoretical analysis 5. Experimental results 5.1 Synthetic data 5.2 Real gene expression data,"['- Use dynamic programming to efficiently search network space', '- Identify possible parent sets using marginal associations to reduce dimensionality', '- Option for phenotype-driven search focusing on associations with outcome', ""- Restrict search to 'generational orderings' respecting parent sets"", '- Compute local scores and best parents using scoring functions like BIC, BGe', '- Find optimal network by tracing back best sinks in generational ordering']","On an ovarian cancer gene expression dataset with 513 genes and a survival outcome, CausNet found an optimal 6-gene network describing the disease pathway in just a few minutes. This showcases its ability to learn interpretable networks from high-dimensional data efficiently.","Finding a globally optimal Bayesian Network using exhaustive search is a
problem with super-exponential complexity, which severely restricts the number
of variables that it can work for. We implement a dynamic programming based
algorithm with built-in dimensionality reduction and parent set identification.
This reduces the search space drastically and can be applied to
large-dimensional data. We use what we call generational orderings based search
for optimal networks, which is a novel way to efficiently search the space of
possible networks given the possible parent sets. The algorithm supports both
continuous and categorical data, and categorical as well as survival outcomes.
We demonstrate the efficacy of our algorithm on both synthetic and real data.
In simulations, our algorithm performs better than three state-of-art
algorithms that are currently used extensively. We then apply it to an Ovarian
Cancer gene expression dataset with 513 genes and a survival outcome. Our
algorithm is able to find an optimal network describing the disease pathway
consisting of 6 genes leading to the outcome node in a few minutes on a basic
computer. Our generational orderings based search for optimal networks, is both
efficient and highly scalable approach to finding optimal Bayesian Networks,
that can be applied to 1000s of variables. Using specifiable parameters -
correlation, FDR cutoffs, and in-degree - one can increase or decrease the
number of nodes and density of the networks. Availability of two scoring
option-BIC and Bge-and implementation of survival outcomes and mixed data types
makes our algorithm very suitable for many types of high dimensional biomedical
data to find disease pathways.",http://arxiv.org/abs/2207.08365v1
DocPrompting: Generating Code by Retrieving the Docs,"documentation retrieval, natural language to code generation, retrieve-then-generate, unseen library generalization, documentation-grounded generation",2022-07,"The paper introduces DocPrompting, a novel approach that leverages code documentation to improve natural language to code generation models. By retrieving relevant documentation and using it as additional context, DocPrompting allows models to generalize to unseen libraries and functions not present in the training data.","1. Introduction - Motivation and background 2. Code Generation by Reading the Docs - Formulation of the retrieve-then-generate approach 3. Practical Instantiations - Retriever (sparse, dense) and generator model details 4. Experimental Setup - Datasets (new tldr benchmark, re-split CoNaLa) and evaluation metrics 5. Results - Empirical results on shell scripting and Python programming tasks 6. Analysis - Ablation studies and qualitative examples","['- Retrieve relevant code documentation from a pool using sparse (BM25) or dense retrievers trained on oracle docs', '- Concatenate retrieved docs with the natural language intent to form a prompt', '- Use the prompt to condition a generator model (T5, CodeT5, GPT-Neo, Codex) to generate code']","For the NL intent 'Generate HTML with python syntax highlighting for ""print('reading docs')""', DocPrompting: 1) Retrieved docs on the Pygments syntax highlighting library, PythonLexer, and HtmlFormatter 2) Concatenated the docs with the intent as a prompt 3) Generated: 
	from pygments import * 
	code = 'print(""reading docs"")' 
	s = highlight(code, PythonLexer(), HtmlFormatter())","Publicly available source-code libraries are continuously growing and
changing. This makes it impossible for models of code to keep current with all
available APIs by simply training these models on existing code repositories.
Thus, existing models inherently cannot generalize to using unseen functions
and libraries, because these would never appear in the training data. In
contrast, when human programmers use functions and libraries for the first
time, they frequently refer to textual resources such as code manuals and
documentation, to explore and understand the available functionality. Inspired
by this observation, we introduce DocPrompting: a natural-language-to-code
generation approach that explicitly leverages documentation by (1) retrieving
the relevant documentation pieces given an NL intent, and (2) generating code
based on the NL intent and the retrieved documentation. DocPrompting is
general: it can be applied to any programming language and is agnostic to the
underlying neural model. We demonstrate that DocPrompting consistently improves
NL-to-code models: DocPrompting improves strong base models such as CodeT5 by
2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in
execution-based evaluation on the popular Python CoNaLa benchmark; on a new
Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to
absolute 6.9% exact match.",http://arxiv.org/abs/2207.05987v3
Computing Programs for Generalized Planning as Heuristic Search,"generalized planning, heuristic search, planning programs, random-access machine, flags register",2022-05,"This paper presents the first native heuristic search approach for generalized planning (GP), adapting the planning as heuristic search paradigm to GP. It defines a program-based solution space using a random-access machine model and introduces the BFGP algorithm to perform best-first search over this space guided by novel evaluation and heuristic functions.","['1. Introduction', '2. Classical Planning', '3. Generalized Planning as Heuristic Search', '3.1 Classical Planning with a RAM', '3.2 Generalized Planning with a RAM', '3.3 The BFGP algorithm for Generalized Planning']","['- Extend classical planning model with a set of pointers over state variables and primitive operations (increment, decrement, compare, set)', '- Use random-access machine and flags register to define a program-based solution space independent of instance sizes', '- Define generalized planning problems as sets of classical instances sharing pointers, actions and flags', '- Represent solutions as planning programs with branching and looping', '- Implement BFGP best-first search algorithm to explore program space', '- Define evaluation functions based on program structure and performance on instances']","The paper provides an example of reversing lists of varying lengths using a 6-line planning program with 2 pointers. It swaps elements, moves pointers, and uses flags to detect when the list is fully reversed.","Although heuristic search is one of the most successful approaches to
classical planning, this planning paradigm does not apply straightforwardly to
Generalized Planning (GP). This paper adapts the planning as heuristic search
paradigm to the particularities of GP, and presents the first native heuristic
search approach to GP. First, the paper defines a program-based solution space
for GP that is independent of the number of planning instances in a GP problem,
and the size of these instances. Second, the paper defines the BFGP algorithm
for GP, that implements a best-first search in our program-based solution
space, and that is guided by different evaluation and heuristic functions.",http://arxiv.org/abs/2205.06259v1
QuSBT: Search-Based Testing of Quantum Programs,"quantum program testing, search-based testing, genetic algorithms, Qiskit integration, probabilistic failure detection",2022-04,QuSBT is a tool that uses a genetic algorithm to automatically generate test suites for quantum programs with the goal of maximizing the number of failing test cases. It integrates with IBM's Qiskit for simulating quantum programs and employs statistical tests to detect probabilistic failures.,"1. Introduction 2. Background 3. Preliminaries 4. QuSBT Tool Architecture and Methodology 4.1 Input and Configuration 4.2 Process of Test Generation, Execution, and Assessment 5. Validation 6. Conclusion and Future Work","['- Encodes test suite as an individual in the genetic algorithm with integer variables representing test inputs', '- Defines a fitness function to maximize the number of failing tests', '- Executes quantum program with test inputs using Qiskit QasmSimulator', ""- Assesses test results for two types of failures: unexpected outputs and wrong output distributions using Pearson's chi-square test"", '- Provides test suite output as list of inputs/outputs and as unit tests']","For a quantum program implementing the Swap Test, QuSBT generated a test suite with 52 test cases, 75% of which were failing tests. The simulation took 3201 seconds, while the search execution took only 7 seconds.","Generating a test suite for a quantum program such that it has the maximum
number of failing tests is an optimization problem. For such optimization,
search-based testing has shown promising results in the context of classical
programs. To this end, we present a test generation tool for quantum programs
based on a genetic algorithm, called QuSBT (Search-based Testing of Quantum
Programs). QuSBT automates the testing of quantum programs, with the aim of
finding a test suite having the maximum number of failing test cases. QuSBT
utilizes IBM's Qiskit as the simulation framework for quantum programs. We
present the tool architecture in addition to the implemented methodology (i.e.,
the encoding of the search individual, the definition of the fitness function
expressing the search problem, and the test assessment w.r.t. two types of
failures). Finally, we report results of the experiments in which we tested a
set of faulty quantum programs with QuSBT to assess its effectiveness.
Repository (code and experimental results):
https://github.com/Simula-COMPLEX/qusbt-tool Video:
https://youtu.be/3apRCtluAn4",http://arxiv.org/abs/2204.08561v1
Addressing Leakage in Self-Supervised Contextualized Code Retrieval,"contextualized code retrieval, self-supervised learning, identifier masking, dedentation, syntax-aligned targets",2022-04,The paper proposes a novel self-supervised approach for contextualized code retrieval that reduces leakage between code contexts and targets during training. It also introduces a new dataset for evaluating this task based on aligned code clones. The approach achieves strong results on code retrieval as well as related tasks like clone and defect detection.,"1. Introduction - Motivates contextualized code retrieval task 2. Approach - Details the proposed de-leaking techniques 3. Dataset - Describes dataset for pre-training and new COCOS evaluation set 4. Evaluation - Results on code retrieval, clone detection, defect detection 5. Related Work 6. Conclusion","['Randomly split code into context and target for self-supervised training', 'Use tree-based span selection to obtain syntax-aligned targets', 'Apply mutual identifier masking to hide shared variables/names', 'Dedent the target code to remove indentation leakage', 'Train with contrastive loss on encoded context and target representations']","For the code snippet:

import mysql.connector
def totalSalary(id, name):
    ...

The context could be:

import mysql.connector 
def totalSalary(VAR2, name):
    connection = mysql.connector.connect(...)
    cursor = connection.cursor()
    return VAR1 + VAR3

And the dedented, masked target:

query = (""SELECT wage,bonus FROM employees WHERE emp_no = %s AND emp_name = %s"", id, VAR1)
VAR2.execute(query, (id, VAR1))
row = VAR2.fetchone()
salary, bonus = row","We address contextualized code retrieval, the search for code snippets
helpful to fill gaps in a partial input program. Our approach facilitates a
large-scale self-supervised contrastive training by splitting source code
randomly into contexts and targets. To combat leakage between the two, we
suggest a novel approach based on mutual identifier masking, dedentation, and
the selection of syntax-aligned targets. Our second contribution is a new
dataset for direct evaluation of contextualized code retrieval, based on a
dataset of manually aligned subpassages of code clones. Our experiments
demonstrate that our approach improves retrieval substantially, and yields new
state-of-the-art results for code clone and defect detection.",http://arxiv.org/abs/2204.11594v1
Composite Code Sparse Autoencoders for first stage retrieval,"composite codes, sparse autoencoders, inverted indexing, approximate nearest neighbors, HNSW",2022-04,The paper proposes a Composite Code Sparse Autoencoder (CCSA) approach for efficient approximate nearest neighbor search of dense document representations from Siamese-BERT models. CCSA learns to compress dense vectors into sparse composite codes that enable efficient indexing and retrieval.,1. Introduction 2. Related Work 3. Composite Code Sparse Autoencoders 4. Experiments 5. Conclusion,"['Use an autoencoder with gumbel-softmax activation to encode dense vectors into sparse composite codes', 'Apply uniformity regularizer to balance the inverted index posting lists', 'Create inverted index directly from the composite codes for efficient retrieval', 'Combine CCSA with HNSW graph-based ANN for improved memory usage']","For the MSMARCO dataset, CCSA outperforms inverted file indexes with product quantization in terms of recall and MRR metrics. It also provides smaller index sizes and lower memory usage when combined with the HNSW approach, while maintaining good retrieval performance.","We propose a Composite Code Sparse Autoencoder (CCSA) approach for
Approximate Nearest Neighbor (ANN) search of document representations based on
Siamese-BERT models. In Information Retrieval (IR), the ranking pipeline is
generally decomposed in two stages: the first stage focus on retrieving a
candidate set from the whole collection. The second stage re-ranks the
candidate set by relying on more complex models. Recently, Siamese-BERT models
have been used as first stage ranker to replace or complement the traditional
bag-of-word models. However, indexing and searching a large document collection
require efficient similarity search on dense vectors and this is why ANN
techniques come into play. Since composite codes are naturally sparse, we first
show how CCSA can learn efficient parallel inverted index thanks to an
uniformity regularizer. Second, CCSA can be used as a binary quantization
method and we propose to combine it with the recent graph based ANN techniques.
Our experiments on MSMARCO dataset reveal that CCSA outperforms IVF with
product quantization. Furthermore, CCSA binary quantization is beneficial for
the index size, and memory usage for the graph-based HNSW method, while
maintaining a good level of recall and MRR. Third, we compare with recent
supervised quantization methods for image retrieval and find that CCSA is able
to outperform them.",http://arxiv.org/abs/2204.07023v1
CoCoSoDa: Effective Contrastive Learning for Code Search,"multimodal contrastive learning, momentum mechanism, soft data augmentation, code search, pre-trained models",2022-04,"The paper proposes CoCoSoDa, a novel approach that effectively utilizes contrastive learning for code search via soft data augmentation and a momentum mechanism for generating negative samples. It achieves state-of-the-art performance on a large code search dataset across multiple programming languages.",1. Introduction 2. Related Work 2.1 Code Search 2.2 Code Representation Learning with Contrastive Learning 3. Proposed Approach 3.1 An Illustrative Example 3.2 Pre-trained Encoder and Momentum Encoder 3.3 Soft Data Augmentation 3.4 Multimodal Contrastive Learning 4. Experimental Setup 5. Results and Analysis 5.1 Overall Results 5.2 Ablation Study 5.3 Applying to Pre-trained Models 5.4 Hyperparameter Study 5.5 Qualitative and Quantitative Analysis 6. Conclusion,"['Uses pre-trained code/query encoders initialized with UniXcoder', 'Employs momentum encoders to generate consistent negative sample representations', 'Proposes four soft data augmentation (SoDa) methods: dynamic masking, dynamic replacement, dynamic replacement of specified type, dynamic masking of specified type', 'Utilizes multimodal contrastive learning with intra-modal loss (for unimodal data) and inter-modal loss (for aligning code-query pairs)']","For the code snippet 'def save_file(dataframe, filename): df = dataframe; df.to_csv(filename, sep=',', encoding='utf-8', index=False)', dynamic replacement could replace tokens like 'filename' with their type '<Identifier>', resulting in the augmented version: 'def save_file(dataframe, <Identifier>): df = dataframe; df.to_csv(<Identifier>, sep=',', encoding=<String>, index=False)'","Code search aims to retrieve semantically relevant code snippets for a given
natural language query. Recently, many approaches employing contrastive
learning have shown promising results on code representation learning and
greatly improved the performance of code search. However, there is still a lot
of room for improvement in using contrastive learning for code search. In this
paper, we propose CoCoSoDa to effectively utilize contrastive learning for code
search via two key factors in contrastive learning: data augmentation and
negative samples. Specifically, soft data augmentation is to dynamically
masking or replacing some tokens with their types for input sequences to
generate positive samples. Momentum mechanism is used to generate large and
consistent representations of negative samples in a mini-batch through
maintaining a queue and a momentum encoder. In addition, multimodal contrastive
learning is used to pull together representations of code-query pairs and push
apart the unpaired code snippets and queries. We conduct extensive experiments
to evaluate the effectiveness of our approach on a large-scale dataset with six
programming languages. Experimental results show that: (1) CoCoSoDa outperforms
14 baselines and especially exceeds CodeBERT, GraphCodeBERT, and UniXcoder by
13.3%, 10.5%, and 5.9% on average MRR scores, respectively. (2) The ablation
studies show the effectiveness of each component of our approach. (3) We adapt
our techniques to several different pre-trained models such as RoBERTa,
CodeBERT, and GraphCodeBERT and observe a significant boost in their
performance in code search. (4) Our model performs robustly under different
hyper-parameters. Furthermore, we perform qualitative and quantitative analyses
to explore reasons behind the good performance of our model.",http://arxiv.org/abs/2204.03293v3
DiffSearch: A Scalable and Precise Search Engine for Code Changes,"code change search, query language, indexing, precise matching, scalable retrieval",2022-04,"The paper presents DiffSearch, a scalable and precise search engine for finding code changes that match a given query describing the desired change pattern. It introduces a query language, an indexing approach for scalability, and an exact matching algorithm for precision.","1. Introduction, 2. Example and Overview, 3. Approach (Query Language, Representations, Indexing, Retrieval, Matching), 4. Implementation, 5. Evaluation, 6. Related Work, 7. Conclusion","['A query language extending the target programming language with wildcards and placeholders', 'Parsing code changes and queries into a feature space representation', 'Indexing feature vectors of code changes for scalable retrieval', 'Approximate retrieval of candidate matches based on the index', 'Precise tree-based matching to filter true positive results']","To find code changes that swap arguments in a function call used in a condition, the query could be: 

if(ID<1>(EXPR<1>, EXPR<2>)) { 
    <...> 
}
!
if(ID<1>(EXPR<2>, EXPR<1>)) {
    <...>
}","The source code of successful projects is evolving all the time, resulting in
hundreds of thousands of code changes stored in source code repositories. This
wealth of data can be useful, e.g., to find changes similar to a planned code
change or examples of recurring code improvements. This paper presents
DiffSearch, a search engine that, given a query that describes a code change,
returns a set of changes that match the query. The approach is enabled by three
key contributions. First, we present a query language that extends the
underlying programming language with wildcards and placeholders, providing an
intuitive way of formulating queries that is easy to adapt to different
programming languages. Second, to ensure scalability, the approach indexes code
changes in a one-time preprocessing step, mapping them into a feature space,
and then performs an efficient search in the feature space for each query.
Third, to guarantee precision, i.e., that any returned code change indeed
matches the given query, we present a tree-based matching algorithm that checks
whether a query can be expanded to a concrete code change. We present
implementations for Java, JavaScript, and Python, and show that the approach
responds within seconds to queries across one million code changes, has a
recall of 80.7% for Java, 89.6% for Python, and 90.4% for JavaScript, enables
users to find relevant code changes more effectively than a regular
expression-based search, and is helpful for gathering a large-scale dataset of
real-world bug fixes.",http://arxiv.org/abs/2204.02787v2
Code Search: A Survey of Techniques for Finding Code,"code search, code retrieval, query expansion, code representation, ranking search results",2022-04,"This paper provides a comprehensive survey of 30 years of research on techniques for searching and retrieving relevant code examples from large code corpora. It covers different types of queries, query preprocessing and expansion, indexing and retrieval approaches, ranking methods, and empirical studies.","{'1. Introduction': 'Motivates code search and outlines the scope of the survey', '2. Queries for Searching Code': 'Discusses different kinds of queries supported by code search engines', '3. Query Preprocessing and Expansion': 'Techniques for preprocessing and expanding queries', '4. Indexing and Retrieval': 'Core techniques for indexing code and retrieving relevant examples', '5. Ranking and Pruning Results': 'Methods for ranking and filtering search results', '6. Empirical Studies': 'Studies of how developers use code search in practice', '7. Challenges and Opportunities': 'Open challenges and future research directions'}","['Representing queries and code in vector spaces using techniques like word embeddings', 'Information retrieval methods like term matching, TF-IDF, BM25', 'Machine learning models like neural networks and transformers', 'Program analysis techniques to extract code representations', 'Ranking based on similarity scores, clustering, pruning irrelevant results']","One approach supports queries with 'holes' like: 

try {
    File file = File.createTempFile(""foo"", ""bar"");
    //??? code here
} catch (IOException e) {
    //??? 
}

It retrieves code snippets that can fill in the holes to complete the implementation.","The immense amounts of source code provide ample challenges and opportunities
during software development. To handle the size of code bases, developers
commonly search for code, e.g., when trying to find where a particular feature
is implemented or when looking for code examples to reuse. To support
developers in finding relevant code, various code search engines have been
proposed. This article surveys 30 years of research on code search, giving a
comprehensive overview of challenges and techniques that address them. We
discuss the kinds of queries that code search engines support, how to
preprocess and expand queries, different techniques for indexing and retrieving
code, and ways to rank and prune search results. Moreover, we describe
empirical studies of code search in practice. Based on the discussion of prior
work, we conclude the article with an outline of challenges and opportunities
to be addressed in the future.",http://arxiv.org/abs/2204.02765v3
Accelerating Code Search with Deep Hashing and Code Classification,"deep hashing, code search acceleration, code classification, recall and re-rank, binary hash codes",2022-03,"The paper proposes CoSHC, a novel approach to accelerate deep learning-based code search by integrating deep hashing and code classification. It decouples the search into a recall stage using binary hash codes and a re-rank stage, achieving over 90% retrieval time reduction while preserving at least 99% accuracy.",1. Introduction 2. Background 2.1 Code Search 2.2 Deep Hashing 3. Method 3.1 Offline Stage - Multiple Code Hashing Design with Code Classification Module - Deep Hashing Module 3.2 Online Stage - Recall and Re-rank Mechanism - Description Category Prediction Module,"['Cluster code embeddings into categories using K-Means', 'Train deep hashing module to generate binary hash codes preserving embedding similarities', 'Use category prediction module to estimate number of candidates per category', 'Recall candidates based on Hamming distance between query and code hash codes', 'Re-rank recalled candidates using original embeddings and cosine similarity']","For a query 'sort a list of integers', the category prediction module estimates it belongs to the 'sorting' category with high probability. The system retrieves the top N candidates in the 'sorting' category based on Hamming distances to the query's hash code. These candidates are then re-ranked using the original embeddings to produce the final ranked list of relevant code snippets.","Code search is to search reusable code snippets from source code corpus based
on natural languages queries. Deep learning-based methods of code search have
shown promising results. However, previous methods focus on retrieval accuracy
but lacked attention to the efficiency of the retrieval process. We propose a
novel method CoSHC to accelerate code search with deep hashing and code
classification, aiming to perform an efficient code search without sacrificing
too much accuracy. To evaluate the effectiveness of CoSHC, we apply our method
to five code search models. Extensive experimental results indicate that
compared with previous code search baselines, CoSHC can save more than 90% of
retrieval time meanwhile preserving at least 99% of retrieval accuracy.",http://arxiv.org/abs/2203.15287v2
CSRS: Code Search with Relevance Matching and Semantic Matching,"code search, relevance matching, semantic matching, co-attention, neural IR",2022-03,"The paper proposes CSRS, a novel code search model that combines relevance matching based on neural IR techniques and semantic matching using co-attention to improve retrieval of relevant code snippets for natural language queries. CSRS outperforms previous state-of-the-art models on a large code search dataset.","1. Introduction, 2. Related Work, 3. Preliminaries, 4. Proposed Model (CSRS), 5. Experiments, 6. Results, 7. Threats to Validity, 8. Conclusion","['CNN-based embedding module to extract n-gram embeddings from code and queries', 'Relevance matching module using neural IR to measure lexical keyword matching signals', 'Semantic matching module with co-attention to capture semantic correlation between code and query', 'Combined relevance and semantic matching features fed into MLP to produce final ranking score']","For the query 'Reallocates an array with a new size' and example code snippet provided, the model can identify lexical matches like 'array', 'new', 'size' through the relevance module, while also capturing the semantic intent of reallocating/resizing an array in the semantic module.","Developers often search and reuse existing code snippets in the process of
software development. Code search aims to retrieve relevant code snippets from
a codebase according to natural language queries entered by the developer. Up
to now, researchers have already proposed information retrieval (IR) based
methods and deep learning (DL) based methods. The IR-based methods focus on
keyword matching, that is to rank codes by relevance between queries and code
snippets, while DL-based methods focus on capturing the semantic correlations.
However, the existing methods do not consider capturing two matching signals
simultaneously. Therefore, in this paper, we propose CSRS, a code search model
with relevance matching and semantic matching. CSRS comprises (1) an embedding
module containing convolution kernels of different sizes which can extract
n-gram embeddings of queries and codes, (2) a relevance matching module that
measures lexical matching signals, and (3) a co-attention based semantic
matching module to capture the semantic correlation. We train and evaluate CSRS
on a dataset with 18.22M and 10k code snippets. The experimental results
demonstrate that CSRS achieves an MRR of 0.614, which outperforms two
state-of-the-art models DeepCS and CARLCS-CNN by 33.77% and 18.53%
respectively. In addition, we also conducted several experiments to prove the
effectiveness of each component of CSRS.",http://arxiv.org/abs/2203.07736v4
ReACC: A Retrieval-Augmented Code Completion Framework,"retrieval-augmented code completion, code-to-code retrieval, contrastive learning on code, semantic-preserving code transformations, hybrid retriever",2022-03,"This paper proposes ReACC, a retrieval-augmented code completion framework that retrieves similar code snippets from a codebase to assist in predicting the following tokens for an unfinished code snippet. It combines a code retriever trained with contrastive learning and an autoregressive language model.","1. Introduction 2. Related Work 3. Approach: 3.1 Task Formulation, 3.2 Retriever, 3.3 Generator 4. Experiments 5. Conclusion","['Uses a hybrid retriever combining sparse (BM25) and dense retrievers', 'Dense retriever is trained with in-batch negatives and contrastive loss', 'Data augmentation via semantic-preserving code transformations like identifier renaming and dead code insertion', 'Concatenates retrieved code with original context as input to autoregressive language model']","For the Python code snippet `def normalize(a): ma = np.mean(a) sa = np.std(a) return (a-ma)/sa`, the retriever may retrieve a similar function like `def standardization(arr): mu = np.mean(arr) std = np.std(arr) return (arr - mu)/std` to help predict the remaining tokens.","Code completion, which aims to predict the following code token(s) according
to the code context, can improve the productivity of software development.
Recent work has proved that statistical language modeling with transformers can
greatly improve the performance in the code completion task via learning from
large-scale source code datasets. However, current approaches focus only on
code context within the file or project, i.e. internal context. Our distinction
is utilizing ""external"" context, inspired by human behaviors of copying from
the related code snippets when writing code. Specifically, we propose a
retrieval-augmented code completion framework, leveraging both lexical copying
and referring to code with similar semantics by retrieval. We adopt a
stage-wise training approach that combines a source code retriever and an
auto-regressive language model for programming language. We evaluate our
approach in the code completion task in Python and Java programming languages,
achieving a state-of-the-art performance on CodeXGLUE benchmark.",http://arxiv.org/abs/2203.07722v1
Efficient Search of Live-Coding Screencasts from Online Videos,"live-coding screencast identification, video frame classification, IDE window detection, video content analysis, programming video search",2022-03,"This paper presents PSFinder, a tool to automatically identify live-coding screencasts from online videos. It uses a frame sampling strategy and a classifier to detect IDE windows in video frames, and then applies rules to determine if a video qualifies as a live-coding screencast.",1. Introduction 2. Approach 2.1. Frame Sampler 2.2. Video Classifier 2.2.1. Frame-Level Classifier 2.2.2. Video-Level Classification Strategy 3. Preliminary Experiment 3.1. Dataset 3.2. Experimental Settings 3.3. Research Questions 4. Results 4.1. Effectiveness in Video Classification 4.2. Analysis of Misclassified Videos 5. Related Work 6. Conclusion and Future Work,"['- Extract video frames at 1 frame per 30 seconds', '- Mark duplicate frames using normalized root-mean-square error (NRMSE) on pixel values', '- Train a Vision Transformer (ViT) model to classify frames as containing an IDE window or not', '- Identify live-coding screencasts based on: 1) Presence of a minimum contiguous sequence of IDE frames, 2) Proportion of IDE frames exceeds a threshold']","For a video introducing the GPars Java library, PSFinder misclassified it as a live-coding screencast. This video contained IDE screenshots along with a moving live shot of the presenter, causing the frame sampling to not mark the IDE screenshot frames as duplicates. The continuous movement led PSFinder to assume it was a live coding session.","Programming videos on the Internet are valuable resources for learning
programming skills. To find relevant videos, developers typically search online
video platforms (e.g., YouTube) with keywords on topics they wish to learn.
Developers often look for live-coding screencasts, in which the videos' authors
perform live coding. Yet, not all programming videos are live-coding
screencasts. In this work, we develop a tool named PSFinder to identify
live-coding screencasts. PSFinder leverages a classifier to identify whether a
video frame contains an IDE window. It uses a sampling strategy to pick a
number of frames from an input video, runs the classifer on these frames, and
then determines whether the video is a live-coding screencast based on frames
classified as containing IDE window. In our preliminary experiment, PSFinder
can effectively identify live-coding screencasts as it achieves an F1-score of
0.97.",http://arxiv.org/abs/2203.04519v1
Learning logic programs by discovering where not to search,"constraint discovery, inductive logic programming, bias discovery, answer set programming, program synthesis",2022-02,"The paper introduces a novel approach to improve inductive logic programming (ILP) by automatically discovering constraints on hypotheses before searching for a solution. By analyzing the background knowledge, the approach identifies properties like irreflexivity and functional dependencies to build optimally sound constraints that prune the hypothesis space, leading to substantial performance gains.",1. Introduction 2. Related Work 3. Problem Setting 4. BK Constraint Discovery 4.1 Properties 4.2 Constraints 5. Experiments 6. Conclusion,"['Use a bottom-up approach implemented in answer set programming (ASP) to discover relational properties and functional dependencies from the background knowledge', 'Generate constraints corresponding to the discovered properties to prohibit hypotheses violating those properties', 'Use the generated constraints to bootstrap a constraint-driven ILP system (DISCO based on POPPER) to prune the hypothesis space before searching for a solution']","For the list transformation example with background knowledge about head/2, tail/2, odd/1, even/1, the approach can deduce that tail/2 is irreflexive, asymmetric and antitransitive, and odd/1 and even/1 are mutually exclusive. This allows pruning rules like h <- tail(A,A) and h <- head(A,B), odd(B), even(B) from the hypothesis space before search.","The goal of inductive logic programming (ILP) is to search for a hypothesis
that generalises training examples and background knowledge (BK). To improve
performance, we introduce an approach that, before searching for a hypothesis,
first discovers where not to search. We use given BK to discover constraints on
hypotheses, such as that a number cannot be both even and odd. We use the
constraints to bootstrap a constraint-driven ILP system. Our experiments on
multiple domains (including program synthesis and game playing) show that our
approach can (i) substantially reduce learning times by up to 97%, and (ii)
scale to domains with millions of facts.",http://arxiv.org/abs/2202.09806v2
A Probabilistic Programming Idiom for Active Knowledge Search,"probabilistic programming idiom, active knowledge acquisition, robot exploration, variational inference, cognitive architecture",2022-02,This paper derives and implements a probabilistic programming idiom for making decisions to acquire new knowledge about an environment. The idiom is validated through simulations for the active mapping and robot exploration problem.,1. Introduction 2. Preliminaries 3. Decision Model Derivation 4. Application to Active Mapping and Exploration 5. Simulation Results 6. Conclusion,"['Derives a probabilistic model dividing cognitive tasks into learning and planning components', 'Defines distributions for progress, information gain, constraints, and attention variables to guide exploration', 'Uses variational inference and stochastic gradient ascent for approximate inference', 'Employs KL divergence approximations to quantify progress and information gain']","For active mapping, the progress variable quantifies how different the current map state is from past states using an approximation of the KL divergence between the distributions. The information gain encourages exploring unknown areas by maximizing the entropy of occupied/free space cells.","In this paper, we derive and implement a probabilistic programming idiom for
the problem of acquiring new knowledge about an environment. The idiom is
implemented utilizing a modern probabilistic programming language. We
demonstrate the utility of this idiom by implementing an algorithm for the
specific problem of active mapping and robot exploration. Finally, we evaluate
the functionality of the implementation through an extensive simulation study
utilizing the HouseExpo dataset.",http://arxiv.org/abs/2202.09555v1
Code Search based on Context-aware Code Translation,"context-aware code translation, shared word mapping, code search, instruction simulation, translation rules",2022-02,The paper proposes a novel context-aware code translation technique called TranCS that translates code snippets into natural language descriptions to bridge the representation gap between code and queries for improved code search. It uses instruction simulation and translation rules to capture code semantics and a shared word mapping to reduce embedding discrepancy.,"1. Introduction - Motivation and overview 2. Background - Machine instructions, deep learning code search 3. Motivation - Limitations of existing code representations 4. Methodology - Context-aware translation, shared word mapping 5. Experimental Evaluation 6. Related Work 7. Conclusion","['Disassemble code to get instruction sequence', 'Simulate instruction execution to collect context (variables, dependencies, etc.)', 'Apply translation rules to generate natural language description from instructions+context', 'Use shared word mapping function/vocabulary to generate embeddings for descriptions and queries']","For the code snippets calculating array sum in for and while loops, TranCS generates the translations: 
0: push int constant 0. 1: store int 0 into local variable sum/result. ... 22: load int value_5 from local variable sum/result. 23: return int value_5 from method. The translations capture the semantics using natural language while the variable names (sum vs result) are the only difference, allowing shared embeddings.","Code search is a widely used technique by developers during software
development. It provides semantically similar implementations from a large code
corpus to developers based on their queries. Existing techniques leverage deep
learning models to construct embedding representations for code snippets and
queries, respectively. Features such as abstract syntactic trees, control flow
graphs, etc., are commonly employed for representing the semantics of code
snippets. However, the same structure of these features does not necessarily
denote the same semantics of code snippets, and vice versa. In addition, these
techniques utilize multiple different word mapping functions that map query
words/code tokens to embedding representations. This causes diverged embeddings
of the same word/token in queries and code snippets. We propose a novel
context-aware code translation technique that translates code snippets into
natural language descriptions (called translations). The code translation is
conducted on machine instructions, where the context information is collected
by simulating the execution of instructions. We further design a shared word
mapping function using one single vocabulary for generating embeddings for both
translations and queries. We evaluate the effectiveness of our technique,
called TranCS, on the CodeSearchNet corpus with 1,000 queries. Experimental
results show that TranCS significantly outperforms state-of-the-art techniques
by 49.31% to 66.50% in terms of MRR (mean reciprocal rank).",http://arxiv.org/abs/2202.08029v1
On the Importance of Building High-quality Training Datasets for Neural Code Search,"code search dataset cleaning, rule-based syntactic filtering, variational autoencoder, semantic query filtering, bootstrap query corpus",2022-02,This paper proposes an automated framework to clean code search datasets by filtering out noisy and unnatural queries. It uses a rule-based filter to remove syntactic anomalies and a variational autoencoder model to identify semantically invalid queries.,1. Introduction 2. Preliminaries 3. Data Cleaning Framework 4. Experimental Setup 5. Results 6. Discussion 7. Related Work 8. Conclusion,"['Rule-based syntactic filter to remove comments with HTML tags, URLs, short sentences etc.', ""Variational autoencoder trained on a 'bootstrap query corpus' from Stack Overflow question titles"", 'VAE reconstruction loss used to identify comments semantically divergent from natural queries', 'EM-GMM clustering to separate qualified and unqualified comments based on VAE loss']","For the comment 'Returns a {@link Support}', the rule-based filter would remove the Javadoc tag {@link Support}, while the VAE filter would identify that the semantics of 'Returns a' diverges from natural queries like 'convert string to JSON object'.","The performance of neural code search is significantly influenced by the
quality of the training data from which the neural models are derived. A large
corpus of high-quality query and code pairs is demanded to establish a precise
mapping from the natural language to the programming language. Due to the
limited availability, most widely-used code search datasets are established
with compromise, such as using code comments as a replacement of queries. Our
empirical study on a famous code search dataset reveals that over one-third of
its queries contain noises that make them deviate from natural user queries.
Models trained through noisy data are faced with severe performance degradation
when applied in real-world scenarios. To improve the dataset quality and make
the queries of its samples semantically identical to real user queries is
critical for the practical usability of neural code search. In this paper, we
propose a data cleaning framework consisting of two subsequent filters: a
rule-based syntactic filter and a model-based semantic filter. This is the
first framework that applies semantic query cleaning to code search datasets.
Experimentally, we evaluated the effectiveness of our framework on two
widely-used code search models and three manually-annotated code retrieval
benchmarks. Training the popular DeepCS model with the filtered dataset from
our framework improves its performance by 19.2% MRR and 21.3% Answer@1, on
average with the three validation benchmarks.",http://arxiv.org/abs/2202.06649v1
Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus,"semantic code search, multi-modal embedding, self-attention pooling, representation fusion, CodeSearchNet benchmark",2022-01,This paper proposes a novel deep semantic model for code search that aligns cross-lingual embeddings for multi-modal learning and combines different learned representations using self-attention pooling. The model achieves state-of-the-art results on the CodeSearchNet benchmark.,1. Introduction 2. Related Work 3. Deep Semantic Code Search 3.1 Task Definition 3.2 Deep Match Framework 3.3 Aligning Contextual Embedding 3.4 Self Attention Pooling 3.5 Fusion Representations 3.6 Tokenization 3.7 Learning the Model 4. Indexing and Querying 5. Experiments 6.1 Evaluation Methodology 6.2 Results 7. Conclusion 8. Acknowledgements,"['Aligns cross-lingual embeddings using a linear map to project code token vectors to a shared semantic space', 'Uses self-attention pooling to aggregate encoder outputs into single vector representations for query and code', 'Fuses different aggregated vectors using a weighted sum to obtain a combined representation', 'Employs a cosine similarity loss to encourage similar embeddings for matching query-code pairs during training']","For the query 'how to read a file in python', the model would embed the query using the description encoder, and retrieve code snippets with high cosine similarity in the shared embedding space, such as: with open('file.txt', 'r') as f: contents = f.read() ","Semantic code search is the task of retrieving relevant code snippet given a
natural language query. Different from typical information retrieval tasks,
code search requires to bridge the semantic gap between the programming
language and natural language, for better describing intrinsic concepts and
semantics. Recently, deep neural network for code search has been a hot
research topic. Typical methods for neural code search first represent the code
snippet and query text as separate embeddings, and then use vector distance
(e.g. dot-product or cosine) to calculate the semantic similarity between them.
There exist many different ways for aggregating the variable length of code or
query tokens into a learnable embedding, including bi-encoder, cross-encoder,
and poly-encoder. The goal of the query encoder and code encoder is to produce
embeddings that are close with each other for a related pair of query and the
corresponding desired code snippet, in which the choice and design of encoder
is very significant.
  In this paper, we propose a novel deep semantic model which makes use of the
utilities of not only the multi-modal sources, but also feature extractors such
as self-attention, the aggregated vectors, combination of the intermediate
representations. We apply the proposed model to tackle the CodeSearchNet
challenge about semantic code search. We align cross-lingual embedding for
multi-modality learning with large batches and hard example mining, and combine
different learned representations for better enhancing the representation
learning. Our model is trained on CodeSearchNet corpus and evaluated on the
held-out data, the final model achieves 0.384 NDCG and won the first place in
this benchmark. Models and code are available at
https://github.com/overwindows/SemanticCodeSearch.git.",http://arxiv.org/abs/2201.11313v1
CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search,"contrastive code-text pretraining, unsupervised code-code pair mining, cross-language code representation, function-level code semantics, large-scale code corpus",2022-01,"This paper proposes CodeRetriever, a model that learns function-level code semantic representations through large-scale contrastive pre-training on code-text and code-code pairs. It achieves new state-of-the-art performance on 11 code search benchmarks across 6 programming languages.","1. Introduction - Motivation and limitations of existing code pre-training 2. Preliminary on code search 3. Approach - Unimodal/bimodal contrastive losses, overall objective 4. Building positive pairs - Code-document, code-comment, unsupervised code-code 5. Experiments - Datasets, results analysis 6. Related work 7. Conclusion","['Siamese encoder architecture with code encoder and text encoder', 'Unimodal contrastive loss on code-code pairs to learn code semantics', 'Bimodal contrastive loss on code-text pairs using documentation and comments', 'Unsupervised mining of code-code pairs based on name/doc matching and denoising']","For the Fibonacci number example, the model can learn that two different implementations 'def Fibonacci(n):' and 'def Fibonacci_Number(index):' are semantically related by contrasting the code pairs along with their documentation 'Return/Get the Fibonacci number'.","In this paper, we propose the CodeRetriever model, which learns the
function-level code semantic representations through large-scale code-text
contrastive pre-training. We adopt two contrastive learning schemes in
CodeRetriever: unimodal contrastive learning and bimodal contrastive learning.
For unimodal contrastive learning, we design an unsupervised learning approach
to build semantic-related code pairs based on the documentation and function
name. For bimodal contrastive learning, we leverage the documentation and
in-line comments of code to build code-text pairs. Both contrastive objectives
can fully leverage large-scale code corpus for pre-training. Extensive
experimental results show that CodeRetriever achieves new state-of-the-art with
significant improvement over existing code pre-trained models, on eleven
domain/language-specific code search tasks with six programming languages in
different code granularity (function-level, snippet-level and statement-level).
These results demonstrate the effectiveness and robustness of CodeRetriever.",http://arxiv.org/abs/2201.10866v3
Generating Clarifying Questions for Query Refinement in Source Code Search,"clarifying questions, query refinement, source code search, natural language processing, task extraction",2022-01,"The paper proposes a method called ZaCQ (Zero-aspect Clarifying Question) to generate clarifying questions for refining source code search queries using natural language processing techniques. The key novelty is dynamically identifying relevant query aspects from the search results to ask targeted clarifying questions, without relying on predefined aspects.",1. Introduction 2. Background and Related Work 2.1 Clarifying Questions for Query Refinement 2.2 Query Refinement in Software Engineering 3. Approach 3.1 Overview 3.2 Task Extraction 3.3 Clarifying Question Generation 3.4 Result Reranking 4. Evaluation 4.1 Synthetic Evaluation 4.2 Human Evaluation 5. Conclusion,"[""Extract development 'tasks' (verb+object phrases) from query and search results using natural language processing"", 'Identify potential query aspects by finding common/differing task attributes across results', 'Generate clarifying question targeting a salient aspect to confirm relevance or elicit missing information', ""Based on user's response, update query representation and rerank search results""]","For the query 'convert float', ZaCQ may extract tasks like 'convert float to int' and 'convert float to string' from the results. It could then ask a clarifying question like 'Would you like to convert a float to an integer or to a string?' to disambiguate the user's intent.","In source code search, a common information-seeking strategy involves
providing a short initial query with a broad meaning, and then iteratively
refining the query using terms gleaned from the results of subsequent searches.
This strategy requires programmers to spend time reading search results that
are irrelevant to their development needs. In contrast, when programmers seek
information from other humans, they typically refine queries by asking and
answering clarifying questions. Clarifying questions have been shown to benefit
general-purpose search engines, but have not been examined in the context of
code search. We present a method for generating natural-sounding clarifying
questions using information extracted from function names and comments. Our
method outperformed a keyword-based method for single-turn refinement in
synthetic studies, and was associated with shorter search duration in human
studies.",http://arxiv.org/abs/2201.09974v1
Cross-Domain Deep Code Search with Meta Learning,"few-shot meta learning, cross-domain code search, model-agnostic meta-learning (MAML), transfer learning, domain adaptation",2022-01,"This paper proposes CroCS, a novel approach for cross-domain code search that employs few-shot meta learning to adapt a pre-trained code model from data-rich languages to low-resource domain-specific languages. It outperforms conventional fine-tuning methods, especially when data is scarce.",1. Introduction 2. Background 2.1 Deep learning for code search 2.2 Pre-trained models like CodeBERT 2.3 Meta learning and few-shot learning 3. Approach 3.1 Overview of CroCS 3.2 Pre-training 3.3 Meta learning with MAML 3.4 Fine-tuning 3.5 Code search 4. Experiments 5. Related Work 6. Conclusion,"['Pre-train a code representation model (e.g. CodeBERT) on a large corpus of common programming languages', 'Employ model-agnostic meta-learning (MAML) to adapt the pre-trained model to the target domain-specific language', 'Fine-tune the meta-learned model on code search data of the target language', 'Use the fine-tuned model for code search by computing vector similarities between queries and code']","For example, to adapt a pre-trained Python/Java model to Solidity smart contracts, CroCS first pre-trains on Python/Java data. It then uses MAML to learn good initial parameters from the Python/Java data that can quickly adapt to the Solidity domain with just a few Solidity examples. After meta-learning, it fine-tunes the model on a Solidity code search dataset to learn the final mapping between natural language and Solidity code.","Recently, pre-trained programming language models such as CodeBERT have
demonstrated substantial gains in code search. Despite showing great
performance, they rely on the availability of large amounts of parallel data to
fine-tune the semantic mappings between queries and code. This restricts their
practicality in domain-specific languages with relatively scarce and expensive
data. In this paper, we propose CroCS, a novel approach for domain-specific
code search. CroCS employs a transfer learning framework where an initial
program representation model is pre-trained on a large corpus of common
programming languages (such as Java and Python) and is further adapted to
domain-specific languages such as SQL and Solidity. Unlike cross-language
CodeBERT, which is directly fine-tuned in the target language, CroCS adapts a
few-shot meta-learning algorithm called MAML to learn the good initialization
of model parameters, which can be best reused in a domain-specific language. We
evaluate the proposed approach on two domain-specific languages, namely, SQL
and Solidity, with model transferred from two widely used languages (Python and
Java). Experimental results show that CDCS significantly outperforms
conventional pre-trained code models that are directly fine-tuned in
domain-specific languages, and it is particularly effective for scarce data.",http://arxiv.org/abs/2201.00150v6
Semantic Code Search for Smart Contracts,"smart contract code search, contract elements dependency graph, multi-modal embedding, multi-head attention, pretrained language model",2021-11,"This paper proposes a multi-modal neural code search model called MM-SCS tailored for smart contracts, which incorporates a novel graph representation called Contract Elements Dependency Graph (CEDG) to capture code structure. It also uses multi-head attention and a pretrained language model to improve code and query embeddings.",1. Introduction 2. Background and Related Work 3. Contract Elements Dependency Graph 4. Multi-Modal Smart Contract Code Search Model 5. Experiments 6. Conclusion,"['Construct a Contract Elements Dependency Graph (CEDG) to represent control-flow, data-flow and unique elements in smart contracts', 'Use multi-head self-attention to generate embeddings for code tokens, function names and API sequences', 'Use a modified graph attention network to embed the CEDG graph', 'Fine-tune a pretrained ALBERT model as the query encoder']","For the smart contract function 'function withdraw() public { ... }' with query 'Withdraw all the deposits of the caller?', MM-SCS would construct a CEDG with nodes for the withdraw function, variables like amount, control-flow edges for the function body, etc. It would use multi-head attention to embed the code tokens, name 'withdraw', and relevant APIs, and the graph attention network to embed the CEDG. The query would be encoded by the pretrained ALBERT model. The embeddings are compared to retrieve relevant code snippets.","Semantic code search technology allows searching for existing code snippets
through natural language, which can greatly improve programming efficiency.
Smart contracts, programs that run on the blockchain, have a code reuse rate of
more than 90%, which means developers have a great demand for semantic code
search tools. However, the existing code search models still have a semantic
gap between code and query, and perform poorly on specialized queries of smart
contracts. In this paper, we propose a Multi-Modal Smart contract Code Search
(MM-SCS) model. Specifically, we construct a Contract Elements Dependency Graph
(CEDG) for MM-SCS as an additional modality to capture the data-flow and
control-flow information of the code. To make the model more focused on the key
contextual information, we use a multi-head attention network to generate
embeddings for code features. In addition, we use a fine-tuned pretrained model
to ensure the model's effectiveness when the training data is small. We
compared MM-SCS with four state-of-the-art models on a dataset with 470K (code,
docstring) pairs collected from Github and Etherscan. Experimental results show
that MM-SCS achieves an MRR (Mean Reciprocal Rank) of 0.572, outperforming four
state-of-the-art models UNIF, DeepCS, CARLCS-CNN, and TAB-CS by 34.2%, 59.3%,
36.8%, and 14.1%, respectively. Additionally, the search speed of MM-SCS is
second only to UNIF, reaching 0.34s/query.",http://arxiv.org/abs/2111.14139v1
GraphSearchNet: Enhancing GNNs via Capturing Global Dependencies for Semantic Code Search,"graph neural networks, code search, multi-head attention, global dependencies, bidirectional GGNN",2021-11,"This paper proposes GraphSearchNet, a novel neural network framework that enhances graph neural networks (GNNs) to better capture global dependencies for accurate semantic code search. It uses bidirectional GGNNs with multi-head attention to jointly learn rich semantics from source code and natural language queries.","{'1. Introduction': 'Motivates the problem of code search and limitations of existing approaches', '2. Background and Motivation': 'Provides background on GNNs, existing datasets, and multi-head attention', '3. Approach': 'Describes the proposed GraphSearchNet framework', '4. Experimental Setup': 'Details the datasets, baselines, and evaluation metrics', '5. Results': 'Presents empirical results comparing GraphSearchNet to baselines', '6. Discussion': ""Analyzes GraphSearchNet's performance and characteristics"", '7. Related Work': 'Surveys related work on code search and GNNs', '8. Conclusion': 'Summarizes the key contributions'}","['Construct graphs for source code (using AST, data flow edges) and natural language queries (using dependency parsing)', 'Use bidirectional GGNNs (BiGGNNs) to capture local structural information in the graphs', 'Enhance BiGGNNs with multi-head attention to capture global dependencies missed by GGNNs', 'Train the model on (code, summary) pairs to learn semantic mappings', 'At inference, encode queries and retrieve top-k code candidates based on vector similarity']","For the function 'next_power_of_2', GraphSearchNet can capture the global dependency between variables n4 and n6 that traditional GGNNs would miss due to the lack of a direct path between them in the graph.","Code search aims to retrieve accurate code snippets based on a natural
language query to improve software productivity and quality. With the massive
amount of available programs such as (on GitHub or Stack Overflow), identifying
and localizing the precise code is critical for the software developers. In
addition, Deep learning has recently been widely applied to different
code-related scenarios, e.g., vulnerability detection, source code
summarization. However, automated deep code search is still challenging since
it requires a high-level semantic mapping between code and natural language
queries. Most existing deep learning-based approaches for code search rely on
the sequential text i.e., feeding the program and the query as a flat sequence
of tokens to learn the program semantics while the structural information is
not fully considered. Furthermore, the widely adopted Graph Neural Networks
(GNNs) have proved their effectiveness in learning program semantics, however,
they also suffer the problem of capturing the global dependencies in the
constructed graph, which limits the model learning capacity. To address these
challenges, in this paper, we design a novel neural network framework, named
GraphSearchNet, to enable an effective and accurate source code search by
jointly learning the rich semantics of both source code and natural language
queries. Specifically, we propose to construct graphs for the source code and
queries with bidirectional GGNN (BiGGNN) to capture the local structural
information of the source code and queries. Furthermore, we enhance BiGGNN by
utilizing the multi-head attention module to supplement the global dependencies
that BiGGNN missed to improve the model learning capacity. The extensive
experiments on Java and Python programming language from the public benchmark
CodeSearchNet confirm that GraphSearchNet outperforms current state-of-the-art
works.",http://arxiv.org/abs/2111.02671v5
Learning Large Neighborhood Search Policy for Integer Programming,"reinforcement learning, integer programming, large neighborhood search, graph neural networks, factorized action space",2021-11,The paper proposes a deep reinforcement learning method to learn large neighborhood search policies for solving integer programming problems more efficiently. The key novelty is using a graph neural network to factorize the exponentially large action space of variable subsets into binary decisions per variable.,"{'1. Introduction': 'Motivates learning heuristics for integer programs and large neighborhood search', '2. Related Work': 'Discusses prior work on learning for specific COPs vs integer programs, and RL with large action spaces', '3. Preliminaries': 'Defines integer programming, large neighborhood search', '4. Methodology': 'Formulates the LNS as an MDP, factorizes the action space, describes the GNN policy network and training algorithm', '5. Experiments': 'Evaluates the method on 4 NP-hard benchmarks and compares to baselines', '6. Conclusion': 'Summarizes the key contributions'}","['Formulate the LNS for integer programs as a Markov Decision Process (MDP)', 'Factorize the exponentially large action space of variable subsets into binary decisions per variable', 'Parametrize policies for each variable using a Graph Neural Network (GNN) that shares parameters across variables', 'Train the GNN policy network using a customized actor-critic reinforcement learning algorithm']","For an IP with 4 variables and 3 constraints, the GNN policy network takes as input the current solution, constraint matrix, and dynamic statistics. It outputs 4 probabilities, one per variable, indicating whether to reoptimize that variable. The selected variable subset is then reoptimized by an IP solver as the repair operator.","We propose a deep reinforcement learning (RL) method to learn large
neighborhood search (LNS) policy for integer programming (IP). The RL policy is
trained as the destroy operator to select a subset of variables at each step,
which is reoptimized by an IP solver as the repair operator. However, the
combinatorial number of variable subsets prevents direct application of typical
RL algorithms. To tackle this challenge, we represent all subsets by
factorizing them into binary decisions on each variable. We then design a
neural network to learn policies for each variable in parallel, trained by a
customized actor-critic algorithm. We evaluate the proposed method on four
representative IP problems. Results show that it can find better solutions than
SCIP in much less time, and significantly outperform other LNS baselines with
the same runtime. Moreover, these advantages notably persist when the policies
generalize to larger problems. Further experiments with Gurobi also reveal that
our method can outperform this state-of-the-art commercial solver within the
same time limit.",http://arxiv.org/abs/2111.03466v1
Scaling Neural Program Synthesis with Distribution-based Search,"distribution-based search, program synthesis, enumerative search, sampling algorithms, parallel search",2021-10,"The paper introduces a framework called distribution-based search for neural program synthesis, along with two new search algorithms - Heap Search (enumerative) and SQRT Sampling (probabilistic). Theoretical analysis and empirical evaluation show these algorithms can effectively integrate with neural models to solve complex program synthesis tasks at scale.","{'1. Introduction': 'Motivates program synthesis and using machine learning models to guide search', '2. Distribution-based search': 'Defines the theoretical framework and loss metric for search algorithms', '3. Enumerative methods': 'Introduces the new Heap Search enumerative algorithm', '4. Sampling methods': 'Analyzes sampling algorithms and proposes SQRT Sampling', '5. Parallel implementations': 'Describes extensions for parallel execution', '6. Experiments': 'Evaluates the new and existing algorithms on benchmarks'}","['Define a probabilistic context-free grammar (PCFG) from the domain-specific language', 'Use a neural network to predict probabilities over the PCFG rules', 'Search through programs sampled from the induced distribution to find one matching the input-output examples', 'Heap Search enumerates programs in decreasing probability order using heap data structures', 'SQRT Sampling draws programs from the square root of the PCFG distribution']","For the distribution D(n) = 1/(2^(n+1)) over natural numbers n, the SQRT Sampling algorithm samples from the distribution sqrt(D(n)) = 1/((1+sqrt(2))*(2^(n+1/2))), which is theoretically optimal among all sampling algorithms for D.","We consider the problem of automatically constructing computer programs from
input-output examples. We investigate how to augment probabilistic and neural
program synthesis methods with new search algorithms, proposing a framework
called distribution-based search. Within this framework, we introduce two new
search algorithms: Heap Search, an enumerative method, and SQRT Sampling, a
probabilistic method. We prove certain optimality guarantees for both methods,
show how they integrate with probabilistic and neural techniques, and
demonstrate how they can operate at scale across parallel compute environments.
Collectively these findings offer theoretical and applied studies of search
algorithms for program synthesis that integrate with recent developments in
machine-learned program synthesizers.",http://arxiv.org/abs/2110.12485v1
"Neural-guided, Bidirectional Program Search for Abstraction and Reasoning","bidirectional program search, inverse semantics, execution-guided program synthesis, abstraction via compression, visual reasoning",2021-10,"The paper proposes two novel approaches for solving visual reasoning tasks from the Abstraction and Reasoning Corpus (ARC): 1) using program synthesis with compression to learn abstractions, and 2) a bidirectional neural-guided search algorithm that leverages inverse semantics for deductive reasoning.","1. Introduction 2. Abstraction using DreamCoder 2.1 Warmup: Forming Abstractions 2.2 Enabling Generalization on ARC Symmetry Tasks 2.3 Discussion 3. Bidirectional, Neural-guided Program Search 3.1 Algorithm Description 3.2 Experiments","['Use DreamCoder program synthesis system to create symbolic abstractions from solved tasks via compression', 'Extend execution-guided program synthesis with deductive reasoning based on function inverse semantics', 'Formulate as a reinforcement learning problem over a graph of grounded/ungrounded nodes', 'Use neural networks to guide search over programs applying functions forward, inversely or conditionally inversely']","For the ARC task of arranging copies of the input grid based on the most common color (Figure 4), the reasoning steps are: 1) Notice output has copies of input arranged in a pattern, 2) Determine where to place copies, 3) Observe placements match pixels of a color in input, 4) Deduce that color is the most common one, 5) Solution is to place copies along pixels of most common color.","One of the challenges facing artificial intelligence research today is
designing systems capable of utilizing systematic reasoning to generalize to
new tasks. The Abstraction and Reasoning Corpus (ARC) measures such a
capability through a set of visual reasoning tasks. In this paper we report
incremental progress on ARC and lay the foundations for two approaches to
abstraction and reasoning not based in brute-force search. We first apply an
existing program synthesis system called DreamCoder to create symbolic
abstractions out of tasks solved so far, and show how it enables solving of
progressively more challenging ARC tasks. Second, we design a reasoning
algorithm motivated by the way humans approach ARC. Our algorithm constructs a
search graph and reasons over this graph structure to discover task solutions.
More specifically, we extend existing execution-guided program synthesis
approaches with deductive reasoning based on function inverse semantics to
enable a neural-guided bidirectional search algorithm. We demonstrate the
effectiveness of the algorithm on three domains: ARC, 24-Game tasks, and a
'double-and-add' arithmetic puzzle.",http://arxiv.org/abs/2110.11536v2
AugmentedCode: Examining the Effects of Natural Language Resources in Code Retrieval Models,"augmented code, code retrieval, natural language resources, docstrings, code comments, commit messages",2021-10,"This paper introduces AugmentedCode, a framework that leverages natural language resources like docstrings, code comments, and commit messages to improve the performance of code retrieval models. Experiments show significant improvements over baselines like CodeSearchNet and CodeBERT.","1. Introduction 2. AugmentedCode Framework 2.1 Why (Objective and Goal) 2.2 What (Docstrings, Code Comments, Commit Messages) 2.3 How (Augmented Code Scenarios) 3. Experiments 3.1 Results 4. Demonstration 5. Conclusion","['Constructed different Augmented Code Scenarios (ACS) using combinations of code comments, docstrings, commit messages', 'Trained code retrieval models on CodeSearchNet and CodeBERT architectures using the ACS data', 'Evaluated models using Mean Reciprocal Rank (MRR) on test sets', 'Analyzed performance on larger datasets to test scalability']","For the ACS=4 scenario, which combined code comments and entire docstrings with the code, the model achieved an MRR of 0.961 on CodeBERT, significantly outperforming the baselines. Even on a 45x larger search space, it maintained an MRR of 0.821, showing strong scalability.","Code retrieval is allowing software engineers to search codes through a
natural language query, which relies on both natural language processing and
software engineering techniques. There have been several attempts on code
retrieval from searching snippet codes to function codes. In this paper, we
introduce Augmented Code (AugmentedCode) retrieval which takes advantage of
existing information within the code and constructs augmented programming
language to improve the code retrieval models' performance. We curated a large
corpus of Python and showcased the the framework and the results of augmented
programming language which outperforms on CodeSearchNet and CodeBERT with a
Mean Reciprocal Rank (MRR) of 0.73 and 0.96, respectively. The outperformed
fine-tuned augmented code retrieval model is published in HuggingFace at
https://huggingface.co/Fujitsu/AugCode and a demonstration video is available
at: https://youtu.be/mnZrUTANjGs .",http://arxiv.org/abs/2110.08512v1
Finding Backdoors to Integer Programs: A Monte Carlo Tree Search Framework,"Monte Carlo Tree Search, Mixed Integer Programming, Backdoor Sets, Branch-and-Bound, Constraint Optimization",2021-10,The paper proposes a Monte Carlo Tree Search (MCTS) framework called BaMCTS for finding small 'backdoor' sets of integer variables that can be branched on to solve Mixed Integer Programs (MIPs) efficiently. BaMCTS outperforms existing sampling-based methods by better balancing exploration and exploitation.,"1. Introduction, 2. Related Work, 3. Technical Background (MIP, Backdoors, MCTS), 4. BaMCTS Method, 5. Experimental Evaluation, 6. Conclusion","['Formulates backdoor search as a single-player deterministic game', 'Uses MCTS with customized selection, expansion, simulation, and backpropagation steps', 'Incorporates MIP domain knowledge like reward functions, action scoring, and elimination rules', 'Tightly integrates with the CPLEX MIP solver for candidate evaluation']","For a MIP with integer variables I = {1, 2, 3} and backdoor size limit K = 2, the MCTS state space is P(I, K) = {(), (1), (2), (3), (1,2), (1,3), (2,1), (2,3), (3,1), (3,2)}. Evaluating the terminal state (1,3) involves running CPLEX branching only on variables 1 and 3 to check if it solves the MIP.","In Mixed Integer Linear Programming (MIP), a (strong) backdoor is a ""small""
subset of an instance's integer variables with the following property: in a
branch-and-bound procedure, the instance can be solved to global optimality by
branching only on the variables in the backdoor. Constructing datasets of
pre-computed backdoors for widely used MIP benchmark sets or particular problem
families can enable new questions around novel structural properties of a MIP,
or explain why a problem that is hard in theory can be solved efficiently in
practice. Existing algorithms for finding backdoors rely on sampling candidate
variable subsets in various ways, an approach which has demonstrated the
existence of backdoors for some instances from MIPLIB2003 and MIPLIB2010.
However, these algorithms fall short of consistently succeeding at the task due
to an imbalance between exploration and exploitation. We propose BaMCTS, a
Monte Carlo Tree Search framework for finding backdoors to MIPs. Extensive
algorithmic engineering, hybridization with traditional MIP concepts, and close
integration with the CPLEX solver have enabled our method to outperform
baselines on MIPLIB2017 instances, finding backdoors more frequently and more
efficiently.",http://arxiv.org/abs/2110.08423v2
Cascaded Fast and Slow Models for Efficient Semantic Code Search,"cascaded retrieval, semantic code search, transformer encoders, transformer classifiers, shared parameters",2021-10,"The paper proposes CasCode, an efficient and accurate framework for semantic code search that combines fast transformer encoders for initial retrieval with slow transformer classifiers for re-ranking the top results. A shared parameter variant reduces memory overhead while maintaining high performance.","1. Introduction - Motivation and background 2. Background - Prior encoder and classifier approaches 3. CasCode - The proposed cascaded fast/slow model approach 4. Experiments - Setup, baselines, results 5. Analysis - Ablations, parameter study, examples","['Use a transformer encoder to independently encode query and code snippets, retrieve top K candidates based on representation similarity', 'Pass top K candidates to a transformer classifier that jointly encodes query+code and predicts match probability', 'Optionally share transformer weights between encoder and classifier stages to reduce model size', 'Train the shared model on a combined objective of contrastive loss (for encoder) and cross-entropy (for classifier)']","For the query 'Implement bubble sort', the fast encoder retrieves top 10 candidates like 'def insertion_sort(...)', 'def mergeSort(...)' etc. The slow classifier then re-ranks them by processing [query, code] pairs and outputting 'def bubbleSort(...)' as the top result.","The goal of natural language semantic code search is to retrieve a
semantically relevant code snippet from a fixed set of candidates using a
natural language query. Existing approaches are neither effective nor efficient
enough towards a practical semantic code search system. In this paper, we
propose an efficient and accurate semantic code search framework with cascaded
fast and slow models, in which a fast transformer encoder model is learned to
optimize a scalable index for fast retrieval followed by learning a slow
classification-based re-ranking model to improve the performance of the top K
results from the fast retrieval. To further reduce the high memory cost of
deploying two separate models in practice, we propose to jointly train the fast
and slow model based on a single transformer encoder with shared parameters.
The proposed cascaded approach is not only efficient and scalable, but also
achieves state-of-the-art results with an average mean reciprocal ranking (MRR)
score of 0.7795 (across 6 programming languages) as opposed to the previous
state-of-the-art result of 0.713 MRR on the CodeSearchNet benchmark.",http://arxiv.org/abs/2110.07811v1
Searching for More Efficient Dynamic Programs,"dynamic programming, program optimization, semiring parsing, search algorithms, program transformations",2021-09,"The paper proposes an automated approach to discover more efficient dynamic programming algorithms for NLP problems by searching over semantics-preserving program transformations. It defines a space of possible programs, a cost metric based on asymptotic complexity, and a set of program transformations. It then uses search algorithms like beam search and Monte Carlo tree search to find optimized programs with lower asymptotic runtime.","1. Introduction - Motivation and background 
2. Dynamic program representation 
3. Program analysis using degree cost 
4. Program transformations 
5. Search algorithms 
6. Experiments 
7. Conclusion","['Represent dynamic programs in the Dyna language', ""Use the 'degree' (maximum number of variables in a rule) as a proxy for asymptotic runtime complexity"", 'Define a set of semantics-preserving program transformations like folding, unfolding, etc.', 'Formulate as a search problem to find a transformed program with minimum degree', 'Use beam search and Monte Carlo tree search (MCTS) to explore the search space']","For the CKY parsing program with initial degree 6, the search finds an equivalent program with degree 5 by introducing a temporary relation tmp(I,J,X,Z) to split the summation over variables Y and (Z,J,K).","Computational models of human language often involve combinatorial problems.
For instance, a probabilistic parser may marginalize over exponentially many
trees to make predictions. Algorithms for such problems often employ dynamic
programming and are not always unique. Finding one with optimal asymptotic
runtime can be unintuitive, time-consuming, and error-prone. Our work aims to
automate this laborious process. Given an initial correct declarative program,
we search for a sequence of semantics-preserving transformations to improve its
running time as much as possible. To this end, we describe a set of program
transformations, a simple metric for assessing the efficiency of a transformed
program, and a heuristic search procedure to improve this metric. We show that
in practice, automated search -- like the mental search performed by human
programmers -- can find substantial improvements to the initial program.
Empirically, we show that many common speed-ups described in the NLP literature
could have been discovered automatically by our system.",http://arxiv.org/abs/2109.06966v1
Contrastive Quantization with Code Memory for Unsupervised Image Retrieval,"contrastive quantization, code memory, codeword diversity regularization, debiased contrastive learning, unsupervised image retrieval",2021-09,This paper proposes a novel unsupervised deep quantization method called Contrastive Quantization with Code Memory (MeCoQ) that combines contrastive learning and deep quantization in a mutually beneficial framework. It introduces codeword diversity regularization to prevent model degeneration during training.,1. Introduction 2. Related Work 3. Modeling Framework 3.1. Problem Formulation and Model Overview 3.2. Debiased Contrastive Learning for Quantization 3.3. Regularization to Avoid Model Degeneration 3.4. Quantization Code Memory 4. Experiments 5. Conclusion,"['Uses trainable quantization with codebook attention to enable end-to-end learning', 'Employs debiased contrastive loss to handle false negative samples', 'Introduces codeword diversity regularization to prevent codewords in the same codebook from getting too close', 'Proposes a quantization code memory module to cache codes as additional negative keys, enhancing contrastive learning']","For an input image, MeCoQ extracts continuous embeddings from a CNN, quantizes them into binary codes using the trainable quantization module, and optimizes the debiased contrastive loss between the query codes and positive/negative codes from the code memory bank. The codeword diversity regularization prevents the codewords from collapsing.","The high efficiency in computation and storage makes hashing (including
binary hashing and quantization) a common strategy in large-scale retrieval
systems. To alleviate the reliance on expensive annotations, unsupervised deep
hashing becomes an important research problem. This paper provides a novel
solution to unsupervised deep quantization, namely Contrastive Quantization
with Code Memory (MeCoQ). Different from existing reconstruction-based
strategies, we learn unsupervised binary descriptors by contrastive learning,
which can better capture discriminative visual semantics. Besides, we uncover
that codeword diversity regularization is critical to prevent contrastive
learning-based quantization from model degeneration. Moreover, we introduce a
novel quantization code memory module that boosts contrastive learning with
lower feature drift than conventional feature memories. Extensive experiments
on benchmark datasets show that MeCoQ outperforms state-of-the-art methods.
Code and configurations are publicly available at
https://github.com/gimpong/AAAI22-MeCoQ.",http://arxiv.org/abs/2109.05205v2
Retrieval Augmented Code Generation and Summarization,"retrieval-augmented generation, dense retrieval, code generation, code summarization, bimodal retrieval",2021-08,"The paper proposes REDCODER, a framework that retrieves relevant code or summaries from a database and uses them to augment inputs to code generation or summarization models. It extends dense retrieval techniques for this retrieval and can handle unimodal or bimodal retrieval databases.","1. Introduction - Motivates retrieval-augmented generation 2. Background - Describes dense passage retrieval and PLBART generator 3. Proposed Framework (REDCODER) - Details the retriever (SCODE-R) and generator (SCODE-G) modules 4. Experiments - Setup, datasets, evaluation metrics 5. Results - Shows gains over baselines on code gen and summarization 6. Analysis - Analyzes different aspects of the framework","['Uses dense retrieval with separate encoders for code and text to retrieve relevant candidates', 'Retrieves top-k candidates and concatenates them with input to generator model', 'Can handle unimodal (code or text) or bimodal (code-text pairs) retrieval databases', 'Adopts PLBART as the generator model without modifying its architecture']","For code generation, given an input like 'Return the sum of two integers a and b', it retrieves relevant code snippets like 'def add(a, b): return a + b', concatenates them with the input, and provides this augmented input to the PLBART generator to produce the target code.","Software developers write a lot of source code and documentation during
software development. Intrinsically, developers often recall parts of source
code or code summaries that they had written in the past while implementing
software or documenting them. To mimic developers' code or summary generation
behavior, we propose a retrieval augmented framework, REDCODER, that retrieves
relevant code or summaries from a retrieval database and provides them as a
supplement to code generation or summarization models. REDCODER has a couple of
uniqueness. First, it extends the state-of-the-art dense retrieval technique to
search for relevant code or summaries. Second, it can work with retrieval
databases that include unimodal (only code or natural language description) or
bimodal instances (code-description pairs). We conduct experiments and
extensive analysis on two benchmark datasets of code generation and
summarization in Java and Python, and the promising results endorse the
effectiveness of our proposed retrieval augmented framework.",http://arxiv.org/abs/2108.11601v2
A Systematic Review of Automated Query Reformulations in Source Code Search,"term weighting, relevance feedback, semantic relations, co-occurrence analysis, thesaurus lookup",2021-08,"This systematic review analyzes 70 primary studies on automated query reformulation techniques for source code search. It identifies 8 major methodologies used, evaluates their approaches, highlights key limitations, and provides recommendations for future research in this area.",1. Introduction 2. Background 3. Review Methodology 4. Review Results 4.1 Methodologies and Algorithms 4.2 Evaluation Methods 4.3 Limitations and Challenges 4.4 ... 5. Open Challenges and Opportunities 6. Threats to Validity 7. Related Work 8. Conclusion,"['- Term weighting algorithms for constructing queries from change requests', '- Relevance feedback mechanisms for query reformulation', '- Semantic relation analysis between terms', '- Term co-occurrence analysis for related keywords', '- Thesaurus lookup for synonyms and related terms', '- Data mining and repository mining for relevant keywords', '- API recommendation for query expansion']","One approach used term co-occurrence analysis on software repositories to identify related keywords for query expansion. For example, for the query 'sort list', it recommended adding 'array' based on frequent co-occurrences, reformulating the query to 'sort list array' to improve code search results.","Fixing software bugs and adding new features are two of the major maintenance
tasks. Software bugs and features are reported as change requests. Developers
consult these requests and often choose a few keywords from them as an ad hoc
query. Then they execute the query with a search engine to find the exact
locations within software code that need to be changed. Unfortunately, even
experienced developers often fail to choose appropriate queries, which leads to
costly trials and errors during a code search. Over the years, many studies
attempt to reformulate the ad hoc queries from developers to support them. In
this systematic literature review, we carefully select 70 primary studies on
query reformulations from 2,970 candidate studies, perform an in-depth
qualitative analysis (e.g., Grounded Theory), and then answer seven research
questions with major findings. First, to date, eight major methodologies (e.g.,
term weighting, term co-occurrence analysis, thesaurus lookup) have been
adopted to reformulate queries. Second, the existing studies suffer from
several major limitations (e.g., lack of generalizability, vocabulary mismatch
problem, subjective bias) that might prevent their wide adoption. Finally, we
discuss the best practices and future opportunities to advance the state of
research in search query reformulations.",http://arxiv.org/abs/2108.09646v2
Improving Readability of Scratch Programs with Search-based Refactoring,"search-based refactoring, Scratch programming, code transformations, readability metrics, multi-objective optimization",2021-08,"This paper proposes a search-based approach to automatically refactor Scratch programs to improve their readability, without changing functionality. It defines a set of code transformations tailored for Scratch and uses multi-objective optimization to find sequences of these transformations that reduce complexity, size, and entropy of the programs.",1. Introduction 2. Background 2.1 Search-based refactoring 2.2 Code quality analysis for Scratch 2.3 Code readability 3. Approach 3.1 Code transformations for Scratch 3.2 Fitness functions 3.3 Search algorithm 3.4 Transformation application 3.5 Preserving semantics 4. Evaluation 5. Threats to Validity 6. Related Work 7. Conclusion,"['- Define 26 atomic code transformations for Scratch programs, categorized into control flow and concurrency transformations', '- Use 3 fitness functions based on program size, complexity (Halstead metrics), and entropy to measure readability', '- Apply multi-objective optimization algorithm (NSGA-II) to search for sequences of transformations that improve the fitness functions', '- Ensure semantic preservation by checking control, data, and time dependencies when applying transformations']","For example, the complex Scratch script in Figure 1a with a nested loop condition can be refactored to the simpler version in Figure 1b using transformations like 'Extract Loop Condition' and 'Forever If to Forever Wait'. This reduces the number of blocks, complexity, and entropy, improving readability.","Block-based programming languages like Scratch have become increasingly
popular as introductory languages for novices. These languages are intended to
be used with a ""tinkering"" approach which allows learners and teachers to
quickly assemble working programs and games, but this often leads to low code
quality. Such code can be hard to comprehend, changing it is error-prone, and
learners may struggle and lose interest. The general solution to improve code
quality is to refactor the code. However, Scratch lacks many of the common
abstraction mechanisms used when refactoring programs written in higher
programming languages. In order to improve Scratch code, we therefore propose a
set of atomic code transformations to optimise readability by (1) rewriting
control structures and (2) simplifying scripts using the inherently concurrent
nature of Scratch programs. By automating these transformations it is possible
to explore the space of possible variations of Scratch programs. In this paper,
we describe a multi-objective search-based approach that determines sequences
of code transformations which improve the readability of a given Scratch
program and therefore form refactorings. Evaluation on a random sample of 1000
Scratch programs demonstrates that the generated refactorings reduce complexity
and entropy in 70.4% of the cases, and 354 projects are improved in at least
one metric without making any other metric worse. The refactored programs can
help both novices and their teachers to improve their code.",http://arxiv.org/abs/2108.07114v1
On the Effectiveness of Transfer Learning for Code Search,"code search, transfer learning, BERT, multimodal embeddings, StackOverflow dataset",2021-08,"This paper investigates the use of transfer learning with BERT models for improving code search performance. Pre-trained BERT encoders for natural language queries and source code are combined into a multimodal embedding model and fine-tuned on StackOverflow data, outperforming baselines.","1. Introduction, 2. Background, 3. Approach (pre-training, data mining, fine-tuning), 4. Experimental Evaluation, 5. Results, 6. Related Work, 7. Conclusion","['Pre-train BERT encoders for queries (using existing model) and code (on GitHub data) separately', 'Mine StackOverflow Q&A pairs as proxy for code search (title = query, answer = code)', 'Assemble pre-trained encoders into multimodal embedding model (MEM)', 'Fine-tune MEM on StackOverflow code search dataset']","For the query 'how to convert string to int in java', the MEM would return code like 'int i = Integer.parseInt(intString);' by encoding the query and code into vectors, finding the closest vectors in the shared embedding space.","The Transformer architecture and transfer learning have marked a quantum leap
in natural language processing, improving the state of the art across a range
of text-based tasks. This paper examines how these advancements can be applied
to and improve code search. To this end, we pre-train a BERT-based model on
combinations of natural language and source code data and fine-tune it on pairs
of StackOverflow question titles and code answers. Our results show that the
pre-trained models consistently outperform the models that were not
pre-trained. In cases where the model was pre-trained on natural language ""and""
source code data, it also outperforms an information retrieval baseline based
on Lucene. Also, we demonstrated that the combined use of an information
retrieval-based approach followed by a Transformer leads to the best results
overall, especially when searching into a large search pool. Transfer learning
is particularly effective when much pre-training data is available and
fine-tuning data is limited. We demonstrate that natural language processing
models based on the Transformer architecture can be directly applied to source
code analysis tasks, such as code search. With the development of Transformer
models designed more specifically for dealing with source code data, we believe
the results of source code analysis tasks can be further improved.",http://arxiv.org/abs/2108.05890v2
Searching for Multi-Fault Programs in Defects4J,"multi-fault programs, test case transplantation, fault lifespan, failure clustering, fault localization",2021-08,"The paper presents a systematic approach to construct a multi-fault Java dataset by extending the popular Defects4J benchmark. By transplanting fault-revealing test cases across different faulty versions, the authors found that over 95% of the studied Defects4J versions actually contain multiple co-existing faults, ranging from 2 to 24 faults.",1. Introduction 2. Proposed Approach 2.1 Motivating Example 2.2 Searching for Multiple Fault Versions 2.3 Implementation Details 3. Results - Multiple Fault Subjects - Lifespan of Faults 4. Conclusion,"['Iteratively transplant fault-revealing test cases from one faulty version to previous faulty versions', 'Check if transplanted tests fail with the same error message, indicating the presence of the fault', 'Stop searching older versions once a fault is not revealed', 'Collect all revealed faults for each faulty version to identify multi-fault subjects']","For the Math-5 fault in Defects4J, the authors found that the faulty version Math-6b also contains Math-5 by transplanting the fault-revealing test case testReciprocalZero() from Math-5b to Math-6b and observing the same failing behavior.","Defects4J has enabled numerous software testing and debugging research work
since its introduction. A large part of its contribution, and the resulting
popularity, lies in the clear separation and distillation of the root cause of
each individual test failure based on careful manual analysis, which in turn
allowed researchers to easily study individual faults in isolation. However, in
a realistic debugging scenario, multiple faults can coexist and affect test
results collectively. Study of automated debugging techniques for these
situations, such as failure clustering or fault localisation for multiple
faults, would significantly benefit from a reliable benchmark of multiple,
coexisting faults. We search for versions of Defects4J subjects that contain
multiple faults, by iteratively transplanting fault-revealing test cases across
Defects4J versions. Out of 326 studied versions of Defects4J subjects, we
report that over 95% (311 versions) actually contain from two to 24 faults. We
hope that the extended, multi-fault Defects4J can provide a platform for future
research of testing and debugging techniques for multi-fault programs.",http://arxiv.org/abs/2108.04455v1
Improved Retrieval of Programming Solutions With Code Examples Using a Multi-featured Score,"crowd knowledge mining, semantic gap, word embeddings, sentence embeddings, convolutional neural networks, social features, antonym handling",2021-08,"This paper proposes CRAR, an approach to improve the retrieval of relevant programming solutions from Stack Overflow by combining various features like word embeddings, sentence embeddings, social signals, and handling antonyms. CRAR outperforms the previous state-of-the-art CROKAGE in retrieving complete solutions with both code examples and explanations.","1. Introduction 2. State-of-the-Art Limitations 3. CRAR Architecture 3.1. Constructing Dictionary of Antonyms 3.2. Constructing Models, Maps, and Indices 3.3. Searching for Relevant Answers 3.4. Features 4. Experimental Evaluation 5. Qualitative Discussion 6. Threats to Validity 7. Related Work 8. Conclusion","['Construct dictionary of antonyms from multiple sources', 'Build dataset, process Q&A, and index threads and answers from Stack Overflow', 'Employ various features like word/sentence embeddings, social signals, antonym handling', 'First retrieve relevant threads, then select relevant answers within those threads', 'Combine multiple features into a relevance score to rank answers']","For the query 'How to resize images in Java?', CROKAGE did not retrieve any of the 37 relevant answers in the top-10, while CRAR could leverage thread popularity and retrieve relevant high-scoring answers like those with IDs 244177, 5951429 and 4528136 from the same popular thread.","Developers often depend on code search engines to obtain solutions for their
programming tasks. However, finding an expected solution containing code
examples along with their explanations is challenging due to several issues.
There is a vocabulary mismatch between the search keywords (the query) and the
appropriate solutions. Semantic gap may increase for similar bag of words due
to antonyms and negation. Moreover, documents retrieved by search engines might
not contain solutions containing both code examples and their explanations. So,
we propose CRAR (Crowd Answer Recommender) to circumvent those issues aiming at
improving retrieval of relevant answers from Stack Overflow containing not only
the expected code examples for the given task but also their explanations.
Given a programming task, we investigate the effectiveness of combining
information retrieval techniques along with a set of features to enhance the
ranking of important threads (i.e., the units containing questions along with
their answers) for the given task and then selects relevant answers contained
in those threads, including semantic features, like word embeddings and
sentence embeddings, for instance, a Convolutional Neural Network (CNN). CRAR
also leverages social aspects of Stack Overflow discussions like popularity to
select relevant answers for the tasks. Our experimental evaluation shows that
the combination of the different features performs better than each one
individually. We also compare the retrieval performance with the state-of-art
CROKAGE (Crowd Knowledge Answer Generator), which is also a system aimed at
retrieving relevant answers from Stack Overflow. We show that CRAR outperforms
CROKAGE in Mean Reciprocal Rank and Mean Recall with small and medium effect
sizes, respectively.",http://arxiv.org/abs/2108.02702v1
Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning for Semantic Code Search,"ensemble learning, data augmentation, code representation, multi-perspective modeling, semantic code search",2021-07,"This paper proposes MuCoS, a multi-model ensemble learning architecture for semantic code search that combines several individual learners focused on different perspectives of code snippets. Data augmentation is used to generate datasets emphasizing structure, variables, and API usage, which are then used to train separate models that are ensembled.",1. Introduction 2. Proposed Model: MuCoS 2.1 Data Augmentation/Separation 2.2 Individual Model Fine-Tuning and Model Combination 3. Experimental Setup 3.1 Dataset 3.2 Evaluation Metrics 4. Results 4.1 RQ1: Performance vs SOTA 4.2 RQ2: Performance on Small Dataset 4.3 RQ3: Contribution of Individual Models 4.4 Case Study 5. Related Work,"['- Use data augmentation to generate datasets focused on code structure, variables, and API usage', '- Train separate CodeBERT models on each augmented dataset to get structure, variable, and API-focused models', '- Ensemble the individual models by concatenating their embeddings and adding an MLP classifier on top', '- Fine-tune the ensemble on the original dataset using cross-entropy loss']","For the query 'get the field label', the API-focused model ranks the correct code snippet first by capturing the overlap between the query and API calls like getString(). The structure and variable models rank it 3rd and 5th respectively, while the ensembled MuCoS ranks it 1st by combining all perspectives.","Recently, deep learning methods have become mainstream in code search since
they do better at capturing semantic correlations between code snippets and
search queries and have promising performance. However, code snippets have
diverse information from different dimensions, such as business logic, specific
algorithm, and hardware communication, so it is hard for a single code
representation module to cover all the perspectives. On the other hand, as a
specific query may focus on one or several perspectives, it is difficult for a
single query representation module to represent different user intents. In this
paper, we propose MuCoS, a multi-model ensemble learning architecture for
semantic code search. It combines several individual learners, each of which
emphasizes a specific perspective of code snippets. We train the individual
learners on different datasets which contain different perspectives of code
information, and we use a data augmentation strategy to get these different
datasets. Then we ensemble the learners to capture comprehensive features of
code snippets.",http://arxiv.org/abs/2107.04773v2
Multimodal Representation for Neural Code Search,"tree-serialized code representation, simplified semantic tree, multimodal code encoding, tree traversal serialization, code search evaluation metrics",2021-07,"This paper proposes using tree-serialized representations of simplified abstract syntax trees, along with multimodal learning, to improve neural code search. Novel tree serialization methods and information completeness metrics are introduced and evaluated on the CodeSearchNet dataset, showing improvements over baselines.",1. Introduction 2. Background 3. Multimodal Representation Approach 3.1 Overview 3.2 Simplified Semantic Tree 3.3 Tree Serialization 3.4 Multimodal Learning 4. Experimental Methodology 5. Results and Analysis 5.1 Quantitative Results 5.2 Qualitative Analysis 5.3 Ablation Study 5.4 Threats to Validity 6. Related Work 7. Conclusion,"['- Introduce Simplified Semantic Tree (SST) - a simplified form of AST highlighting semantic information', '- Serialize SST into sequences using sampling (RootPath, LeafPath) and traversal methods (SBT, LCRS)', '- Use token sequences and tree sequences as input modalities to multimodal pseudo-siamese network', '- Evaluate on CodeSearchNet dataset using common IR metrics like MRR', '- Define link coverage and node coverage metrics to quantify syntactic/semantic information completeness']","For the Python code snippet:

def birthday_marketing(self):
    """"""send birthday messages to members""""""
    today = datetime.date.today()
    for member in self.members:
        birthday = member.birthday
        if self.anniversary(today, birthday):
            member.SMS()

The Simplified Semantic Tree (SST) removes type declarations like 'self', modifiers like 'def', and replaces loops/expressions with semantic tags like 'loop' and 'literal'. The SST is then serialized into a sequence like ['module', 'literal', 'loop', 'call', 'call'] using traversal, which is used as the tree modality input along with the token modality.","Semantic code search is about finding semantically relevant code snippets for
a given natural language query. In the state-of-the-art approaches, the
semantic similarity between code and query is quantified as the distance of
their representation in the shared vector space. In this paper, to improve the
vector space, we introduce tree-serialization methods on a simplified form of
AST and build the multimodal representation for the code data. We conduct
extensive experiments using a single corpus that is large-scale and
multi-language: CodeSearchNet. Our results show that both our tree-serialized
representations and multimodal learning model improve the performance of code
search. Last, we define intuitive quantification metrics oriented to the
completeness of semantic and syntactic information of the code data, to help
understand the experimental findings.",http://arxiv.org/abs/2107.00992v3
Leveraging Language to Learn Program Abstractions and Search Heuristics,"language-guided program synthesis, joint language-program abstraction learning, neural program search, compositional generative models, hierarchical Bayesian inference",2021-06,"The paper introduces LAPS (Language for Abstraction and Program Search), a technique that leverages natural language annotations to jointly learn reusable program abstractions and neural search heuristics for program synthesis. By integrating language into a hierarchical Bayesian model, LAPS improves library learning, search efficiency, and generalization across domains like string editing and image composition.",1. Introduction 2. Related Work 3. Inductive synthesis and library learning (problem formulation) 4. Base learning algorithm: DreamCoder 5. LAPS Approach 6. Experiments and Results,"['Extends hierarchical Bayesian formulation to jointly generate programs and natural language descriptions', 'Learns a joint generative model over programs and language', 'Uses language to guide learning of program abstractions via compositional generativity', 'Trains a neural search model conditioned on language and program executions', 'Iteratively refines program library and search model on language-annotated tasks']","For the task 'draw a large hexagon', LAPS could learn an abstraction like 'λxy.(for ∞ (move_pen (* unit_line y) (/ 2π x)))' corresponding to the language fragment 'large six gon'. This reusable 'polygon' function generalizes better than lower-level abstractions learned without language.","Inductive program synthesis, or inferring programs from examples of desired
behavior, offers a general paradigm for building interpretable, robust, and
generalizable machine learning systems. Effective program synthesis depends on
two key ingredients: a strong library of functions from which to build
programs, and an efficient search strategy for finding programs that solve a
given task. We introduce LAPS (Language for Abstraction and Program Search), a
technique for using natural language annotations to guide joint learning of
libraries and neurally-guided search models for synthesis. When integrated into
a state-of-the-art library learning system (DreamCoder), LAPS produces
higher-quality libraries and improves search efficiency and generalization on
three domains -- string editing, image composition, and abstract reasoning
about scenes -- even when no natural language hints are available at test time.",http://arxiv.org/abs/2106.11053v3
Cross-Language Code Search using Static and Dynamic Analyses,"cross-language code search, non-dominated sorting, static analysis, dynamic analysis, code similarity",2021-06,"The paper proposes COSAL, a novel approach for cross-language code-to-code search that combines static and dynamic code analyses using non-dominated sorting. It outperforms existing techniques in precision and recall for finding similar code across programming languages.",1. Introduction - Motivation and challenges of cross-language code search 2. Background and examples 3. COSAL approach a. Token-based search b. AST-based search c. Dynamic analysis d. Non-dominated sorting 4. Experimental setup 5. Results 6. Discussion 7. Threats to validity 8. Related work 9. Conclusion,"['Uses token-based, AST-based, and dynamic input/output analysis for code similarity', 'Employs non-dominated sorting to rank results across the different similarity measures without aggregation', 'Builds indices for tokens, generic ASTs, and I/O behavior offline', 'During search, computes similarity between query and corpus for each measure', 'Returns ranked list of non-dominated results balancing the similarity measures']","For a Java function to get even numbers and a Python function to get odd numbers (Figure 1a, 1b), token similarity is 0 as names differ, AST similarity is 0 as structures are different, but I/O behavior is similar as both return filtered lists of numbers. Non-dominated sorting can identify this behavioral similarity without being dominated by token/AST differences.","As code search permeates most activities in software development,code-to-code
search has emerged to support using code as a query and retrieving similar code
in the search results. Applications include duplicate code detection for
refactoring, patch identification for program repair, and language translation.
Existing code-to-code search tools rely on static similarity approaches such as
the comparison of tokens and abstract syntax trees (AST) to approximate dynamic
behavior, leading to low precision. Most tools do not support cross-language
code-to-code search, and those that do, rely on machine learning models that
require labeled training data.
  We present Code-to-Code Search Across Languages (COSAL), a cross-language
technique that uses both static and dynamic analyses to identify similar code
and does not require a machine learning model. Code snippets are ranked using
non-dominated sorting based on code token similarity, structural similarity,
and behavioral similarity. We empirically evaluate COSAL on two datasets of
43,146Java and Python files and 55,499 Java files and find that 1) code search
based on non-dominated ranking of static and dynamic similarity measures is
more effective compared to single or weighted measures; and 2) COSAL has better
precision and recall compared to state-of-the-art within-language and
cross-language code-to-code search tools. We explore the potential for using
COSAL on large open-source repositories and discuss scalability to more
languages and similarity metrics, providing a gateway for
practical,multi-language code-to-code search.",http://arxiv.org/abs/2106.09173v1
Clone-Seeker: Effective Code Clone Search Using Annotations,"code clone search, annotation, keyword extraction, identifier analysis, clone class features",2021-06,"This paper proposes a novel approach called Clone-Seeker that utilizes clone class features and keyword annotations to improve code clone search, especially for retrieving semantically similar (Type-4) clones. It generates metadata for each clone by extracting identifiers and augmenting them with keywords from the clone class description.",1. Introduction 2. Related Work 3. Methodology 3.1 Dataset Selection 3.2 Identifier Extraction 3.3 Annotating Code Clones 3.4 Building Natural Language Document 4. Evaluation 4.1 Code-to-Code Search 4.2 Natural Language Query Search 5. Results and Discussion 6. Threats to Validity 7. Conclusion,"['- Select clone methods from BigCloneBench dataset', '- Extract identifiers from clone methods by tokenization, normalization, splitting, etc.', '- Annotate clone classes with keywords manually or automatically', '- Build natural language document for each clone by combining identifiers and annotations']","For the 'bubble_sort' clone class, the extracted identifiers could be ['swap', 'array', 'element', 'temp', 'iterate']. If manually annotated with keywords like 'sorting, comparison', the natural language document becomes: 'swap array element temp iterate sorting comparison bubble_sort'","Source code search plays an important role in software development, e.g. for
exploratory development or opportunistic reuse of existing code from a code
base. Often, exploration of different implementations with the same
functionality is needed for tasks like automated software transplantation,
software diversification, and software repair. Code clones, which are
syntactically or semantically similar code fragments, are perfect candidates
for such tasks. Searching for code clones involves a given search query to
retrieve the relevant code fragments. We propose a novel approach called
Clone-Seeker that focuses on utilizing clone class features in retrieving code
clones. For this purpose, we generate metadata for each code clone in the form
of a natural language document. The metadata includes a pre-processed list of
identifiers from the code clones augmented with a list of keywords indicating
the semantics of the code clone. This keyword list can be extracted from a
manually annotated general description of the clone class, or automatically
generated from the source code of the entire clone class. This approach helps
developers to perform code clone search based on a search query written either
as source code terms, or as natural language. In our quantitative evaluation,
we show that (1) Clone-Seeker has a higher recall when searching for semantic
code clones (i.e., Type-4) in BigCloneBench than the state-of-the-art; and (2)
Clone-Seeker can accurately search for relevant code clones by applying natural
language queries.",http://arxiv.org/abs/2106.03042v1
"CoSQA: 20,000+ Web Queries for Code Search and Question Answering","real user web queries, query-code matching, contrastive learning, data augmentation, code search",2021-05,"This paper introduces CoSQA, a new dataset of 20,604 pairs of natural language web queries and code snippets annotated for whether the code answers the query. It also proposes a contrastive learning method called CoCLR to augment the dataset for better query-code matching.",1. Introduction 2. Related Work 2.1 Datasets 2.2 Code Search Models 3. CoSQA Dataset 3.1 Data Collection 3.2 Data Annotation 4. CoCLR: Contrastive Learning for Query-Code Matching 5. Experiments 5.1 Code Question Answering 5.2 Code Search 6. Analysis 6.1 Effect of Data Size 6.2 Effect of Negative Instances 6.3 Effect of Query Quality 6.4 Leveraging Documentation 7. Conclusion,"['Collected real user web queries from Bing search logs related to code search', 'Retrieved Python code functions from GitHub repositories', 'Used a CodeBERT model to get candidate query-code pairs', 'Annotated pairs via crowdsourcing on whether code answers query', 'Proposed CoCLR method using contrastive loss to generate synthetic training pairs']","For the query 'python check if path is absolute path or relative path', an annotated positive example code function is:

def is_relative_url(url):
    """"""simple method to determine if a url is relative or absolute""""""
    if url.startswith(""#""): return None
    if url.find(""://"") > 0 or url.startswith(""//""):
        # either 'http(s)://...' or '//cdn...' and therefore absolute
        return False
    return True","Finding codes given natural language query isb eneficial to the productivity
of software developers. Future progress towards better semantic matching
between query and code requires richer supervised training resources. To remedy
this, we introduce the CoSQA dataset.It includes 20,604 labels for pairs of
natural language queries and codes, each annotated by at least 3 human
annotators. We further introduce a contrastive learning method dubbed CoCLR to
enhance query-code matching, which works as a data augmenter to bring more
artificially generated training instances. We show that evaluated on CodeXGLUE
with the same CodeBERT model, training on CoSQA improves the accuracy of code
question answering by 5.1%, and incorporating CoCLR brings a further
improvement of 10.5%.",http://arxiv.org/abs/2105.13239v1
Enriching Query Semantics for Code Search with Reinforcement Learning,"query semantics enrichment, reinforcement learning for code search, semantic gap between queries and descriptions, generating enriched queries, hybrid ranking with original and enriched queries",2021-05,"This paper proposes QueCos, a novel approach to improve code search by enriching the semantics of user queries through reinforcement learning. It generates semantic-augmented queries that better capture the intent behind short user queries, which are then used along with the original queries for improved code retrieval.",1. Introduction 2. Methodology 2.1 Code Search (CS) 2.2 Query Semantic Enrichment (QSE) 2.2.1 QSE Model 2.2.2 Training QSE via RL 2.2.3 Optimization 2.3 Hybrid Ranking 3. Experimental Setup 3.1 Dataset 3.1.1 CodeSearchNet 3.1.2 Collected Datasets 3.2 Evaluation Metrics 3.2.1 R@k 3.2.2 MRR 3.3 Baselines 4. Results and Analysis 5. Related Work 6. Conclusion,"['- Trains a code search (CS) model on code-description pairs to learn joint embeddings', '- Trains a query semantic enrichment (QSE) model via reinforcement learning to generate enriched queries from input queries', '- Uses advantage actor-critic (A2C) algorithm for RL, with reward based on ranking of retrieved code and similarity to ground truth description', '- Employs a hybrid ranking that combines original query, enriched query, and retrieved code embeddings']","For the query 'How to remove noise from a picture in Python?', the QSE model generates an enriched query like: 'You can initialize a 5x5 kernel matrix. OpenCV provides a function cv2.filter2D() to convolve a kernel with an image and produce the output smoothed image.' This enriched query better captures the semantics to retrieve the relevant Python code for noise removal.","Code search is a common practice for developers during software
implementation. The challenges of accurate code search mainly lie in the
knowledge gap between source code and natural language (i.e., queries). Due to
the limited code-query pairs and large code-description pairs available, the
prior studies based on deep learning techniques focus on learning the semantic
matching relation between source code and corresponding description texts for
the task, and hypothesize that the semantic gap between descriptions and user
queries is marginal. In this work, we found that the code search models trained
on code-description pairs may not perform well on user queries, which indicates
the semantic distance between queries and code descriptions. To mitigate the
semantic distance for more effective code search, we propose QueCos, a
Query-enriched Code search model. QueCos learns to generate semantic enriched
queries to capture the key semantics of given queries with reinforcement
learning (RL). With RL, the code search performance is considered as a reward
for producing accurate semantic enriched queries. The enriched queries are
finally employed for code search. Experiments on the benchmark datasets show
that QueCos can significantly outperform the state-of-the-art code search
models.",http://arxiv.org/abs/2105.09630v1
BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?,"semantic code search, embedding models, code embeddings, sentence embeddings, neural network transformation",2021-04,"This paper proposes BERT2Code, a neural network-based approach that leverages pretrained embedding models for natural language (BERT) and source code (Code2Vec, CodeBERT) to perform semantic code search by learning a transformation between the embedding spaces. The method achieves reasonable performance but is limited by the quality of available code embedding models.","1. Introduction 2. Background (sentence embeddings, code embeddings, similarity search) 3. Methodology (dataset, generating embeddings, neural network architecture, training/evaluation) 4. Results and Analysis 5. Related Work 6. Conclusion","['- Use Sentence-BERT to generate sentence embeddings for natural language queries', '- Use Code2Vec and CodeBERT to generate code embeddings for source code snippets', '- Train a 2-layer feed-forward neural network to transform sentence embeddings to code embeddings using Euclidean distance loss', '- For new queries, generate sentence embedding, transform to code embedding using trained network, and retrieve nearest code embeddings using FAISS']","For the natural language query 'Check if string is a valid Byte', the method generates a sentence embedding, transforms it to a code embedding vector, and retrieves the Java code snippet:

private static final boolean checkByte(String s) throws AttributeBadValueException {
    try {
        short val = Short.parseShort(s);
        if (val > 0xFF || val < 0) return false;
        else return true;
    } catch (NumberFormatException e) {
        throw new AttributeBadValueException('`' + s + '` is not a Byte value.');
    }
}","Millions of repetitive code snippets are submitted to code repositories every
day. To search from these large codebases using simple natural language queries
would allow programmers to ideate, prototype, and develop easier and faster.
Although the existing methods have shown good performance in searching codes
when the natural language description contains keywords from the code, they are
still far behind in searching codes based on the semantic meaning of the
natural language query and semantic structure of the code. In recent years,
both natural language and programming language research communities have
created techniques to embed them in vector spaces. In this work, we leverage
the efficacy of these embedding models using a simple, lightweight 2-layer
neural network in the task of semantic code search. We show that our model
learns the inherent relationship between the embedding spaces and further
probes into the scope of improvement by empirically analyzing the embedding
methods. In this analysis, we show that the quality of the code embedding model
is the bottleneck for our model's performance, and discuss future directions of
study in this area.",http://arxiv.org/abs/2104.08017v1
deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search,"variable-based flow graph, intermediate representation, code search, graph neural networks, LLVM IR",2021-03,"This paper proposes DEGRAPH CS, a novel approach for neural code search that represents source code as variable-based flow graphs derived from LLVM intermediate representation (IR). It outperforms existing methods by more precisely capturing code semantics through data/control dependencies between variables.","1. Introduction, 2. Motivating Examples, 3. Proposed Model (Overview, Neural Code Representation, Comment Representation, Model Learning), 4. Experiments, 5. Related Work, 6. Conclusion","['Construct variable-based flow graphs from LLVM IR, with nodes as tokens and edges as data/control dependencies', 'Optimize graphs by removing redundant nodes without changing semantics', 'Use attentional gated graph neural network to embed flow graphs into vectors', 'Represent natural language queries with RNN encoder', 'Train model to minimize ranking loss between code and query vectors']","For code snippets that sum an array (Fig 1d-f), their ASTs differ significantly despite having the same semantics. However, the proposed variable-based flow graphs (Fig 1g-h) are nearly identical, accurately capturing the shared underlying data/control flow.","With the rapid increase in the amount of public code repositories, developers
maintain a great desire to retrieve precise code snippets by using natural
language. Despite existing deep learning based approaches(e.g., DeepCS and
MMAN) have provided the end-to-end solutions (i.e., accepts natural language as
queries and shows related code fragments retrieved directly from code corpus),
the accuracy of code search in the large-scale repositories is still limited by
the code representation (e.g., AST) and modeling (e.g., directly fusing the
features in the attention stage). In this paper, we propose a novel learnable
deep Graph for Code Search (calleddeGraphCS), to transfer source code into
variable-based flow graphs based on the intermediate representation technique,
which can model code semantics more precisely compared to process the code as
text directly or use the syntactic tree representation. Furthermore, we propose
a well-designed graph optimization mechanism to refine the code representation,
and apply an improved gated graph neural network to model variable-based flow
graphs. To evaluate the effectiveness of deGraphCS, we collect a large-scale
dataset from GitHub containing 41,152 code snippets written in C language, and
reproduce several typical deep code search methods for comparison. Besides, we
design a qualitative user study to verify the practical value of our approach.
The experimental results have shown that deGraphCS can achieve state-of-the-art
performances, and accurately retrieve code snippets satisfying the needs of the
users.",http://arxiv.org/abs/2103.13020v3
RPT: Effective and Efficient Retrieval of Program Translations from Big Code,"cross-language code retrieval, program representation, hierarchical filtering, path-type-bucket index, unsupervised translation",2021-03,"This paper proposes Rpt, a novel system for retrieving program translations from a large code database (Big Code) in an efficient and effective manner. The key contribution is a generalizable program representation and a hierarchical filtering mechanism with a customized index structure for cross-language code retrieval.",1. Introduction 2. Our Approach 2.1 Program Representation 2.2 Translation Retrieval 3. Experiments 4. Conclusion and Future Work,"['Represents programs using a combination of structural (simplified syntax tree paths) and textual features', 'Generalizes the path types across languages by substituting node types with category labels', 'Implements a path-type-bucket index (Pbi) based on path type frequencies', 'Uses a hierarchical filtering mechanism with structural similarity filtering followed by textual similarity filtering', 'Calculates weighted sum of structural and textual similarities to rank candidates']","For a JavaScript code snippet, Rpt first parses it to a concrete syntax tree, simplifies it by pruning, extracts path types like 'Program -> BlockStatement -> ExpressionStatement -> CallExpression', generalizes the path by substituting node types with categories, and stores the path type frequencies in the Pbi index.","Program translation is a growing demand in software engineering. Manual
program translation requires programming expertise in source and target
language. One way to automate this process is to make use of the big data of
programs, i.e., Big Code. In particular, one can search for program
translations in Big Code. However, existing code retrieval techniques are not
designed for cross-language code retrieval. Other data-driven approaches
require human efforts in constructing cross-language parallel datasets to train
translation models. In this paper, we present RPT, a novel code translation
retrieval system. We propose a lightweight but informative program
representation, which can be generalized to all imperative PLs. Furthermore, we
present our index structure and hierarchical filtering mechanism for efficient
code retrieval from a Big Code database.",http://arxiv.org/abs/2103.12797v1
Refinement Type Directed Search for Meta-Interpretive-Learning of Higher-Order Logic Programs,"meta-interpretive learning, inductive logic programming, polymorphic type inference, refinement types, search space pruning",2021-02,"This paper introduces techniques for leveraging types to prune the search space and improve the efficiency of meta-interpretive learning (MIL) for synthesizing logic programs. It presents methods for polymorphic type checking and inferring types of synthesized clauses, as well as an approach for using refinement types with SMT solvers to further constrain the search space.",1. Introduction 2. Review of Program Synthesis 3. Untyped Meta-Interpretive Learning 4. Typed Meta-Interpretive Learning 5. Synthesis with Polymorphic Types 6. Synthesis with Refinement Types 7. Conclusion,"['- Introduces type checking to prune nonsensical programs from the search space in MIL', '- Presents an algorithm (Metagol PT) for polymorphic type checking through unification and type inference', '- Develops an approach to leverage refinement types with SMT solvers for further search space pruning', '- Provides theoretical analysis on soundness, completeness, and search space reduction', '- Conducts experiments to evaluate search space reduction and synthesis time improvements']","For the 'droplasts' example of dropping the last element from lists, the paper shows how polymorphic type checking can significantly reduce the search space by pruning inconsistent predicate combinations. It also demonstrates using refinement types to enforce properties like 'the output list is one element shorter than the input list'.","The program synthesis problem within the Inductive Logic Programming (ILP)
community has typically been seen as untyped. We consider the benefits of user
provided types on background knowledge. Building on the Meta-Interpretive
Learning (MIL) framework, we show that type checking is able to prune large
parts of the hypothesis space of programs. The introduction of polymorphic type
checking to the MIL approach to logic program synthesis is validated by strong
theoretical and experimental results, showing a cubic reduction in the size of
the search space and synthesis time, in terms of the number of typed background
predicates. Additionally we are able to infer polymorphic types of synthesized
clauses and of entire programs. The other advancement is in developing an
approach to leveraging refinement types in ILP. Here we show that further
pruning of the search space can be achieved, though the SMT solving used for
refinement type checking comes",http://arxiv.org/abs/2102.12553v1
Searching CUDA code autotuning spaces with hardware performance counters: data from benchmarks running on various GPU architectures,"CUDA autotuning, GPU performance counters, tuning space exploration, nonlinear regression models, decision tree models",2021-02,The paper presents a dataset of computation times and hardware performance counter measurements across the full tuning spaces of several CUDA benchmarks running on multiple GPU architectures. It also provides scripts to generate nonlinear regression and decision tree models to predict performance counter values from tuning parameters.,1. Introduction 2. Data Description 3. Experimental Setup 4. Obtaining Raw Data 5. Generating Prediction Models,"['Used Kernel Tuning Toolkit to exhaustively explore tuning spaces of 5 CUDA benchmarks on 4 GPUs', 'Measured computation time and hardware performance counters for each tuning configuration', 'Provided scripts to generate nonlinear least-squares regression models predicting performance counters from tuning parameters', 'Provided scripts to generate decision tree models for the same prediction task']","For the GEMM benchmark on a GTX 1070 GPU, the raw tuning data file '1070-gemm-reduced_output.csv' contains measurements across the full tuning space. The script 'create_least_squares_models.R' can be used to generate multiple nonlinear regression models predicting each performance counter based on the tuning parameters, with different models for different combinations of binary tuning parameters.","We have developed several autotuning benchmarks in CUDA that take into
account performance-relevant source-code parameters and reach near
peak-performance on various GPU architectures. We have used them during the
development and evaluation of a novel search method for tuning space proposed
in [1]. With our framework Kernel Tuning Toolkit, freely available at Github,
we measured computation times and hardware performance counters on several GPUs
for the complete tuning spaces of five benchmarks. These data, which we provide
here, might benefit research of search algorithms for the tuning spaces of GPU
codes or research of relation between applied code optimization, hardware
performance counters, and GPU kernels' performance.
  Moreover, we describe the scripts we used for robust evaluation of our
searcher and comparison to others in detail. In particular, the script that
simulates the tuning, i.e., replaces time-demanding compiling and executing the
tuned kernels with a quick reading of the computation time from our measured
data, makes it possible to inspect the convergence of tuning search over a
large number of experiments. These scripts, freely available with our other
codes, make it easier to experiment with search algorithms and compare them in
a robust way.
  During our research, we generated models for predicting values of performance
counters from values of tuning parameters of our benchmarks. Here, we provide
the models themselves and describe the scripts we implemented for their
training. These data might benefit researchers who want to reproduce or build
on our research.",http://arxiv.org/abs/2102.05299v1
A Search-Based Testing Framework for Deep Neural Networks of Source Code Embedding,"adversarial robustness, code embedding, search-based testing, code refactoring, mutation testing",2021-01,The paper proposes a search-based testing framework called Guided Mutation (GM) to generate adversarial examples for deep neural networks used in source code processing tasks like code embedding. It uses code refactoring techniques to create semantically equivalent variants and leverages mutation testing to guide the test generation process. Empirical evaluation shows GM can reduce the performance of state-of-the-art code embedding models while having a low negative impact on regular test data.,1. Introduction 2. Background 2.1 DNN Testing 2.2 Code Embedding 2.3 Code Adversarial Models 3. Methodology 3.1 Refactoring as Test Generation Basis 3.2 Guided Mutation Framework 4. Experiment Setup 5. Results and Analysis 6. Threats to Validity 7. Related Work 8. Conclusion,"['Uses 10 code refactoring operators like variable/method renaming, loop changes, etc. to generate semantically equivalent code variants', 'Adopts an evolutionary search strategy guided by mutation testing metrics', 'Generates adversarial examples that trigger robustness issues in code embedding DNNs', 'Retrains DNNs with the adversarial examples to improve robustness']","For example, applying the 'Argument Adding' refactoring to the code snippet in Figure 3(a) generates the adversarial variant in Figure 3(b). The DNN embedding produces different vectors for the original and refactored code despite having the same semantics.","Over the past few years, deep neural networks (DNNs) have been continuously
expanding their real-world applications for source code processing tasks across
the software engineering domain, e.g., clone detection, code search, comment
generation. Although quite a few recent works have been performed on testing of
DNNs in the context of image and speech processing, limited progress has been
achieved so far on DNN testing in the context of source code processing, that
exhibits rather unique characteristics and challenges.
  In this paper, we propose a search-based testing framework for DNNs of source
code embedding and its downstream processing tasks like Code Search. To
generate new test inputs, we adopt popular source code refactoring tools to
generate the semantically equivalent variants. For more effective testing, we
leverage the DNN mutation testing to guide the testing direction. To
demonstrate the usefulness of our technique, we perform a large-scale
evaluation on popular DNNs of source code processing based on multiple
state-of-the-art code embedding methods (i.e., Code2vec, Code2seq and
CodeBERT). The testing results show that our generated adversarial samples can
on average reduce the performance of these DNNs from 5.41% to 9.58%. Through
retraining the DNNs with our generated adversarial samples, the robustness of
DNN can improve by 23.05% on average. The evaluation results also show that our
adversarial test generation strategy has the least negative impact (median of
3.56%), on the performance of the DNNs for regular test data, compared to the
other methods.",http://arxiv.org/abs/2101.07910v1
